Directory structure:
└── middleware/
    ├── add-to-last-user-message.ts
    ├── generate-text-cache-middleware.ts
    ├── generate-text-log-middleware.ts
    ├── get-last-user-message-text.ts
    ├── stream-text-log-middleware.ts
    ├── stream-text-rag-middleware.ts
    ├── your-cache-middleware.ts
    ├── your-guardrail-middleware.ts
    ├── your-log-middleware.ts
    └── your-rag-middleware.ts

================================================
File: examples/ai-core/src/middleware/add-to-last-user-message.ts
================================================
import { LanguageModelV1CallOptions } from 'ai';

export function addToLastUserMessage({
  text,
  params,
}: {
  text: string;
  params: LanguageModelV1CallOptions;
}): LanguageModelV1CallOptions {
  const { prompt, ...rest } = params;

  const lastMessage = prompt.at(-1);

  if (lastMessage?.role !== 'user') {
    return params;
  }

  return {
    ...rest,
    prompt: [
      ...prompt.slice(0, -1),
      {
        ...lastMessage,
        content: [{ type: 'text', text }, ...lastMessage.content],
      },
    ],
  };
}


================================================
File: examples/ai-core/src/middleware/generate-text-cache-middleware.ts
================================================
import { openai } from '@ai-sdk/openai';
import { generateText, wrapLanguageModel } from 'ai';
import 'dotenv/config';
import { yourCacheMiddleware } from './your-cache-middleware';

async function main() {
  const modelWithCaching = wrapLanguageModel({
    model: openai('gpt-4o'),
    middleware: yourCacheMiddleware,
  });

  const start1 = Date.now();
  const result1 = await generateText({
    model: modelWithCaching,
    prompt: 'What cities are in the United States?',
  });
  const end1 = Date.now();

  const start2 = Date.now();
  const result2 = await generateText({
    model: modelWithCaching,
    prompt: 'What cities are in the United States?',
  });
  const end2 = Date.now();

  console.log(`Time taken for result1: ${end1 - start1}ms`);
  console.log(`Time taken for result2: ${end2 - start2}ms`);

  console.log(result1.text === result2.text);
}

main().catch(console.error);


================================================
File: examples/ai-core/src/middleware/generate-text-log-middleware.ts
================================================
import { openai } from '@ai-sdk/openai';
import { generateText, wrapLanguageModel } from 'ai';
import 'dotenv/config';
import { yourLogMiddleware } from './your-log-middleware';

async function main() {
  const result = await generateText({
    model: wrapLanguageModel({
      model: openai('gpt-4o'),
      middleware: yourLogMiddleware,
    }),
    prompt: 'What cities are in the United States?',
  });
}

main().catch(console.error);


================================================
File: examples/ai-core/src/middleware/get-last-user-message-text.ts
================================================
import { LanguageModelV1Prompt } from 'ai';

export function getLastUserMessageText({
  prompt,
}: {
  prompt: LanguageModelV1Prompt;
}): string | undefined {
  const lastMessage = prompt.at(-1);

  if (lastMessage?.role !== 'user') {
    return undefined;
  }

  return lastMessage.content.length === 0
    ? undefined
    : lastMessage.content.filter(c => c.type === 'text').join('\n');
}


================================================
File: examples/ai-core/src/middleware/stream-text-log-middleware.ts
================================================
import { openai } from '@ai-sdk/openai';
import { streamText, wrapLanguageModel } from 'ai';
import 'dotenv/config';
import { yourLogMiddleware } from './your-log-middleware';

async function main() {
  const result = streamText({
    model: wrapLanguageModel({
      model: openai('gpt-4o'),
      middleware: yourLogMiddleware,
    }),
    prompt: 'What cities are in the United States?',
  });

  for await (const textPart of result.textStream) {
    // consume the stream
  }
}

main().catch(console.error);


================================================
File: examples/ai-core/src/middleware/stream-text-rag-middleware.ts
================================================
import { openai } from '@ai-sdk/openai';
import { streamText, wrapLanguageModel } from 'ai';
import 'dotenv/config';
import { yourRagMiddleware } from './your-rag-middleware';

async function main() {
  const result = streamText({
    model: wrapLanguageModel({
      model: openai('gpt-4o'),
      middleware: yourRagMiddleware,
    }),
    prompt: 'What cities are in the United States?',
  });

  for await (const textPart of result.textStream) {
    process.stdout.write(textPart);
  }
}

main().catch(console.error);


================================================
File: examples/ai-core/src/middleware/your-cache-middleware.ts
================================================
import type { LanguageModelV1Middleware } from 'ai';

const cache = new Map<string, any>();

export const yourCacheMiddleware: LanguageModelV1Middleware = {
  wrapGenerate: async ({ doGenerate, params }) => {
    const cacheKey = JSON.stringify(params);

    if (cache.has(cacheKey)) {
      return cache.get(cacheKey);
    }

    const result = await doGenerate();

    cache.set(cacheKey, result);

    return result;
  },

  // here you would implement the caching logic for streaming
};


================================================
File: examples/ai-core/src/middleware/your-guardrail-middleware.ts
================================================
import type { LanguageModelV1Middleware } from 'ai';

export const yourGuardrailMiddleware: LanguageModelV1Middleware = {
  wrapGenerate: async ({ doGenerate }) => {
    const { text, ...rest } = await doGenerate();

    // filtering approach, e.g. for PII or other sensitive information:
    const cleanedText = text?.replace(/badword/g, '<REDACTED>');

    return { text: cleanedText, ...rest };
  },

  // here you would implement the guardrail logic for streaming
  // Note: streaming guardrails are difficult to implement, because
  // you do not know the full content of the stream until it's finished.
};


================================================
File: examples/ai-core/src/middleware/your-log-middleware.ts
================================================
import type { LanguageModelV1Middleware, LanguageModelV1StreamPart } from 'ai';

export const yourLogMiddleware: LanguageModelV1Middleware = {
  wrapGenerate: async ({ doGenerate, params }) => {
    console.log('doGenerate called');
    console.log(`params: ${JSON.stringify(params, null, 2)}`);

    const result = await doGenerate();

    console.log('doGenerate finished');
    console.log(`generated text: ${result.text}`);

    return result;
  },

  wrapStream: async ({ doStream, params }) => {
    console.log('doStream called');
    console.log(`params: ${JSON.stringify(params, null, 2)}`);

    const { stream, ...rest } = await doStream();

    let generatedText = '';

    const transformStream = new TransformStream<
      LanguageModelV1StreamPart,
      LanguageModelV1StreamPart
    >({
      transform(chunk, controller) {
        if (chunk.type === 'text-delta') {
          generatedText += chunk.textDelta;
        }

        controller.enqueue(chunk);
      },

      flush() {
        console.log('doStream finished');
        console.log(`generated text: ${generatedText}`);
      },
    });

    return {
      stream: stream.pipeThrough(transformStream),
      ...rest,
    };
  },
};


================================================
File: examples/ai-core/src/middleware/your-rag-middleware.ts
================================================
import { addToLastUserMessage } from './add-to-last-user-message';
import { getLastUserMessageText } from './get-last-user-message-text';
import type { LanguageModelV1Middleware } from 'ai';

export const yourRagMiddleware: LanguageModelV1Middleware = {
  transformParams: async ({ params }) => {
    const lastUserMessageText = getLastUserMessageText({
      prompt: params.prompt,
    });

    if (lastUserMessageText == null) {
      return params; // do not use RAG (send unmodified parameters)
    }

    const instruction =
      'Use the following information to answer the question:\n' +
      findSources({ text: lastUserMessageText })
        .map(chunk => JSON.stringify(chunk))
        .join('\n');

    return addToLastUserMessage({ params, text: instruction });
  },
};

// example, could implement anything here:
function findSources({ text }: { text: string }): Array<{
  title: string;
  previewText: string | undefined;
  url: string | undefined;
}> {
  return [
    {
      title: 'New York',
      previewText: 'New York is a city in the United States.',
      url: 'https://en.wikipedia.org/wiki/New_York',
    },
    {
      title: 'San Francisco',
      previewText: 'San Francisco is a city in the United States.',
      url: 'https://en.wikipedia.org/wiki/San_Francisco',
    },
  ];
}


