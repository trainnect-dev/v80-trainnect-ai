Directory structure:
└── zipstack-unstract-sdk/
    ├── README.md
    ├── CODE_OF_CONDUCT.md
    ├── CONTRIBUTING.md
    ├── LICENSE
    ├── MANIFEST.in
    ├── SECURITY.md
    ├── pdm.lock
    ├── pyproject.toml
    ├── .pre-commit-config.yaml
    ├── docs/
    │   ├── README.md
    │   ├── assets/
    │   └── site/
    │       ├── README.md
    │       ├── babel.config.js
    │       ├── docusaurus.config.js
    │       ├── package-lock.json
    │       ├── package.json
    │       ├── sidebars.js
    │       ├── .gitignore
    │       ├── docs/
    │       │   ├── Introduction.md
    │       │   ├── Unstract platform components.md
    │       │   ├── API Reference/
    │       │   │   ├── cache.md
    │       │   │   ├── constants.md
    │       │   │   ├── dbs.md
    │       │   │   ├── documents.md
    │       │   │   ├── file_system.md
    │       │   │   ├── helper.md
    │       │   │   ├── llm.md
    │       │   │   ├── platform.md
    │       │   │   ├── tool.base.md
    │       │   │   ├── tool.entrypoint.md
    │       │   │   ├── tool.executor.md
    │       │   │   ├── tool.md
    │       │   │   ├── tool.mixin.md
    │       │   │   ├── tool.parser.md
    │       │   │   ├── tool.stream.md
    │       │   │   └── utils.md
    │       │   └── Tools/
    │       │       ├── 1_Overview.md
    │       │       ├── 2_Tool architecture.md
    │       │       ├── 3_Installation and quick start.md
    │       │       ├── 4_Tool definition.md
    │       │       ├── 5_Tool settings and icon.md
    │       │       ├── 6_Environment variables.md
    │       │       ├── 7_Unstract protocol.md
    │       │       ├── 8_Implementing and testing the tool.md
    │       │       └── _category_.json
    │       ├── src/
    │       │   ├── components/
    │       │   │   └── HomepageFeatures/
    │       │   │       ├── index.js
    │       │   │       └── styles.module.css
    │       │   ├── css/
    │       │   │   └── custom.css
    │       │   └── pages/
    │       │       ├── index.js
    │       │       ├── index.module.css
    │       │       └── markdown-page.md
    │       └── static/
    │           ├── .nojekyll
    │           └── img/
    │               └── page_content/
    ├── src/
    │   └── unstract/
    │       └── sdk/
    │           ├── __init__.py
    │           ├── adapter.py
    │           ├── audit.py
    │           ├── cache.py
    │           ├── constants.py
    │           ├── embedding.py
    │           ├── exceptions.py
    │           ├── helper.py
    │           ├── index.py
    │           ├── llm.py
    │           ├── metrics_mixin.py
    │           ├── ocr.py
    │           ├── platform.py
    │           ├── prompt.py
    │           ├── vector_db.py
    │           ├── x2txt.py
    │           ├── adapters/
    │           │   ├── __init__.py
    │           │   ├── adapterkit.py
    │           │   ├── base.py
    │           │   ├── constants.py
    │           │   ├── enums.py
    │           │   ├── exceptions.py
    │           │   ├── registry.py
    │           │   ├── utils.py
    │           │   ├── embedding/
    │           │   │   ├── __init__.py
    │           │   │   ├── embedding_adapter.py
    │           │   │   ├── helper.py
    │           │   │   ├── register.py
    │           │   │   ├── azure_open_ai/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── azure_open_ai.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── bedrock/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── bedrock.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── hugging_face/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── hugging_face.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── no_op/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── no_op_custom_embedding.py
    │           │   │   │       ├── no_op_embedding.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── ollama/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── ollama.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── open_ai/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── open_ai.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── palm/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── palm.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── qdrant_fast_embed/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── qdrant_fast_embed.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   └── vertex_ai/
    │           │   │       ├── README.md
    │           │   │       ├── pyproject.toml
    │           │   │       └── src/
    │           │   │           ├── __init__.py
    │           │   │           ├── vertex_ai.py
    │           │   │           └── static/
    │           │   │               └── json_schema.json
    │           │   ├── llm/
    │           │   │   ├── __init__.py
    │           │   │   ├── constants.py
    │           │   │   ├── exceptions.py
    │           │   │   ├── llm_adapter.py
    │           │   │   ├── register.py
    │           │   │   ├── anthropic/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── anthropic.py
    │           │   │   │       ├── exceptions.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── any_scale/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── anyscale.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── azure_open_ai/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── azure_open_ai.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── bedrock/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── bedrock.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── mistral/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── mistral.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── no_op/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── no_op_custom_llm.py
    │           │   │   │       ├── no_op_llm.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── ollama/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── ollama.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── open_ai/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── open_ai.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── palm/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── palm.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── replicate/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── replicate.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   └── vertex_ai/
    │           │   │       ├── README.md
    │           │   │       ├── pyproject.toml
    │           │   │       └── src/
    │           │   │           ├── __init__.py
    │           │   │           ├── vertex_ai.py
    │           │   │           └── static/
    │           │   │               └── json_schema.json
    │           │   ├── ocr/
    │           │   │   ├── __init__.py
    │           │   │   ├── constants.py
    │           │   │   ├── ocr_adapter.py
    │           │   │   ├── register.py
    │           │   │   └── google_document_ai/
    │           │   │       ├── README.md
    │           │   │       ├── pyproject.toml
    │           │   │       └── src/
    │           │   │           ├── README.md
    │           │   │           ├── __init__.py
    │           │   │           ├── google_document_ai.py
    │           │   │           └── static/
    │           │   │               └── json_schema.json
    │           │   ├── vectordb/
    │           │   │   ├── __init__.py
    │           │   │   ├── constants.py
    │           │   │   ├── exceptions.py
    │           │   │   ├── helper.py
    │           │   │   ├── register.py
    │           │   │   ├── vectordb_adapter.py
    │           │   │   ├── milvus/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── milvus.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── no_op/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── no_op_custom_vectordb.py
    │           │   │   │       ├── no_op_vectordb.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── pinecone/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── pinecone.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── postgres/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── postgres.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── qdrant/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── qdrant.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   ├── samples/
    │           │   │   │   └── sample1.txt
    │           │   │   ├── supabase/
    │           │   │   │   ├── README.md
    │           │   │   │   ├── pyproject.toml
    │           │   │   │   └── src/
    │           │   │   │       ├── __init__.py
    │           │   │   │       ├── supabase.py
    │           │   │   │       └── static/
    │           │   │   │           └── json_schema.json
    │           │   │   └── weaviate/
    │           │   │       ├── README.md
    │           │   │       ├── pyproject.toml
    │           │   │       └── src/
    │           │   │           ├── __init__.py
    │           │   │           ├── weaviate.py
    │           │   │           └── static/
    │           │   │               └── json_schema.json
    │           │   └── x2text/
    │           │       ├── __init__.py
    │           │       ├── constants.py
    │           │       ├── dto.py
    │           │       ├── helper.py
    │           │       ├── register.py
    │           │       ├── x2text_adapter.py
    │           │       ├── llama_parse/
    │           │       │   ├── README.md
    │           │       │   ├── pyproject.toml
    │           │       │   └── src/
    │           │       │       ├── __init__.py
    │           │       │       ├── constants.py
    │           │       │       ├── llama_parse.py
    │           │       │       └── static/
    │           │       │           ├── json_schema.json
    │           │       │           └── test_input.doc
    │           │       ├── llm_whisperer/
    │           │       │   ├── README.md
    │           │       │   ├── pyproject.toml
    │           │       │   └── src/
    │           │       │       ├── __init__.py
    │           │       │       ├── constants.py
    │           │       │       ├── llm_whisperer.py
    │           │       │       └── static/
    │           │       │           └── json_schema.json
    │           │       ├── llm_whisperer_v2/
    │           │       │   ├── README.md
    │           │       │   ├── pyproject.toml
    │           │       │   └── src/
    │           │       │       ├── __init__.py
    │           │       │       ├── constants.py
    │           │       │       ├── dto.py
    │           │       │       ├── helper.py
    │           │       │       ├── llm_whisperer_v2.py
    │           │       │       └── static/
    │           │       │           └── json_schema.json
    │           │       ├── no_op/
    │           │       │   ├── README.md
    │           │       │   ├── pyproject.toml
    │           │       │   └── src/
    │           │       │       ├── __init__.py
    │           │       │       ├── no_op_x2text.py
    │           │       │       └── static/
    │           │       │           └── json_schema.json
    │           │       ├── unstructured_community/
    │           │       │   ├── README.md
    │           │       │   ├── pyproject.toml
    │           │       │   └── src/
    │           │       │       ├── __init__.py
    │           │       │       ├── unstructured_community.py
    │           │       │       └── static/
    │           │       │           └── json_schema.json
    │           │       └── unstructured_enterprise/
    │           │           ├── README.md
    │           │           ├── pyproject.toml
    │           │           └── src/
    │           │               ├── __init__.py
    │           │               ├── unstructured_enterprise.py
    │           │               └── static/
    │           │                   └── json_schema.json
    │           ├── file_storage/
    │           │   ├── __init__.py
    │           │   ├── constants.py
    │           │   ├── env_helper.py
    │           │   ├── helper.py
    │           │   ├── impl.py
    │           │   ├── interface.py
    │           │   ├── permanent.py
    │           │   ├── provider.py
    │           │   └── shared_temporary.py
    │           ├── scripts/
    │           │   └── tool_gen.py
    │           ├── static/
    │           │   └── tool_template/
    │           │       └── v1/
    │           │           ├── README.md
    │           │           ├── Dockerfile
    │           │           ├── requirements.txt
    │           │           ├── sample.env
    │           │           ├── .dockerignore
    │           │           ├── data_dir/
    │           │           │   ├── INFILE
    │           │           │   ├── METADATA.json
    │           │           │   ├── SOURCE
    │           │           │   └── COPY_TO_FOLDER/
    │           │           │       └── .gitkeep
    │           │           └── src/
    │           │               ├── main.py
    │           │               └── config/
    │           │                   ├── properties.json
    │           │                   └── spec.json
    │           ├── tool/
    │           │   ├── __init__.py
    │           │   ├── base.py
    │           │   ├── entrypoint.py
    │           │   ├── executor.py
    │           │   ├── mime_types.py
    │           │   ├── mixin.py
    │           │   ├── parser.py
    │           │   ├── stream.py
    │           │   └── validator.py
    │           └── utils/
    │               ├── __init__.py
    │               ├── callback_manager.py
    │               ├── common_utils.py
    │               ├── file_storage_utils.py
    │               ├── token_counter.py
    │               ├── tool_utils.py
    │               └── usage_handler.py
    ├── tests/
    │   ├── __init__.py
    │   ├── sample.env
    │   ├── test_cache.py
    │   ├── test_embedding.py
    │   ├── test_file_storage.py
    │   ├── test_fs_permanent.py
    │   ├── test_index.py
    │   ├── test_llm.py
    │   ├── test_ocr.py
    │   ├── test_vector_db.py
    │   ├── test_x2text.py
    │   ├── config/
    │   │   ├── properties.json
    │   │   ├── runtime_variables.json
    │   │   └── spec.json
    │   ├── fsspec-test/
    │   │   └── input/
    │   │       ├── 1.txt
    │   │       └── 3.txt
    │   └── tool/
    │       ├── __init__.py
    │       ├── test_entrypoint.py
    │       ├── test_static.py
    │       └── config/
    │           ├── properties.json
    │           ├── runtime_variables.json
    │           ├── spec.json
    │           ├── tool_properties.json
    │           └── tool_spec.json
    └── .github/
        ├── pull_request_template.md
        ├── ISSUE_TEMPLATE/
        │   ├── bug_report.md
        │   └── feature_request.md
        └── workflows/
            └── pypi-publish.yml

================================================
File: README.md
================================================
<div align="center">
<img src="https://raw.githubusercontent.com/Zipstack/unstract-sdk/main/docs/assets/unstract_u_logo.png" style="height: 120px">

# Unstract

## No-code LLM Platform to launch APIs and ETL Pipelines to structure unstructured documents

</div>

# Unstract SDK

The `unstract-sdk` package helps with developing tools that are meant to be run on the Unstract platform. This includes
modules to help with tool development and execution, caching, making calls to LLMs / vectorDBs / embeddings .etc.
They also contain helper methods/classes to aid with other tasks such as indexing and auditing the LLM calls.

## Installation

- The below libraries need to be installed to run the SDK
  - Linux

    ```
    sudo apt install build-essential pkg-config libmagic-dev
    ```

  - Mac

    ```
    brew install pkg-config libmagic pandoc tesseract-ocr
    ```

## Tools

### Create a scaffolding for a new tool

Example

```bash
unstract-tool-gen --command NEW --tool-name <name of tool> \
 --location ~/path_to_repository/unstract-tools/ --overwrite false
```

Supported commands:

- `NEW` - Create a new tool

### Environment variables required for all Tools

| Variable                   | Description                                                           |
| -------------------------- | --------------------------------------------------------------------- |
| `PLATFORM_SERVICE_HOST`    | The host in which the platform service is running                     |
| `PLATFORM_SERVICE_PORT`    | The port in which the service is listening                            |
| `PLATFORM_SERVICE_API_KEY` | The API key for the platform                                          |
| `TOOL_DATA_DIR`            | The directory in the filesystem which has contents for tool execution |

### Llama Index support

Unstract SDK 0.3.2 uses the following version of Llama
Index Version **0.9.28** as on January 14th, 2024

### Developing with the SDK

Ensure that you have all the required dependencies and pre-commit hooks installed
```shell
pdm install
pre-commit install
```

Once the changes have been made, it can be tested with [Unstract](https://github.com/Zipstack/unstract) through the following means.

#### With PDM
Specify the SDK as a dependency to a project using a tool like `pdm` by adding the following to your `pyproject.toml`

```toml
[tool.pdm.dev-dependencies]
local_copies = [
    "-e unstract-adapters @ file:///${UNSTRACT_ADAPTERS_PATH}",
    "-e unstract-sdk @ file:///${UNSTRACT_SDK_PATH}",
]
```
Or by running the below command
```shell
pdm add -e /path/to/unstract-sdk --dev
```

#### With pip
- If the project is using `pip` it might be possible to add it as a dependency in `requirements.txt`
```
-e /path/to/unstract-sdk
```
NOTE: Building locally might require the below section to be replaced in the `unstract-sdk`'s build system configuration
```
[build-system]
requires = ["setuptools", "wheel"]
build-backend = "setuptools.build_meta"
```
- Another option is to provide a git URL in `requirements.txt`, this can come in handy while building tool
docker images. Don't forget to run `apt install git` within the `Dockerfile` for this
```shell
unstract-sdk @ git+https://github.com/Zipstack/unstract-sdk@feature-branch
```

- Or try installing a [local PyPI server](https://pypi.org/project/pypiserver/) and upload / download your package from this server

#### Additonal dependencies for tool
Tools may need to be backed up by a file storage. unstract.sdk.file_storage contains the required interfaces for the 
same. fssepc is being used underneath to implement these interfaces. Hence, one can choose to use a file_system 
supported by fsspec for this. However, the required dependencies need to be added in the tool dependency manager. 
Eg. If the tool is using Minio as the underlying file storage, then s3fs can be added to support it. 
Similarly, for Google Cloud Storage, gcsfs is to be added. 
The following versions are tested in the SDK using unit test cases for the above package.
    gcsfs==2024.10.0
    s3fs==2024.10.0


### Documentation generation

Follow [this README.md](https://github.com/Zipstack/unstract-sdk/blob/main/docs/README.md) for generating documentation.


================================================
File: CODE_OF_CONDUCT.md
================================================
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity
and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
  advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
  address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
community-reports@unstract.com.
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series
of actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or
permanent ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior,  harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within
the community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0, available at
https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
https://www.contributor-covenant.org/faq. Translations are available at
https://www.contributor-covenant.org/translations.


================================================
File: CONTRIBUTING.md
================================================
# Contributing

See [docs.unstract.com](https://docs.unstract.com/contributing/unstract/sdk).

================================================
File: LICENSE
================================================
                    GNU AFFERO GENERAL PUBLIC LICENSE
                       Version 3, 19 November 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU Affero General Public License is a free, copyleft license for
software and other kinds of works, specifically designed to ensure
cooperation with the community in the case of network server software.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
our General Public Licenses are intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  Developers that use our General Public Licenses protect your rights
with two steps: (1) assert copyright on the software, and (2) offer
you this License which gives you legal permission to copy, distribute
and/or modify the software.

  A secondary benefit of defending all users' freedom is that
improvements made in alternate versions of the program, if they
receive widespread use, become available for other developers to
incorporate.  Many developers of free software are heartened and
encouraged by the resulting cooperation.  However, in the case of
software used on network servers, this result may fail to come about.
The GNU General Public License permits making a modified version and
letting the public access it on a server without ever releasing its
source code to the public.

  The GNU Affero General Public License is designed specifically to
ensure that, in such cases, the modified source code becomes available
to the community.  It requires the operator of a network server to
provide the source code of the modified version running there to the
users of that server.  Therefore, public use of a modified version, on
a publicly accessible server, gives the public access to the source
code of the modified version.

  An older license, called the Affero General Public License and
published by Affero, was designed to accomplish similar goals.  This is
a different license, not a version of the Affero GPL, but Affero has
released a new version of the Affero GPL which permits relicensing under
this license.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU Affero General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Remote Network Interaction; Use with the GNU General Public License.

  Notwithstanding any other provision of this License, if you modify the
Program, your modified version must prominently offer all users
interacting with it remotely through a computer network (if your version
supports such interaction) an opportunity to receive the Corresponding
Source of your version by providing access to the Corresponding Source
from a network server at no charge, through some standard or customary
means of facilitating copying of software.  This Corresponding Source
shall include the Corresponding Source for any work covered by version 3
of the GNU General Public License that is incorporated pursuant to the
following paragraph.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the work with which it is combined will remain governed by version
3 of the GNU General Public License.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU Affero General Public License from time to time.  Such new versions
will be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU Affero General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU Affero General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU Affero General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If your software can interact with users remotely through a computer
network, you should also make sure that it provides a way for users to
get its source.  For example, if your program is a web application, its
interface could display a "Source" link that leads users to an archive
of the code.  There are many ways you could offer source, and different
solutions will be better for different programs; see section 13 for the
specific requirements.

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU AGPL, see
<https://www.gnu.org/licenses/>.


================================================
File: MANIFEST.in
================================================
recursive-include src/unstract/sdk/static *


================================================
File: SECURITY.md
================================================
# Security Policy

## Supported Versions

The following versions are currently being supported with security updates.

| Version | Supported          |
| ------- | ------------------ |
| < 0.17  | :x:                |
| 0.17.x  | :white_check_mark: |
| > 0.17  | :white_check_mark: |

## Reporting a Vulnerability or Security Issue

The Unstract team and community take security issues very seriously. We appreciate your efforts to responsibly disclose your findings, and will make every effort to acknowledge your contributions.

To report a vulnerability or security issue, please use the GitHub Security Advisory page ["Report a Vulnerability"](https://github.com/Zipstack/unstract-sdk/security/advisories/) button.

The Unstract team will send a response indicating the next steps in handling the reported issue. After the initial reply to your report, the Unstract team will keep you informed of the progress towards a fix and full announcement, and may ask for additional information or guidance.


================================================
File: pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"

[project]
name = "unstract-sdk"
dynamic = ["version"]
description = "A framework for writing Unstract Tools/Apps"
dependencies = [
    # Specs and validation
    "jsonschema~=4.18.2",
    "python-magic~=0.4.27",
    "python-dotenv==1.0.0",
    # Adapter changes
    "llama-index==0.12.8",
    "tiktoken~=0.4.0",
    "transformers==4.37.0",
    "llama-index-embeddings-google==0.3.0",
    "llama-index-embeddings-azure-openai==0.3.0",
    # Disabling Hugging Face & FastEmbed to
    # keep the image size under check
    #        "llama-index-embeddings-huggingface==0.2.0",
    # Disabling fast embed due to high processing power
    #        "llama-index-embeddings-fastembed==0.1.4",
    "llama-index-embeddings-openai==0.3.1",
    "llama-index-embeddings-ollama==0.5.0",
    "llama-index-embeddings-bedrock==0.5.0",
    "llama-index-embeddings-vertex==0.3.1",
    "llama-index-vector-stores-postgres==0.4.1",
    # Including Supabase conflicts with postgres on pg-vector.
    # Hence, commenting it out at the moment
    #        "llama-index-vector-stores-supabase==0.1.3",
    "llama-index-vector-stores-milvus==0.4.0",
    "llama-index-vector-stores-weaviate==1.3.1",
    "llama-index-vector-stores-pinecone==0.4.2",
    "llama-index-vector-stores-qdrant==0.4.2",
    "llama-index-llms-openai==0.3.17",
    "llama-index-llms-palm==0.3.0",
    "llama-index-llms-mistralai==0.3.1",
    "mistralai==1.2.5",
    "llama-index-llms-anyscale==0.3.0",
    "llama-index-llms-anthropic==0.6.3",
    "llama-index-llms-azure-openai==0.3.0",
    "llama-index-llms-vertex==0.4.2",
    "llama-index-llms-replicate==0.4.0",
    "llama-index-llms-ollama==0.5.0",
    "llama-index-llms-bedrock==0.3.3",
    # For Llama Parse X2Text
    "llama-parse==0.5.19",
    # OCR
    "filetype~=1.2.0",
    # Others
    # For singleton classes
    "singleton-decorator~=1.0.0",
    "httpx>=0.25.2",
    "pdfplumber>=0.11.2",
    "redis>=5.2.1",
    "llmwhisperer-client>=2.2.1",
]
readme = "README.md"
urls = { Homepage = "https://unstract.com", "Release notes" = "https://github.com/Zipstack/unstract-sdk/releases", Source = "https://github.com/Zipstack/unstract-sdk" }
license = {text = "AGPL v3"}
authors = [
    {name = "Zipstack Inc", email = "devsupport@zipstack.com"},
]
keywords = ["unstract tools-development-kit apps development-kit sdk"]
requires-python = ">=3.9,<3.11.1"
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: GNU Affero General Public License v3",
    "Operating System :: POSIX :: Linux",
    "Operating System :: MacOS :: MacOS X",
    "Programming Language :: Python",
    "Programming Language :: Python :: 3.9",
    "Topic :: Scientific/Engineering",
    "Topic :: Software Development :: Libraries :: Python Modules",
]
scripts = { unstract-tool-gen = "unstract.sdk.scripts.tool_gen:main" }

[project.optional-dependencies]
# Pinning boto3 to 1.34.x for remote storage compatibility.
aws = ["s3fs[boto3]~=2024.10.0", "boto3~=1.34.131"]
gcs = ["gcsfs~=2024.10.0"]
azure = ["adlfs~=2024.7.0"]

[tool.pdm.dev-dependencies]
docs = [ "lazydocs~=0.4.8" ]
test = [
    "parameterized==0.9.0",
    "pytest==8.3.3",
    "pytest-mock==3.14.0",
    "gcsfs==2024.10.0",
    "s3fs==2024.10.0",
    "adlfs~=2024.7.0"
]
lint = [
    "autopep8~=2.0.2",
    "black~=23.3.0",
    "docutils~=0.20.1",
    "flake8~=6.0.0",
    "flake8-pyproject~=1.2.2",
    "isort~=5.12.0",
    "pre-commit~=3.3.1",
    "yamllint>=1.35.1",
]

[tool.isort]
line_length = 88
multi_line_output = 3
include_trailing_comma = true
force_grid_wrap = 0
use_parentheses = true
ensure_newline_before_comments = true
profile = "black"

[tool.pdm.build]
includes = ["src"]
package-dir = "src"

[tool.pdm.version]
source = "file"
path = "src/unstract/sdk/__init__.py"

# Adding the following override to resolve dependency version
# for environs. Otherwise, it stays stuck while resolving pins
[tool.pdm.resolution.overrides]
grpcio = "1.62.3"
grpcio-tools = "1.62.3"
grpcio-health-checking = "1.62.3"


================================================
File: .pre-commit-config.yaml
================================================
---
# See https://pre-commit.com for more information
# See https://pre-commit.com/hooks.html for more hooks
# - Added pkgs feature flag auto generated code to flake8 exclude list
# Force all unspecified python hooks to run python 3.10
default_language_version:
  python: python3.9
default_stages:
  - commit
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
        exclude_types:
          - "markdown"
      - id: end-of-file-fixer
      - id: check-yaml
        args: [--unsafe]
      - id: check-added-large-files
        args: ["--maxkb=10240"]
      - id: check-case-conflict
      - id: check-docstring-first
      - id: check-ast
      - id: check-json
        exclude: ".vscode/launch.json"
      - id: check-executables-have-shebangs
      - id: check-shebang-scripts-are-executable
      - id: check-toml
      - id: debug-statements
      - id: detect-private-key
      - id: check-merge-conflict
      - id: check-symlinks
      - id: destroyed-symlinks
      - id: forbid-new-submodules
      - id: mixed-line-ending
      - id: no-commit-to-branch
  - repo: https://github.com/adrienverge/yamllint
    rev: v1.35.1
    hooks:
      - id: yamllint
        args: ["-d", "relaxed"]
        language: system
  - repo: https://github.com/rhysd/actionlint
    rev: v1.6.26
    hooks:
      - id: actionlint-docker
        args: [-ignore, 'label ".+" is unknown']
  - repo: https://github.com/psf/black
    rev: 24.2.0
    hooks:
      - id: black
        args: [--config=pyproject.toml, -l 88]
        language: system
        exclude: |
          (?x)^(
              pkgs/unstract-flags/src/unstract/flags/evaluation_.*\.py|
          )$
  - repo: https://github.com/pycqa/flake8
    rev: 7.0.0
    hooks:
      - id: flake8
        args: [--max-line-length=88]
        exclude: |
          (?x)^(
              .*migrations/.*\.py|
              unstract-core/tests/.*|
              pkgs/unstract-flags/src/unstract/flags/evaluation_.*\.py|
          )$
  - repo: https://github.com/pycqa/isort
    rev: 5.13.2
    hooks:
      - id: isort
        files: "\\.(py)$"
        args:
          [
            "--profile",
            "black",
            "--filter-files",
            --settings-path=pyproject.toml,
          ]
  - repo: https://github.com/hadialqattan/pycln
    rev: v2.4.0
    hooks:
      - id: pycln
        args: [--config=pyproject.toml]
  - repo: https://github.com/pycqa/docformatter
    rev: v1.7.5
    hooks:
      - id: docformatter
  # - repo: https://github.com/MarcoGorelli/absolufy-imports
  #   rev: v0.3.1
  #   hooks:
  #     - id: absolufy-imports
  - repo: https://github.com/asottile/pyupgrade
    rev: v3.15.0
    hooks:
      - id: pyupgrade
        entry: pyupgrade --py39-plus --keep-runtime-typing
        types:
          - python
  - repo: https://github.com/gitleaks/gitleaks
    rev: v8.18.2
    hooks:
      - id: gitleaks
  - repo: https://github.com/asottile/yesqa
    rev: v1.5.0
    hooks:
      - id: yesqa
  # TODO: Uncomment after typing the SDK
  # - repo: https://github.com/pre-commit/mirrors-mypy
  #   rev: v1.8.0
  #   hooks:
  #     - id: mypy
  #       language: system
  #       entry: mypy .
  #       pass_filenames: false
  #       # IMPORTANT!
  #       # Keep args same as tool.mypy section in pyproject.toml
  #       args:
  #         [
  #           --allow-subclassing-any,
  #           --allow-untyped-decorators,
  #           --check-untyped-defs,
  #           --exclude, ".*migrations/.*.py",
  #           --exclude, "backend/prompt/.*",
  #           --exclude, "document_display_service/.*",
  #           --exclude, "pkgs/unstract-connectors/tests/.*",
  #           --exclude, "pkgs/unstract-core/.*",
  #           --exclude, "pkgs/unstract-flags/src/unstract/flags/.*",
  #           --exclude, "sdks/.*",
  #           --exclude, "unstract-document-service/.*",
  #           --exclude, "__pypackages__/.*",
  #           --follow-imports, "silent",
  #           --ignore-missing-imports,
  #           --implicit-reexport,
  #           --pretty,
  #           --python-version=3.9,
  #           --show-column-numbers,
  #           --show-error-codes,
  #           --strict,
  #           --warn-redundant-casts,
  #           --warn-return-any,
  #           --warn-unreachable,
  #           --warn-unused-configs,
  #           --warn-unused-ignores,
  #         ]
  - repo: https://github.com/igorshubovych/markdownlint-cli
    rev: v0.39.0
    hooks:
      - id: markdownlint
        args: [--disable, MD013]
      - id: markdownlint-fix
        args: [--disable, MD013]
  - repo: https://github.com/pdm-project/pdm
    rev: 2.12.3
    hooks:
      - id: pdm-lock-check


================================================
File: docs/README.md
================================================
# Documentation generation

[Lazydocs](https://github.com/ml-tooling/lazydocs) is used for markdown generation from the Google style python docstrings.

Install this dependency along with the SDK using
```
pip install -e ".[docs]"
```

Execute the below command from [sdks](/sdks/) in order to generate the markdown files into [API reference](/sdks/docs/site/docs/API%20Reference/).
```
lazydocs src/unstract/sdk \
--output-path ./docs/site/docs/API\ Reference/ \
--src-base-url https://github.com/Zipstack/unstract/tree/development
```

**NOTE:** There exists issues with lazydocs' generation and all image tags in the generated markdown need to be removed for
the being. A find and replace with the regex of `<a href=.*` into an empty string might help.

Test by building the site with the following command
```
npm run serve -- --build --port 4500 --host 127.0.0.1
```


================================================
File: docs/site/README.md
================================================
# Website

This website is built using [Docusaurus 2](https://docusaurus.io/), a modern static website generator.

### Installation

```
$ yarn
```

### Local Development

```
$ yarn start
```

This command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server.

### Build

```
$ yarn build
```

This command generates static content into the `build` directory and can be served using any static contents hosting service.

### Deployment

Using SSH:

```
$ USE_SSH=true yarn deploy
```

Not using SSH:

```
$ GIT_USER=<Your GitHub username> yarn deploy
```

If you are using GitHub pages for hosting, this command is a convenient way to build the website and push to the `gh-pages` branch.


================================================
File: docs/site/babel.config.js
================================================
module.exports = {
  presets: [require.resolve('@docusaurus/core/lib/babel/preset')],
};


================================================
File: docs/site/docusaurus.config.js
================================================
// @ts-check
// Note: type annotations allow type checking and IDEs autocompletion

const lightCodeTheme = require('prism-react-renderer/themes/github');
const darkCodeTheme = require('prism-react-renderer/themes/dracula');

/** @type {import('@docusaurus/types').Config} */
const config = {
  title: 'Unstract Developer Docs',
  tagline: 'Enable enterprise workflow automations powered by LLMs with No-Code',
  favicon: 'img/favicon.ico',

  // Set the production url of your site here
  url: 'https://developer.unstract.com',
  // Set the /<baseUrl>/ pathname under which your site is served
  // For GitHub pages deployment, it is often '/<projectName>/'
  baseUrl: '/',

  // GitHub pages deployment config.
  // If you aren't using GitHub pages, you don't need these.
  organizationName: 'unstract', // Usually your GitHub org/user name.
  projectName: 'unstract-sdks-doc', // Usually your repo name.

  onBrokenLinks: 'throw',
  onBrokenMarkdownLinks: 'warn',

  // Even if you don't use internalization, you can use this field to set useful
  // metadata like html lang. For example, if your site is Chinese, you may want
  // to replace "en" with "zh-Hans".
  i18n: {
    defaultLocale: 'en',
    locales: ['en'],
  },

  presets: [
    [
      'classic',
      /** @type {import('@docusaurus/preset-classic').Options} */
      ({
        docs: {
          sidebarPath: require.resolve('./sidebars.js'),
          // Please change this to your repo.
          // Remove this to remove the "edit this page" links.
          editUrl:
            'https://github.com/Zipstack/unstract/tree/main',
        },
        theme: {
          customCss: require.resolve('./src/css/custom.css'),
        },
      }),
    ],
  ],

  themeConfig:
    /** @type {import('@docusaurus/preset-classic').ThemeConfig} */
    ({
      // Replace with your project's social card
      image: 'img/logo.svg',
      navbar: {
        title: 'Unstract Developer Docs',
        logo: {
          alt: 'Unstract SDKs Logo',
          src: 'img/logo.svg',
        },
        items: [
          {
            type: 'docSidebar',
            sidebarId: 'tutorialSidebar',
            position: 'left',
            label: 'Home',
          }
        ],
      },
      footer: {
        style: 'dark',
        links: [
          {
            title: 'Docs',
            items: [
              {
                label: 'Documentation',
                to: '/docs/introduction',
              },
            ],
          },
          {
            title: 'Community',
            items: [
              {
                label: 'Twitter',
                href: 'https://twitter.com/ZipstackHQ',
              },
            ],
          },
          {
            title: 'More',
            items: [
              // {
              //   label: 'Blog',
              //   to: '/blog',
              // },
              {
                label: 'Home',
                href: 'https://unstract.com',
              },
            ],
          },
        ],
        copyright: `Copyright © ${new Date().getFullYear()} Zipstack, Inc.`,
      },
      prism: {
        theme: lightCodeTheme,
        darkTheme: darkCodeTheme,
      },
    }),
};

module.exports = config;


================================================
File: docs/site/package.json
================================================
{
  "name": "site",
  "version": "0.0.0",
  "private": true,
  "scripts": {
    "docusaurus": "docusaurus",
    "start": "docusaurus start",
    "build": "docusaurus build",
    "swizzle": "docusaurus swizzle",
    "deploy": "docusaurus deploy",
    "clear": "docusaurus clear",
    "serve": "docusaurus serve",
    "write-translations": "docusaurus write-translations",
    "write-heading-ids": "docusaurus write-heading-ids"
  },
  "dependencies": {
    "@docusaurus/core": "2.4.3",
    "@docusaurus/preset-classic": "2.4.3",
    "@mdx-js/react": "^1.6.22",
    "clsx": "^1.2.1",
    "prism-react-renderer": "^1.3.5",
    "react": "^17.0.2",
    "react-dom": "^17.0.2"
  },
  "devDependencies": {
    "@docusaurus/module-type-aliases": "2.4.3"
  },
  "browserslist": {
    "production": [
      ">0.5%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 1 chrome version",
      "last 1 firefox version",
      "last 1 safari version"
    ]
  },
  "engines": {
    "node": ">=16.14"
  }
}


================================================
File: docs/site/sidebars.js
================================================
/**
 * Creating a sidebar enables you to:
 - create an ordered group of docs
 - render a sidebar for each doc of that group
 - provide next/previous navigation

 The sidebars can be generated from the filesystem, or explicitly defined here.

 Create as many sidebars as you want.
 */

// @ts-check

/** @type {import('@docusaurus/plugin-content-docs').SidebarsConfig} */
const sidebars = {
  // By default, Docusaurus generates a sidebar from the docs folder structure
  tutorialSidebar: [{type: 'autogenerated', dirName: '.'}],

  // But you can create a sidebar manually
  /*
  tutorialSidebar: [
    'intro',
    'hello',
    {
      type: 'category',
      label: 'Tutorial',
      items: ['tutorial-basics/create-a-document'],
    },
  ],
   */
};

module.exports = sidebars;


================================================
File: docs/site/.gitignore
================================================
# Dependencies
/node_modules

# Production
/build

# Generated files
.docusaurus
.cache-loader

# Misc
.DS_Store
.env.local
.env.development.local
.env.test.local
.env.production.local

npm-debug.log*
yarn-debug.log*
yarn-error.log*


================================================
File: docs/site/docs/Introduction.md
================================================
---
sidebar_position: 1
---

# Welcome to the Unstract SDK documentation

Large Language Models (LLMs) are a generation-defining technology that can be used to solve critical business process problems that currently require a human in the loop. While LLMs can be leveraged to speed up tasks across the enterprise, technology across the LLM ecosystem is fast developing, oft-changing and not to mention vast and unwieldy.

## A No-Code Approach
Unstract takes a no-code approach, hiding a lot of technology complexity for you, letting you take full advantage of new LLM technology, while it takes care of the heavy lifting.
* Unstract uses the state-of-the-art techniques like Retrieval Augmented Generation (RAG) with the ability for you to easily switch out one LLM, Embedding Model or Vector Database for another—as simple as changing values in a drop-down control.
* Using LLMWhisperer, our proprietary retriever which uses advanced techniques to translate PDF, Word documents and other custom unstructured sources into input that is optimized for LLMs to understand, we're able to produce industry-leading results for various LLM applications across the enterprise.

## No-Code Prompt Studio
Easily create new Tools that are able to handle new classes of unstructured documents with either our Python SDK or with the no-code Prompt Studio.

## Built for the Enterprise

Unstract also makes it easy to implement common enterprise use case patterns when it comes to dealing with unstructured data—powered by Large Language Models, such as:

* **Unstructured Data APIs:** When there's a need to integrate workflows that involve unstructured data into your existing applications, it is possible to invoke APIs you create and deploy on the Unstract platform to help automate these workflows. You can create no-code Workflows that help structure a variety of document types and deploy them as APIs, which get their own unique endpoint URLs. Make your products more powerful, eliminate slowdowns by automating workflows that previously needed a human to step in to review or to help structure unstructured documents, killing your UX and potentially introducing more errors into the process.
* **Unstructured data ETL worflows:** These are like normal ETL worflows, except that data sources are complex documents that until now required a human in the loop to help structure. This opens up the whole world of unstructured data to your existing data stack. Once the data is in your warehouse or database, you can use familiar tools to transform, govern or visualize the data. Unstract allows unstructured data to be treated like any other data from a structure source leveraging the power of LLMs.
* **Q&A on your own data:** While the previous 2 use cases mentioned are useful more in situations where automation is required, there are times when you need human interactivity over your internal data. For times like these, you can create no-code Workflows in Unstract and deploy them as Q&A Apps. Each one gets its own unique URL, which can then be used to ask questions over your own data, no matter where it lives.

## Welcome to the Unstract Python Tool SDK
While Unstract ships with a variety of tools to parse various document types and connect to various data sources and destinations, there might be times when you need to extend the platform's capabilities to support custom document types, data sources or destinations. 

The Unstract platform allows users to create and use their own custom tools in no-code Workflows. The Unstract SDK provides a convenient interface to the platform to create your own tools. These tools can be invoked as part of Workflows created in the platform.

The platform uses Large Language Models (LLMs) extensively to manage many of the comprehension and reasoning task involved in these worflows and applications. The platform and the SDK hides the complexities involved in these operations to both the end user and most tool creators to achieve complex transformations and processing of unstructured data.


================================================
File: docs/site/docs/Unstract platform components.md
================================================
---
sidebar_position: 2
---

The following diagram shows the overall system components of the Unstract platform. Please note that this documentation will focus on the SDK component of the platform. The SDK component is the component that allows the users to create their own Tools for use in the platform. The SDK and the platform abstracts away the complexities of the underlying components and allows the users to focus on the business logic of their Tools.

As you can see from the diagram, the SDKs can be used to create **Tools** and **Applications**. Tools can be used in the Workflows created in the platform. Applications or Apps are standalone and secure. Apps can be used to provide Q&A (RAG) capabilities over unstructured sources. Apps are not limited to Q&A and can be used to provide any other capabilities that can use the platform's indexer and related LLM features.

![platform components](/img/page_content/platform_components.png)


================================================
File: docs/site/docs/API Reference/cache.md
================================================
<!-- markdownlint-disable -->


# <kbd>module</kbd> `cache`






---


## <kbd>class</kbd> `UnstractToolCache`
Class to handle caching for Unstract Tools. 



**Notes:**

> - PLATFORM_SERVICE_API_KEY environment variable is required. 


### <kbd>method</kbd> `__init__`

```python
__init__(
    tool: UnstractAbstractTool,
    platform_host: str,
    platform_port: int
) → None
```



**Args:**
 
 - <b>`tool`</b> (UnstractAbstractTool):  Instance of UnstractAbstractTool 
 - <b>`platform_host`</b> (str):  The host of the platform. 
 - <b>`platform_port`</b> (int):  The port of the platform. 



**Notes:**

> - PLATFORM_SERVICE_API_KEY environment variable is required. - The platform_host and platform_port are the host and port of the platform service. 




---


### <kbd>method</kbd> `delete`

```python
delete(key: str) → bool
```

Deletes the value for a key in the cache. 



**Args:**
 
 - <b>`key`</b> (str):  The key. 



**Returns:**
 
 - <b>`bool`</b>:  Whether the operation was successful. 

---


### <kbd>method</kbd> `get`

```python
get(key: str) → Optional[Any]
```

Gets the value for a key in the cache. 



**Args:**
 
 - <b>`key`</b> (str):  The key. 



**Returns:**
 
 - <b>`str`</b>:  The value. 

---


### <kbd>method</kbd> `set`

```python
set(key: str, value: str) → bool
```

Sets the value for a key in the cache. 



**Args:**
 
 - <b>`key`</b> (str):  The key. 
 - <b>`value`</b> (str):  The value. 



**Returns:**
 
 - <b>`bool`</b>:  Whether the operation was successful. 




---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/API Reference/constants.md
================================================
<!-- markdownlint-disable -->



# <kbd>module</kbd> `constants`






---



## <kbd>class</kbd> `PlatformServiceKeys`








---



## <kbd>class</kbd> `ConnectorKeys`








---



## <kbd>class</kbd> `ConnectorType`








---



## <kbd>class</kbd> `LogType`








---



## <kbd>class</kbd> `LogStage`








---



## <kbd>class</kbd> `LogState`
State of logs INPUT_UPDATE tag for update the FE input component OUTPUT_UPDATE tag for update the FE output component. 





---



## <kbd>class</kbd> `Connector`








---



## <kbd>class</kbd> `Command`







---



### <kbd>classmethod</kbd> `static_commands`

```python
static_commands() → set[str]
```






---



## <kbd>class</kbd> `LogLevel`
An enumeration. 







---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/API Reference/dbs.md
================================================
<!-- markdownlint-disable -->



# <kbd>module</kbd> `dbs`




**Global Variables**
---------------
- **connectors**


---



## <kbd>class</kbd> `UnstractToolDB`
Class to handle DB connectors for Unstract Tools. 



**Notes:**

> - PLATFORM_SERVICE_API_KEY environment variable is required. 



### <kbd>method</kbd> `__init__`

```python
__init__(
    tool: UnstractAbstractTool,
    platform_host: str,
    platform_port: str
) → None
```



**Args:**
 
 - <b>`tool`</b> (UnstractAbstractTool):  Instance of UnstractAbstractTool 
 - <b>`platform_host`</b> (str):  Host of platform service 
 - <b>`platform_port`</b> (str):  Port of platform service 



**Notes:**

> - PLATFORM_SERVICE_API_KEY environment variable is required. - The platform_host and platform_port are the host and port of the platform service. 




---



### <kbd>method</kbd> `get_engine`

```python
get_engine(tool_instance_id: str) → Any
```

Get DB engine  1. Get the connection settings from platform service  using the tool_instance_id  2. Create UnstractFileSystem based object using the settings  2.1 Find the type of the database (From Connector Registry)  2.2 Create the object using the type  (derived class of UnstractFileSystem)  3. Send Object.get_fsspec_fs() to the caller 



**Args:**
 
 - <b>`tool_instance_id`</b> (str):  tool Instance Id 



**Returns:**
 
 - <b>`Any`</b>:  _description_ 




---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/API Reference/documents.md
================================================
<!-- markdownlint-disable -->



# <kbd>module</kbd> `documents`






---



## <kbd>class</kbd> `UnstractToolDocs`
Class to handle documents for Unstract Tools. 



**Notes:**

> - PLATFORM_SERVICE_API_KEY environment variable is required. 



### <kbd>method</kbd> `__init__`

```python
__init__(tool: UnstractAbstractTool, platform_host: str, platform_port: int)
```



**Args:**
 
 - <b>`tool`</b> (UnstractAbstractTool):  Instance of UnstractAbstractTool 
 - <b>`platform_host`</b> (str):  The host of the platform. 
 - <b>`platform_port`</b> (int):  The port of the platform. 



**Notes:**

> - PLATFORM_SERVICE_API_KEY environment variable is required. - The platform_host and platform_port are the host and port of the platform service. 




---



### <kbd>method</kbd> `delete`

```python
delete(project_id: str, unique_file_id: str)
```

Deletes data for a document from the platform. 



**Notes:**

> - This method is used internally. Do not call this method directly unless you know what you are doing. 
>

**Args:**
 
 - <b>`project_id`</b> (str):  The project id. 
 - <b>`unique_file_id`</b> (str):  The unique file id. 



**Returns:**
 
 - <b>`bool`</b>:  Whether the operation was successful. 

---



### <kbd>method</kbd> `get`

```python
get(project_id: str, unique_file_id: str) → Optional[dict]
```

Retrieves data for a document from the platform. 



**Args:**
 
 - <b>`project_id`</b> (str):  The project id. 
 - <b>`unique_file_id`</b> (str):  The unique file id. 



**Returns:**
 
 - <b>`Optional[dict]`</b>:  The data for the document. 

---



### <kbd>method</kbd> `index_file`

```python
index_file(
    project_id: str,
    embedding_type: str,
    vector_db: str,
    file_path: str,
    overwrite: bool = False
) → dict
```

Indexes a file to the platform. 



**Args:**
 
 - <b>`project_id`</b> (str):  The project id. 
 - <b>`embedding_type`</b> (str):  The embedding type.  Supported values: 
            - "Azure OpenAI" 
 - <b>`vector_db`</b> (str):  The vector db.  Supported values: 
            - "Postgres pg_vector" 
 - <b>`file_path`</b> (str):  The path to the file to index. 
 - <b>`overwrite`</b> (bool):  Whether to overwrite the file if it already exists.  The default is False. 



**Returns:**
 
 - <b>`dict`</b>:  The result of the indexing operation. 



**Notes:**

> Sample return dict: { "status": "OK", "error": "", "cost": 746, "unique_file_id": "9b44826ff1ed4dfd5dda762776acd4dd" } 

---



### <kbd>method</kbd> `insert`

```python
insert(
    project_id: str,
    unique_file_id: str,
    filename: str,
    filetype: str,
    summary: str,
    embedding_tokens: int,
    llm_tokens: int,
    vector_db: str
)
```

Inserts data for a document into the platform. 



**Notes:**

> - This method is typically called by the tool's index() method. It is not typically called directly. 
>

**Args:**
 
 - <b>`project_id`</b> (str):  The project id. 
 - <b>`unique_file_id`</b> (str):  The unique file id. 
 - <b>`filename`</b> (str):  The filename. 
 - <b>`filetype`</b> (str):  The filetype. Example: "application/pdf" 
 - <b>`summary`</b> (str):  The summary. Note: Currently not used. 
 - <b>`embedding_tokens`</b> (int):  The number of tokens used for the embedding. 
 - <b>`llm_tokens`</b> (int):  The number of tokens used for the LLM. 
 - <b>`vector_db`</b> (str):  The vector db.  Supported values: 
            - "Postgres pg_vector" 



**Returns:**
 
 - <b>`bool`</b>:  Whether the operation was successful. 




---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/API Reference/file_system.md
================================================
<!-- markdownlint-disable -->



# <kbd>module</kbd> `file_system`




**Global Variables**
---------------
- **connectors**


---



## <kbd>class</kbd> `UnstractToolFileSystem`
Class to handle File connectors for Unstract Tools. 



**Notes:**

> - PLATFORM_SERVICE_API_KEY environment variable is required. 



### <kbd>method</kbd> `__init__`

```python
__init__(
    tool: UnstractAbstractTool,
    platform_host: str,
    platform_port: str
) → None
```



**Args:**
 
 - <b>`tool`</b> (UnstractAbstractTool):  Instance of UnstractAbstractTool 
 - <b>`platform_host`</b> (str):  Host of platform service 
 - <b>`platform_port`</b> (str):  Port of platform service 



**Notes:**

> - PLATFORM_SERVICE_API_KEY environment variable is required. - The platform_host and platform_port are the host and port of the platform service. 




---



### <kbd>method</kbd> `get_fsspec`

```python
get_fsspec(
    tool_instance_id: str,
    connector_type: str = 'OUTPUT'
) → Optional[AbstractFileSystem]
```

Get FsSpec for fileSystem  1. Get the connection settings from platform service  using the tool_instance_id  2. Create UnstractFileSystem based object using the settings  2.1 Find the type of the database (From Connector Registry)  2.2 Create the object using the type  (derived class of UnstractFileSystem)  3. Send Object.get_fsspec_fs() to the caller 



**Args:**
 
 - <b>`tool_instance_id`</b> (str):  tool Instance Id 
 - <b>`connector_type`</b> (str, optional):  _description_.  Defaults to ConnectorType.OUTPUT. 



**Returns:**
 
 - <b>`Any`</b>:  _description_ 

---



### <kbd>method</kbd> `get_fsspec_fs`

```python
get_fsspec_fs(
    tool: UnstractAbstractTool,
    tool_instance_id: str,
    fileType: str
) → AbstractFileSystem
```

Get fileSystem spec by the help of unstract DB tool. 



**Args:**
 
 - <b>`tool_instance_id`</b> (str):  ID of the tool instance 
 - <b>`fileType`</b> (str):  INPUT/OUTPUT 
 - <b>`tool`</b> (UnstractAbstractTool):  Instance of UnstractAbstractTool Required env variables: 
 - <b>`PLATFORM_HOST`</b>:  Host of platform service 
 - <b>`PLATFORM_PORT`</b>:  Port of platform service 

**Returns:**
 
 - <b>`Any`</b>:  engine 




---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/API Reference/helper.md
================================================
<!-- markdownlint-disable -->



# <kbd>module</kbd> `helper`






---



## <kbd>class</kbd> `SdkHelper`






### <kbd>method</kbd> `__init__`

```python
__init__() → None
```








---



### <kbd>method</kbd> `get_platform_base_url`

```python
get_platform_base_url(platform_host: str, platform_port: str) → str
```

Make base url from host and port. 



**Args:**
 
 - <b>`platform_host`</b> (str):  Host of platform service 
 - <b>`platform_port`</b> (str):  Port of platform service 



**Returns:**
 
 - <b>`str`</b>:  URL to the platform service 




---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/API Reference/llm.md
================================================
<!-- markdownlint-disable -->



# <kbd>module</kbd> `llm`






---



## <kbd>class</kbd> `UnstractToolLLM`
Class to handle LLMs for Unstract Tools. 



### <kbd>method</kbd> `__init__`

```python
__init__(tool: UnstractAbstractTool, llm_id: str)
```



**Notes:**

> - "Azure OpenAI" : Environment variables required OPENAI_API_KEY, OPENAI_API_BASE, OPENAI_API_VERSION, OPENAI_API_ENGINE, OPENAI_API_MODEL 
>

**Args:**
 
 - <b>`tool`</b> (UnstractAbstractTool):  Instance of UnstractAbstractTool 
 - <b>`llm_id`</b> (str):  The id of the LLM to use.  Supported values: 
            - "Azure OpenAI" 




---



### <kbd>method</kbd> `get_callback_manager`

```python
get_callback_manager() → Optional[CallbackManager]
```

Returns the Callback Manager object for the tool. 



**Returns:**
 
 - <b>`Optional[CallbackManager]`</b>:  The Callback Manager object for the tool.  (llama_index.callbacks.CallbackManager) 

---



### <kbd>method</kbd> `get_llm`

```python
get_llm() → Optional[LLM]
```

Returns the LLM object for the tool. 



**Returns:**
 
 - <b>`Optional[LLM]`</b>:  The LLM object for the tool. (llama_index.llms.base.LLM) 

---



### <kbd>method</kbd> `get_max_tokens`

```python
get_max_tokens(reserved_for_output: int = 0) → int
```

Returns the maximum number of tokens that can be used for the LLM. 



**Args:**
 
 - <b>`reserved_for_output`</b> (int):  The number of tokens reserved for the output.  The default is 0. 



**Returns:**
 
 - <b>`int`</b>:  The maximum number of tokens that can be used for the LLM. 

---



### <kbd>method</kbd> `get_usage_counts`

```python
get_usage_counts() → dict[str, int]
```

Returns the usage counts for the tool. 



**Returns:**
 
 - <b>`dict`</b>:  The usage counts for the tool. 
        - embedding_tokens: The number of tokens used for the embedding. 
        - llm_prompt_tokens: The number of tokens used for the LLM prompt. 
        - llm_completion_tokens:  The number of tokens used for the LLM completion. 
        - total_llm_tokens: The total number of tokens used for the LLM. 

---



### <kbd>method</kbd> `reset_usage_counts`

```python
reset_usage_counts() → None
```

Resets the usage counts for the tool. 



**Returns:**
  None 

---



### <kbd>method</kbd> `stream_usage_counts`

```python
stream_usage_counts() → None
```

Stream all usage costs. 

This function retrieves the usage counts and stream the costs associated with it. 



**Returns:**
  None 




---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/API Reference/platform.md
================================================
<!-- markdownlint-disable -->



# <kbd>module</kbd> `platform`


---





## <kbd>class</kbd> `UnstractPlatformBase`
Base class to handle interactions with Unstract's platform service. 



**Notes:**

> - PLATFORM_SERVICE_API_KEY environment variable is required. 



### <kbd>method</kbd> `__init__`

```python
__init__(
    tool: UnstractAbstractTool,
    platform_host: str,
    platform_port: str
) → None
```



**Args:**
 
 - <b>`tool`</b> (UnstractAbstractTool):  Instance of UnstractAbstractTool 
 - <b>`platform_host`</b> (str):  Host of platform service 
 - <b>`platform_port`</b> (str):  Port of platform service 



**Notes:**

> - PLATFORM_SERVICE_API_KEY environment variable is required. 





---



## <kbd>class</kbd> `UnstractPlatform`
Implementation of `UnstractPlatformBase` to interact with platform service. 



**Notes:**

> - PLATFORM_SERVICE_API_KEY environment variable is required. 



### <kbd>method</kbd> `__init__`

```python
__init__(tool: UnstractAbstractTool, platform_host: str, platform_port: str)
```

Constructor of the implementation of `UnstractPlatformBase` 



**Args:**
 
 - <b>`tool`</b> (UnstractAbstractTool):  Instance of UnstractAbstractTool 
 - <b>`platform_host`</b> (str):  Host of platform service 
 - <b>`platform_port`</b> (str):  Port of platform service 




---



### <kbd>method</kbd> `get_platform_details`

```python
get_platform_details() → Optional[dict[str, Any]]
```

Obtains platform details associated with the platform key. 

Currently helps fetch organization ID related to the key. 



**Returns:**
 
 - <b>`Optional[dict[str, Any]]`</b>:  Dictionary containing the platform details 




---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/API Reference/tool.base.md
================================================
<!-- markdownlint-disable -->



# <kbd>module</kbd> `tool.base`






---



## <kbd>class</kbd> `UnstractAbstractTool`
Abstract class for Unstract tools. 



### <kbd>method</kbd> `__init__`

```python
__init__(log_level: str = <LogLevel.INFO: 'INFO'>) → None
```

Creates an UnstractTool. 



**Args:**
 
 - <b>`log_level`</b> (str):  Log level for the tool  Can be one of INFO, DEBUG, WARN, ERROR, FATAL. 




---



### <kbd>method</kbd> `elapsed_time`

```python
elapsed_time() → float
```

Returns the elapsed time since the tool was created. 

---



### <kbd>classmethod</kbd> `from_tool_args`

```python
from_tool_args(args: list) → UnstractAbstractTool
```

Builder method to create a tool from args passed to a tool. 

Refer the tool's README to know more about the possible args 



**Args:**
 
 - <b>`args`</b> (List[str]):  Arguments passed to a tool 



**Returns:**
 
 - <b>`UnstractAbstractTool`</b>:  Abstract base tool class 

---



### <kbd>method</kbd> `run`

```python
run(params: dict, settings: dict, workflow_id: str) → None
```

Implements RUN command for the tool. 



**Args:**
 
 - <b>`params`</b> (dict[str, Any]):  Params for the tool 
 - <b>`settings`</b> (dict[str, Any]):  Settings for the tool 
 - <b>`workflow_id`</b> (str):  Project GUID used during workflow execution 




---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/API Reference/tool.entrypoint.md
================================================
<!-- markdownlint-disable -->



# <kbd>module</kbd> `tool.entrypoint`






---



## <kbd>class</kbd> `ToolEntrypoint`
Class that contains methods for the entrypoint for a tool. 




---



### <kbd>method</kbd> `launch`

```python
launch(tool: UnstractAbstractTool, args: list) → None
```

Entrypoint function for a tool. 

It parses the arguments passed to a tool and executes the intended command. 



**Args:**
 
 - <b>`tool`</b> (UnstractAbstractTool):  Tool to execute 
 - <b>`args`</b> (List[str]):  Arguments passed to a tool 




---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/API Reference/tool.executor.md
================================================
<!-- markdownlint-disable -->



# <kbd>module</kbd> `tool.executor`






---



## <kbd>class</kbd> `ToolExecutor`
Takes care of executing a tool's intended command. 



### <kbd>method</kbd> `__init__`

```python
__init__(tool: UnstractAbstractTool) → None
```








---



### <kbd>method</kbd> `execute`

```python
execute(args: Namespace) → None
```

Executes the tool with the passed arguments. 



**Args:**
 
 - <b>`args`</b> (argparse.Namespace):  Parsed arguments to execute with 

---



### <kbd>method</kbd> `load_environment`

```python
load_environment(path: Optional[str] = None)
```

Loads env variables with python-dotenv. 



**Args:**
 
 - <b>`path`</b> (Optional[str], optional):  Path to the env file to load.  Defaults to None. 

---



### <kbd>method</kbd> `validate_and_get_params`

```python
validate_and_get_params(args: Namespace) → dict[str, Any]
```

Validates and obtains params for a tool. 

Validation is done against the tool's params based on its declared PROPERTIES 



**Args:**
 
 - <b>`args`</b> (argparse.Namespace):  Parsed arguments for a tool 



**Returns:**
 
 - <b>`dict[str, Any]`</b>:  Params JSON for a tool 

---



### <kbd>method</kbd> `validate_and_get_settings`

```python
validate_and_get_settings(args: Namespace) → dict[str, Any]
```

Validates and obtains settings for a tool. 

Validation is done against the tool's settings based on its declared SPEC 



**Args:**
 
 - <b>`args`</b> (argparse.Namespace):  Parsed arguments for a tool 



**Returns:**
 
 - <b>`dict[str, Any]`</b>:  Settings JSON for a tool 




---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/API Reference/tool.md
================================================
<!-- markdownlint-disable -->



# <kbd>module</kbd> `tool`








---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/API Reference/tool.mixin.md
================================================
<!-- markdownlint-disable -->



# <kbd>module</kbd> `tool.mixin`






---



## <kbd>class</kbd> `StaticCommandsMixin`
Mixin class to handle static commands for tools. 




---



### <kbd>method</kbd> `icon`

```python
icon(icon_file: str = 'config/icon.svg') → str
```

Returns the icon of the tool. 



**Args:**
 
 - <b>`icon_file`</b> (str):  The path to the icon file. The default is config/icon.svg. 

**Returns:**
 
 - <b>`str`</b>:  The icon of the tool. 

---



### <kbd>method</kbd> `properties`

```python
properties(properties_file: str = 'config/properties.json') → str
```

Returns the properties of the tool. 



**Args:**
 
 - <b>`properties_file`</b> (str):  The path to the properties file. The default is config/properties.json. 

**Returns:**
 
 - <b>`str`</b>:  The properties of the tool. 

---



### <kbd>method</kbd> `spec`

```python
spec(spec_file: str = 'config/spec.json') → str
```

Returns the JSON schema of the tool. 



**Args:**
 
 - <b>`spec_file`</b> (str):  The path to the JSON schema file. The default is config/spec.json. 

**Returns:**
 
 - <b>`str`</b>:  The JSON schema of the tool. 

---



### <kbd>method</kbd> `variables`

```python
variables(variables_file: str = 'config/runtime_variables.json') → str
```

Returns the JSON schema of the runtime variables. 



**Args:**
 
 - <b>`variables_file`</b> (str):  The path to the JSON schema file. The default is config/runtime_variables.json. 

**Returns:**
 
 - <b>`str`</b>:  The JSON schema for the runtime variables. 




---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/API Reference/tool.parser.md
================================================
<!-- markdownlint-disable -->



# <kbd>module</kbd> `tool.parser`






---



## <kbd>class</kbd> `ToolArgsParser`
Class to help with parsing arguments to a tool. 




---



### <kbd>classmethod</kbd> `get_log_level`

```python
get_log_level(args: list) → Optional[str]
```

Returns the log level for a tool. 

If its not present in the parsed arguments, `None` is returned. 



**Args:**
 
 - <b>`args`</b> (List[str]):  Command line arguments received by a tool 



**Returns:**
 
 - <b>`Optional[str]`</b>:  Log level of either INFO, DEBUG, WARN, ERROR, FATAL if  present in the args. Otherwise returns `None`. 

---



### <kbd>method</kbd> `parse_args`

```python
parse_args(args_to_parse: list) → Namespace
```

Helps parse arguments to a tool. 



**Args:**
 
 - <b>`args_to_parse`</b> (List[str]):  Command line arguments received by a tool 



**Returns:**
 
 - <b>`argparse.Namespace`</b>:  Parsed arguments 




---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/API Reference/tool.stream.md
================================================
<!-- markdownlint-disable -->



# <kbd>module</kbd> `tool.stream`






---



## <kbd>class</kbd> `StreamableBaseTool`
Helper class for streaming Unstract tool commands. 

A utility class to make writing Unstract tools easier. It provides methods to stream the JSON schema, properties, icon, log messages, cost, single step messages, and results using the Unstract protocol to stdout. 



### <kbd>method</kbd> `__init__`

```python
__init__(log_level: LogLevel = <LogLevel.INFO: 'INFO'>) → None
```



**Args:**
 
 - <b>`log_level`</b> (LogLevel):  The log level for filtering of log messages. The default is INFO.  Allowed values are DEBUG, INFO, WARN, ERROR, and FATAL. 




---



### <kbd>method</kbd> `get_env_or_die`

```python
get_env_or_die(env_key: str) → str
```

Returns the value of an env variable. 

If its empty or None, raises an error and exits 



**Args:**
 
 - <b>`env_key`</b> (str):  Key to retrieve 



**Returns:**
 
 - <b>`str`</b>:  Value of the env 

---



### <kbd>method</kbd> `handle_static_command`

```python
handle_static_command(command: str) → None
```

Handles a static command. 

Used to handle commands that do not require any processing. Currently, the only supported static commands are SPEC, PROPERTIES, VARIABLES and ICON. 

This is used by the Unstract SDK to handle static commands. It is not intended to be used by the tool. The tool stub will automatically handle static commands. 



**Args:**
 
 - <b>`command`</b> (str):  The static command. 

**Returns:**
 None 

---



### <kbd>method</kbd> `stream_cost`

```python
stream_cost(cost: float, cost_units: str, **kwargs) → None
```

Streams the cost of the tool using the Unstract protocol COST to stdout. 



**Args:**
 
 - <b>`cost`</b> (float):  The cost of the tool. 
 - <b>`cost_units`</b> (str):  The cost units of the tool. 
 - <b>`**kwargs`</b>:  Additional keyword arguments to include in the record. 

**Returns:**
 None 

---



### <kbd>method</kbd> `stream_error_and_exit`

```python
stream_error_and_exit(message: str) → None
```

Stream error log and exit. 



**Args:**
 
 - <b>`message`</b> (str):  Error message 

---



### <kbd>method</kbd> `stream_icon`

```python
stream_icon(icon: str) → None
```

Streams the icon of the tool using the Unstract protocol ICON to stdout. 



**Args:**
 
 - <b>`icon`</b> (str):  The icon of the tool. Typically returned by the icon() method. 

**Returns:**
 None 

---



### <kbd>method</kbd> `stream_log`

```python
stream_log(
    log: str,
    level: LogLevel = <LogLevel.INFO: 'INFO'>,
    stage: str = 'TOOL_RUN',
    **kwargs
) → None
```

Streams a log message using the Unstract protocol LOG to stdout. 



**Args:**
 
 - <b>`log`</b> (str):  The log message. 
 - <b>`level`</b> (LogLevel):  The log level. The default is INFO.  Allowed values are DEBUG, INFO, WARN, ERROR, and FATAL. 
 - <b>`stage`</b> (str):  LogStage from constant default Tool_RUN 

**Returns:**
 None 

---



### <kbd>method</kbd> `stream_properties`

```python
stream_properties(properties: str) → None
```

Streams the properties of the tool using the Unstract protocol PROPERTIES to stdout. 



**Args:**
 
 - <b>`properties`</b> (str):  The properties of the tool. Typically returned by the properties() method. 

**Returns:**
 None 

---



### <kbd>method</kbd> `stream_result`

```python
stream_result(result: dict, **kwargs) → None
```

Streams the result of the tool using the Unstract protocol RESULT to stdout. 



**Args:**
 
 - <b>`result`</b> (dict):  The result of the tool. Refer to the Unstract protocol for the format of the result. 
 - <b>`**kwargs`</b>:  Additional keyword arguments to include in the record. 

**Returns:**
 None 

---



### <kbd>method</kbd> `stream_single_step_message`

```python
stream_single_step_message(message: str, **kwargs) → None
```

Streams a single step message using the Unstract protocol SINGLE_STEP_MESSAGE to stdout. 



**Args:**
 
 - <b>`message`</b> (str):  The single step message. 
 - <b>`**kwargs`</b>:  Additional keyword arguments to include in the record. 

**Returns:**
 None 

---



### <kbd>method</kbd> `stream_spec`

```python
stream_spec(spec: str) → None
```

Streams JSON schema of the tool using the Unstract protocol SPEC to stdout. 



**Args:**
 
 - <b>`spec`</b> (str):  The JSON schema of the tool. Typically returned by the spec() method. 



**Returns:**
 None 

---



### <kbd>method</kbd> `stream_update`

```python
stream_update(message: str, component: str, state: str, **kwargs) → None
```

Streams a log message using the Unstract protocol UPDATE to stdout. 



**Args:**
 
 - <b>`message`</b> (str):  The log message. 
 - <b>`component`</b> (str):  Component to update 
 - <b>`state`</b> (str):  LogState from constant 

---



### <kbd>method</kbd> `stream_variables`

```python
stream_variables(variables: str) → None
```

Streams JSON schema of the tool's variables using the Unstract protocol VARIABLES to stdout. 



**Args:**
 
 - <b>`variables`</b> (str):  The tool's runtime variables. Typically returned by the spec() method. 



**Returns:**
 None 




---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/API Reference/utils.md
================================================
<!-- markdownlint-disable -->



# <kbd>module</kbd> `utils`






---



## <kbd>class</kbd> `ToolUtils`
Class containing utility methods. 




---



### <kbd>method</kbd> `hash_str`

```python
hash_str(string_to_hash: str, hash_method: str = 'md5') → str
```

Computes the hash for a given input string. 

Useful to hash strings needed for caching and other purposes. Hash method defaults to "md5" 



**Args:**
 
 - <b>`string_to_hash`</b> (str):  String to be hashed 
 - <b>`hash_method`</b> (str):  Hash hash_method to use, supported ones 
        - "md5" 



**Returns:**
 
 - <b>`str`</b>:  Hashed string 




---

_This file was automatically generated via [lazydocs](https://github.com/ml-tooling/lazydocs)._


================================================
File: docs/site/docs/Tools/1_Overview.md
================================================
![tools overview](/img/page_content/tools-overview.drawio.png)

The diagram shows how tools fit into the worflows enabled by the Unstract platform. The sources and sinks are configured in the workflow setup in the UI.

> Note: The worflow can have any number of tools chained together. The example shows 3 tools chained together. And in most real world use cases a worflow might not sink data to both unstrucured and structured sinks

Please note that:

- A "Tool" with respect to the Unstract platform is a generic concept and can be used to perform any kind of processing. It could do anything from parsing a PDF file to performing complex NLP tasks. It could even be used to just clean up data before passing it to the next tool in the worflow.
- Tools are run sequentially in the order they are chained in the worflow
- If the previous tool generates multiple outputs (array), all the tools after it is run for each of the outputs
- Tools need not use LLMs and Vector DBs. They can be used to perform any kind of processing. However, the platform provides a convenient interface to use LLMs and Vector DBs. The tools can use these interfaces to perform complex NLP tasks and to store and retrieve data from Vector DBs.

### Examples of tools which can be created with the SDK

>All the builtin tools in the platform are built using the same SDK provided.

- **Directory readers** These tools can read a directory and create an array of file paths. This array can be used as input to the next tool in the worflow. This is useful when the worflow needs to process multiple files in a directory. *Most* workflows will have atleast one of these tools as the starting point. The setting for these tools can include file filters, mazimum number of files to read, etc.
- **Indexers** These tools can be used to index (embeddings) documents supplied by directory readers. The builtin indexer tool is fully functional but if you need a more complex of powerful indexer, you can build your own complex indexing tool using the SDK. The generated index can be used to power RAG (Q&A) based applications.
- **Traditional IDP** These tools can be used to perform traditional IDP tasks like OCR, etc. 
- **LLM based document processing** These tools can be used to perform IDP, NLP, Table extraction, Information extraction, Classification tasks using LLMs. 
- **Unstructured to Unstructured** For example, a tool that can take a PDF and extract PII information from it and also generate a redacted PDF.
- **Unstructured to Structured** For example, a tool that can take a PDF and extract financial information and store it in a database. Another simple use case would be extracting candidate information from a resume and store it in a database or spreadsheet.


================================================
File: docs/site/docs/Tools/2_Tool architecture.md
================================================
<img src="/img/page_content/tools-architecture.drawio.png" width="75%"></img>

## The building blocks of the Unstract platform

Tools are the building blocks of the Unstract platform. They take a single string input and produce a single string output. These tools can be chained together to create complex worflows that can be used to process unstructured data. Tools can produce output to be consumed by other tools in the worflow or they can produce output that can be sent to a structured/unstructured data sink using the Unstract SDK.

## Isolation and Sandbox

All tools are isolated from the rest of the system and run in a sandboxed/containerized environment. This means that the tools by default cannot access any resources outside of the sandbox. Only resources shared intentionally by the user are accessible. This is done to ensure that the tools are secure and cannot access any sensitive information. The tools are also run in a separate process from the rest of the system. This ensures that the tools cannot access any resources outside of the sandbox.

>Note that while developing a new tool, the SDK and the architecture allows you to run the tool in a non-sandboxed environment for convenience. This is useful for debugging and testing purposes. However, the tool will be run in a sandboxed environment when it is deployed to the platform.

### Access to unstructured data (PDF files, Text files, etc.)

The file sources configured in the platform automatically show up as folders in the sandboxed environment. The tools can access these folders to read and write files. The files written to these folders will be automatically uploaded to the configured file sink. The files will be available in `/mnt/unstract/fs_input/` and  `/mnt/unstract/fs_output/` folders in the sandboxed environment.

### Access to structured data (SQL databases, Warehouses, Datalakes etc.)

The database sources configured in the platform will be available as **SQLAlchemy** engines through the Unstract SDK. The tools can use these engines to read and write data to the configured database sinks. Note that the security and access control of the database sources and sinks are not managed by the platform. While setting up the database sources and sinks, the user will have to provide the necessary credentials and these credentials will have to take care of access control on the DB side. Also these credentials are not available to the tools. The tools can only access the database engines provided by the SDK.

### Access to Vector Databases

The vector databases configured in the platform will be available as [Llamaindex](https://www.llamaindex.ai/) `index` objects through the Unstract SDK. Refer to [LLamaindex documentaion](https://docs.llamaindex.ai/en/stable/core_modules/data_modules/index/root.html) for more details on Index objects. The tools can use these indexes to read and write data to the configured vector databases. Note that the security and access control of the vector database sources and sinks are not managed by the platform. While setting up the vector database sources and sinks, the user will have to provide the necessary credentials and these credentials will have to take care of access control on the vector DB side. Also these credentials are not available to the tools. The tools can only access the vector database indexes provided by the SDK. 

### Access to LLMs

The LLMs configured in the platform will be available as  [Llamaindex](https://www.llamaindex.ai/) `LLM` objects through the Unstract SDK. Refer to [LLamaindex documentaion](https://docs.llamaindex.ai/en/stable/core_modules/model_modules/llms/root.html) for more details on the LLM objects. The tools can use these LLM objects to perform various NLP tasks. 

## Concentrate on the business logic

The SDK and the platform abstracts away the complexities of the underlying components and allows the users to focus on the business logic of their Tools. The SDK provides a convenient interface to the platform to create your own tools. 

Typical steps to create a new tool:

1. Create a new tool scaffolding using the SDK's command line tool
2. Implement the business logic of the tool
3. Test the tool locally
4. Deploy the tool to the platform


================================================
File: docs/site/docs/Tools/3_Installation and quick start.md
================================================
### Install the SDK from pypi

```bash
pip install unstract-sdk
```

### Using the tool generator

```bash
unstract-tool-gen --help

usage: Unstract tool generator [-h] --command COMMAND [--tool-name TOOL_NAME] [--location LOCATION] [--overwrite OVERWRITE]

Script to generate a new Unstract tool

optional arguments:
  -h, --help            show this help message and exit
  --command COMMAND     Command to execute
  --tool-name TOOL_NAME Tool name
  --location LOCATION   Directory to create the new tool in
  --overwrite OVERWRITE Overwrite existing tool

Unstract SDK
```

To create a new tool, run the following command:

```bash
unstract-tool-gen --command NEW \
--tool-name indexer \
--location ~/Devel/Github/unstract/tools/ \
--overwrite false
```

This will create a new tool in the `~/Devel/Github/unstract/tools/` directory. Replace the directory with yours. The tool will be named `indexer`. 

The tool will be created with the following structure:

<img src="/img/page_content/tool-scaffold.png" width="200px"></img>

- `icon.svg` is the icon which will be displayed in the platform
- `json_schema.json` is the json schema for the tool spec
- `properties.json` is the properties file for the tool
- `main.py` is the main file for the tool. This is where the business logic of the tool will be implemented.
- `requirements.txt` is the requirements file for the tool. This is where the dependencies of the tool will be specified.
- `sample.env` is the sample environment file for the tool. This is where the environment variables for the tool will be specified. Make a copy of this as `.env` and fill in the values for the environment variables.
- `.dockerignore` is the docker ignore file for the tool. This is where the files to be ignored by docker will be specified.
- `Dockerfile` is the docker file for the tool. This is used to build the docker image for the tool.
- `README.md` is the readme file for the tool. This is where the documentation for the tool will be specified.

>The docker related files need not be modified unless you are using a different base image or you need to install additional dependencies. The docker image will be built automatically when the tool is deployed to the platform.


================================================
File: docs/site/docs/Tools/4_Tool definition.md
================================================

```json
{
  "display_name": "Document Indexer",
  "function_name": "document_indexer",
  "description": "This tool creates indexes and embeddings for documents.",
  "parameters": [
    {
      "name": "input_file",
      "type": "string",
      "description": "File path of the input file"
    }
  ],
  "versions": [
    "1.0.0"
  ],
  "is_cacheable": false,
  "input_type": "file",
  "output_type": "index",
  "requires": {
    "files": {
      "input": true,
      "output": false
    },
    "databases": {
      "input": false,
      "output": false
    }
  }
}
```

Tools are defined using the `config/properties.json` file. This json file contains standard properties for the tool,
explained below

## `display_name`
The name of the tool as displayed in the platform

## `function_name`
A unique name for the tool. This name is used to identify the tool in the platform. This name should be a valid
python function name. This name is also used to name the docker image for the tool. The docker image name is
`unstract/<function_name>`

## `description`
A short description of the tool. This description is displayed in the platform

## `parameters`
An *array* of json objects. Each json object represents a parameter for the tool. Each parameter object has the
following properties `name`, `type` and `description`. These are the _inputs_ to the tool.

### `parameters[x].name`
Is the name of the parameter. This name is used to identify the parameter in the platform.

### `parameters[x].type`
Is the type of the parameter. This type is used to validate the user input in the platform. The type can be `string`
or `number`.

### `parameters[x].description`
Is a short description of the parameter. This description is displayed in the platform.

## `is_cachable`
A boolean value indicating whether the tool is cachable or not. If the tool is cachable, the platform will allow
caching if the tool is setup to cache.

## `input_type`
The type of input for the tool. This can be `file`, `db` or `index`

## `output_type`
The type of output for the tool. This can be `file`, `db` or `index`

## `required`

### `required.files.input`
A boolean value to indicate whether the tool requires input files or not.

### `required.files.output`
A boolean value to indicate whether the tool produces output files or not.

### `required.db.input`
A boolean value to indicate whether the tool requires input database or not.

### `required.db.output`
A boolean value to indicate whether the tool produces output database or not.


================================================
File: docs/site/docs/Tools/5_Tool settings and icon.md
================================================
## Settings

```json
{
  "title": "Document Indexer",
  "description": "Index documents based on their semantic content",
  "type": "object",
  "required": [
    "embeddingTransformer",
    "vectorStore"
  ],
  "properties": {
    "embeddingTransformer": {
      "type": "string",
      "title": "Embeddings",
      "description": "Embeddings to use",
      "enum": [
        "Azure OpenAI"
      ],
      "default": "Azure OpenAI"
    },
    "vectorStore": {
      "type": "string",
      "title": "Vector store",
      "description": "Vector store to use",
      "enum": [
        "Postgres pg_vector"
      ],
      "default": "Postgres pg_vector"
    },
    "overwrite": {
      "type": "boolean",
      "title": "Overwrite existing vectors",
      "default": false,
      "description": "Overwrite existing vectors"
    },
    "useCache": {
      "type": "boolean",
      "title": "Cache and use cached results",
      "default": true,
      "description": "Use cached results"
    }
  }
}
```

Tool settings are defined in `config/json_schema.json` file. This json schema is used to display the user input form in the platform. The tool should collect all the information required for its working here. For example, if the tool requires a username and password to connect to a database, the tool should collect these details in the settings form. The settings form is displayed to the user when the tool is added to the workflow. You might also collect API keys and other sensitive information here. The platform will provide this infomation to the tool through command line arguments to the main tool script when it is called as part of the workflow or during debugging runs.

The json schema should be a valid json schema. You can use [jsonschema.net](https://jsonschema.net/) to generate a json schema from a sample json. The json schema should be saved in `config/json_schema.json` file. The platform's front end takes care of validating the user input against this schema. The  platform's backend will pass the user input to the tool as command line arguments and/or environmental variables.

## Tool icon

The tool icon is defined in `config/icon.svg` file. This svg file is displayed in the platform. The icon should be a
square aspect ratio.


================================================
File: docs/site/docs/Tools/6_Environment variables.md
================================================
## Mandatory environment variables

** All tools require the following environment variables to be set. These are mandatory**

>Note that these environment variables are automatically set by the platform when the tool is run. These are only required when running the tool locally for testing purposes.

| Variable           | Description                                       |
| ------------------ | ------------------------------------------------- |
| `PLATFORM_HOST`    | The host in which the platform service is running |
| `PLATFORM_PORT`    | The port in which the service is listening        |
| `PLATFORM_API_KEY` | The API key for the platform                      |


================================================
File: docs/site/docs/Tools/7_Unstract protocol.md
================================================
Unstract platform tools can we written in any language as long as they communicate with the platform using the protocol described below. The **Python SDK** provides a convenient interface to the platform to create your own tools. **The SDK abstracts away the details of the protocol and provides a convenient interface to the platform to create your own tools**.


The protocol is based on simple text messages encapsulated in JSON.

List of message types:

- `SPEC`
- `PROPERTIES`
- `ICON`
- `LOG`
- `COST`
- `RESULT`
- `SINGLE_STEP_MESSAGE`

Message type details:

## `SPEC` message

```json
{
  "type": "SPEC",
  "spec": "<SPEC JSON>",
  "emitted_at": "<TIMESTAMP IN ISO FORMAT>"
}
``` 

The `spec` property contains the json from `json_schema.json`. Refer to tool spec documentation for more details.

## `PROPERTIES` message

```json
{
  "type": "PROPERTIES",
  "properties": "<PROPERTIES JSON>",
  "emitted_at": "<TIMESTAMP IN ISO FORMAT>"
} 
```

The `properties` property contains the json from `properties.json`. Refer to tool definition section for more information

## `ICON` message

```json
{
  "type": "ICON",
  "icon": "<ICON SVG>",
  "emitted_at": "<TIMESTAMP IN ISO FORMAT>"
} 
```

The `icon` property contains the svg from `icon.svg`. Refer to *tool icon* section for more details. Note that this is
returns the SVG text itself and not the path to the SVG file.

## `LOG` message

```json
{
  "type": "LOG",
  "level": "<LOG LEVEL>",
  "log": "<LOG MESSAGE>",
  "emitted_at": "<TIMESTAMP IN ISO FORMAT>"
} 
```

The `log` property contains a log message. The level property can contain one
of `DEBUG`, `INFO`, `WARN`, `ERROR`, `FATAL`

## `COST` message

```json
{
  "type": "COST",
  "cost": "<COST>",
  "cost_units": "<COST UNITS>",
  "emitted_at": "<TIMESTAMP IN ISO FORMAT>"
} 
```

The `cost` property contains the cost of the tool run. The `cost` is a floating point number and `cost_units` is a
string

## `RESULT` message

```json
{
  "type": "RESULT",
  "result": {
    "workflow_id": "<WORKFLOW_ID>",
    "elapsed_time": "<ELAPSED TIME>",
    "output": "<OUTPUT JSON or STRING>"
  },
  "emitted_at": "<TIMESTAMP IN ISO FORMAT>"
} 
```

The `result` property contains the result of the tool run. The `result` is a json object. The `result` json object has a
standard format mentioned above.

## `SINGLE_STEP_MESSAGE` message

```json
{
  "type": "SINGLE_STEP_MESSAGE",
  "message": "<MESSAGE>",
  "emitted_at": "<TIMESTAMP IN ISO FORMAT>"
} 
```

The `message` property contains a message to be displayed to the user. This message is displayed in the platform IDE during
single stepping (debug mode). T


================================================
File: docs/site/docs/Tools/8_Implementing and testing the tool.md
================================================
In most cases, implementing the tool involves writing your business logic in the `run` function in the `main.py` file. All other logic to setup the toll and provide metadata is already implemented in the code generated by the SDK.

Let's walk through a sample code for a **classifier** tool

>Note that this is not a production ready implementation. Just an informal introduction to the tool implementation

### Tool properties
```json
{
  "display_name": "File Classifier",
  "function_name": "classify",
  "description": "Classifies a file or folder based on its contents",
  "parameters": [
    {
      "name": "input_file",
      "type": "string",
      "description": "The file operation which needs to be performed"
    }
  ],
  "versions": [
    "1.0.0"
  ],
  "is_cacheable": true,
  "input_type": "file",
  "output_type": "file",
  "requires": {
    "files": {
      "input": true,
      "output": true
    },
    "databases": {
      "input": false,
      "output": false
    }
  }
}
```

### Tool settings (JSON Schema)
```json
{
  "title": "Document classifier",
  "description": "Classify documents based on their content",
  "type": "object",
  "required": [
  ],
  "properties": {
    "classifier": {
      "type": "string",
      "title": "Classifier",
      "description": "Classifier to use",
      "enum": [
        "Azure OpenAI"
      ],
      "default": "Azure OpenAI"
    },
    "classificationBins": {
      "type": "array",
      "title": "Classification bins",
      "description": "Classification bins",
      "items": {
        "type": "string"
      }
    },
    "action": {
      "type": "string",
      "title": "Action",
      "description": "Action to perform",
      "enum": [
        "Copy",
        "Move"
      ],
      "default": "Copy"
    },
    "outputFolder": {
      "type": "string",
      "title": "Output folder",
      "default": "output",
      "description": "Folder to store the output"
    },
    "useCache": {
      "type": "boolean",
      "title": "Cache and use cached results",
      "default": true,
      "description": "Use cached results"
    }
  }
}
```

### Sample implementation


```python
def run(
    params: dict[str, Any],
    settings: dict[str, Any],
    workflow_id: str,
    utils: UnstractToolUtils,
) -> None:
```

- `params` dictionary will contain the output of the previous stage
- `settings` dictionary will contain the settings configured for the tool
- `workflow_id` is the guid of the project
- `utils` is an instantiated class of `UnstractToolUtils`. Refer to the API reference for more information

```python
    MOUNTED_FSSPEC_DIR_INPUT = "/mnt/unstract/fs_input/"
    MOUNTED_FSSPEC_DIR_OUTPUT = "/mnt/unstract/fs_output/"
```
These directories are automatically mounted with the file connector. For local testing, you can change these to your local directories

```python

    input_file = params["input_file"]
    output_folder = settings["outputFolder"]
    classifier = settings["classifier"]
    bins = settings["classificationBins"]
    action = settings["action"]
    use_cache = settings["useCache"]
```

The keys for the dictionaries are the same names in the properties and json schema for the tools (Refer to the tool properties and settings sections above). Whatever user specifies in the tool settings in the frontend will be provided through the settings dictionary

```python

    if len(bins) < 2:
        utils.stream_log("At least two bins are required", "ERROR")
        exit(1)

    allowed_classifiers = ["Azure OpenAI"]
    if classifier not in allowed_classifiers:
        utils.stream_log(
            f"Invalid classifier. Only {allowed_classifiers} are allowed", "ERROR"
        )
        exit(1)

    if input_file.startswith("/"):
        input_file = input_file[1:]
    input_file = f"{MOUNTED_FSSPEC_DIR_INPUT}{input_file}"

    if output_folder.startswith("/"):
        output_folder = output_folder[1:]
    output_folder = f"{MOUNTED_FSSPEC_DIR_OUTPUT}{output_folder}"

    if not os.path.isfile(input_file):
        utils.stream_log(f"Input file not found: {input_file}", "ERROR")
        exit(1)

    utils.stream_log(f"Input file: {input_file}")
    utils.stream_log(f"Output folder: {output_folder}")

```
Note the use of logging helper function `utils.stream_log()` to log information. This helper function creates Unstract protocol messages which will be captured by the platform and used downstream. Refer to the API reference for more information on how to use the utility functions

```python

    # All validations are done. Now run the function
    input_file_type = None
    input_file_type_mime = None
    with open(input_file, mode="rb") as input_file_obj:
        sample_contents = input_file_obj.read(100)
        input_file_type = filetype.guess(sample_contents)

    if input_file_type is None:
        input_file_type_mime = "text/plain"
    else:
        input_file_type_mime = input_file_type.MIME

    utils.stream_log(f"Input file type: {input_file_type_mime}")

    utils.stream_log("Partitioning file...")
    try:
        elements = partition(filename=input_file)
    except Exception as e:
        utils.stream_log(f"Error partitioning file: {e}", "ERROR")
        exit(1)
    text = "\n\n".join([str(el) for el in elements])

    utils.stream_log(f"Text length: {len(text)}")

```

Files are handled in a pythonic way. Also note the use of two modules `unstructerd.io` (partitioning) and `filetype` which are specific to this tool. These modules should be included in the tool's `requirements.txt`

```python

    input_text_for_log = text
    if len(input_text_for_log) > 500:
        input_text_for_log = input_text_for_log[:500] + "...(truncated)"
    utils.stream_single_step_message(
        f"### Input text\n\n```text\n{input_text_for_log}\n```\n\n"
    )

```

Another utility function is used here. `utils.stream_single_step_message()`. This function is used to display messages to the user during single stepping (debugging) mode. This is useful to display the input and output of the tool to the user during debugging. Refer to the API reference for more information.


```python

    if "unknown" not in bins:
        bins.append("unknown")
    bins_with_quotes = [f"'{b}'" for b in bins]

    tool_llm = UnstractToolLLM(utils=utils, llm_id=classifier)
    llm = tool_llm.get_llm()
    cb = tool_llm.get_callback_manager()
    service_context = ServiceContext.from_defaults(
        llm=llm, callback_manager=cb)
    set_global_service_context(service_context)

    max_tokens = tool_llm.get_max_tokens(reserved_for_output=50 + 1000)
    max_bytes = int(max_tokens * 1.3)
    utils.stream_log(
        f"LLM Max tokens: {max_tokens} ==> Max bytes: {max_bytes}")
    limited_text = ""
    for byte in text.encode():
        if len(limited_text.encode()) < max_bytes:
            limited_text += chr(byte)
        else:
            break
    text = limited_text
    utils.stream_log(f"Lenght of text: {len(text.encode())} {len(text)}")
```

The SDK's LLM module is used in the code above. The LLM module is used to interact with the LLMs configured in the platform. The LLM module abstracts away the complexities of the underlying LLMs and provides a simple interface to interact with the LLMs. Refer to the API reference for more information on how to use the LLM module. This SDK module returns a Llamaindex LLM object. Refer to the Llamaindex documentation for more details on how the object can be used.


```python
    prompt = (
        f"Classify the following text into one of the following categories: {' '.join(bins_with_quotes)}.\n\n"  # noqa: E501
        f"Your categorisation should be strictly exactly one of the items in the "
        f"categories given. Find a semantic match of category if possible. If it does not categorise well "  # noqa: E501
        f"into any of the listed categories, categorise it as 'unknown'.\n\nText:\n\n{text}\n\n\nCategory:"  # noqa: E501
    )

    prompt_hash = (
        f"cache:{workflow_id}:{classifier}:{hashlib.md5(prompt.encode()).hexdigest()}"
    )

    classification = None
    if use_cache:
        cache = UnstractToolCache(
            utils=utils,
            platform_host=utils.get_env_or_die("PLATFORM_HOST"),
            platform_port=int(utils.get_env_or_die("PLATFORM_PORT")),
        )
        cached_response = cache.get(prompt_hash)
        if cached_response is not None:
            classification = cached_response
            utils.stream_cost(cost=0.0, cost_units="cache")

    if classification is None:
        utils.stream_log("Calling LLM")
        try:
            response = llm.complete(prompt, max_tokens=50, stop=["\n"])
            tool_llm.stream_usage_counts()
            if response is None:
                utils.stream_log("Error calling LLM", "ERROR")
                exit(1)
            classification = response.text.strip()
            utils.stream_log(f"LLM response: {response}")
        except Exception as e:
            utils.stream_log(f"Error calling LLM: {e}", "ERROR")
            exit(1)

        if use_cache:
            cache = UnstractToolCache(
                utils=utils,
                platform_host=utils.get_env_or_die("PLATFORM_HOST"),
                platform_port=int(utils.get_env_or_die("PLATFORM_PORT")),
            )
            cache.set(prompt_hash, classification)
```
In the code above, caching module provided by the SDK is used. The results from the LLMs can be cached for better performance and cost saving. Refer to the API reference for more information

```python

    classification = classification.lower()
    bins = [bin.lower() for bin in bins]

    if classification not in bins:
        utils.stream_log(
            f"Invalid classification done: {classification}", "ERROR")
        exit(1)

    try:
        output_folder_bin = os.path.join(output_folder, classification)
        if not os.path.exists(output_folder_bin):
            os.makedirs(output_folder_bin, exist_ok=True)

        output_file_bin = f"{output_folder_bin}/{os.path.basename(input_file)}"
        with open(input_file, "rb") as file_in:
            with open(output_file_bin, "wb") as file_out:
                file_out.write(file_in.read())
    except Exception as e:
        utils.stream_log(f"Error creating output file: {e}", "ERROR")
        exit(1)

    if action.lower() == "move":
        try:
            os.remove(input_file)
        except Exception as e:
            utils.stream_log(f"Error deleting input file: {e}", "ERROR")
            exit(1)

    output_log = "### Classier output\n\n"
    output_log += f"```bash\nCLASSIFICATION={classification}\n```\n\n"
    utils.stream_single_step_message(output_log)

    result = {
        "workflow_id": workflow_id,
        "elapsed_time": utils.elapsed_time(),
        "output": classification,
    }
    utils.stream_result(result)
    return
```

Finally the result is streamed back to the platform using the `utils.stream_result()` function. 

## Testing the tool

### Required environment variables

| Variable           | Description                                       |
| ------------------ | ------------------------------------------------- |
| `PLATFORM_HOST`    | The host in which the platform service is running |
| `PLATFORM_PORT`    | The port in which the service is listening        |
| `PLATFORM_API_KEY` | The API key for the platform                      |

### Testing the tool locally

Setup a virtual environment and install the requirements:

```commandline
python -m venv venv
```

Once a virtual environment is created or if you already have created one, activate it:

```commandline
source venv/bin/activate
```

Install the requirements:

> If you want to use the local sdk (not the one from PyPi), make sure you comment out the `unstract-sdk` line in the `requirements.txt` file.

```commandline
pip install -r requirements.txt
```

To use the local development version of the _unstract sdk_ install it from the local repository. Replace the path with
the path to your local repository:

```commandline
pip install ~/Devel/Github/unstract/sdks/.
```

Load the environment variables:

Make a copy of the `sample.env` file and name it `.env`. Fill in the required values.

```commandline
source .env
```

#### Run SPEC command

To get the tool's spec, run the following command:
```commandline
python main.py --command SPEC
```

#### Run PROPERTIES command

To get the tool's properties, run the following command:
```commandline
python main.py --command PROPERTIES
```

#### Run ICON command

To get the tool's icon, run the following command:
```commandline
python main.py --command ICON
```

#### Run RUN command to classify a document

The format of the jsons required for settings and params can be found by running the SPEC command and the PROPERTIES
command respectively. Alternatively if you have access to the code base, it is located in the `config` folder
as `json_schema.json` and `properties.json`.

> Notes on file locations:
>
> The `input_file` parameter is relative to the `/mnt/unstract/fs_input/` directory. So in the following example the
> file being classified is located at `/mnt/unstract/fs_input/files/bbc-pol-004.txt`. This location can be changed by
> changing the `FILES_PATH` environment variable in the platform service and not in the classifier tool.

```commandline
python main.py \
    --command RUN \
    --params '{
        "input_file": "files/bbc-pol-004.txt"
        }' \
    --settings '{
        "classifier": "Azure OpenAI",
        "classificationBins": [
            "business", "entertainment", "politics", "sports", "tech"
            ],
        "action": "copy",
        "useCache": false,
        "outputFolder": "classified"
        }' \
    --workflow-id '00000000-0000-0000-0000-000000000000' \
    --log-level DEBUG

```

### Currently supported classifiers 

-   Azure OpenAI
    -   Environment variables required:
        -   `OPENAI_API_KEY`
        -   `OPENAI_API_BASE`
        -   `OPENAI_API_VERSION`
        -   `OPENAI_API_ENGINE`
        -   `OPENAI_API_MODEL`

**Make sure the required environment variables are set before testing**

### Testing the tool from its docker image

To test the tool from its docker image, run the following command:

```commandline
docker run \
    -v /Users/arun/Devel/unstract_storage:/mnt/unstract/fs_input \
    unstract-tool-fileops:0.1 \
    python main.py \
    --command RUN \
    --params '{
        "input_file": "files/bbc-pol-004.txt"
        }' \
    --settings '{
        "classifier": "Azure OpenAI",
        "classificationBins": [
            "business", "entertainment", "politics", "sports", "tech"
            ],
        "action": "copy",
        "useCache": false,
        "outputFolder": "classified"
        }' \
    --workflow-id '00000000-0000-0000-0000-000000000000' \
    --log-level DEBUG

```

Notes for Docker:

* The `-v` option mounts the `/Users/arun/Devel/unstract_storage` folder on the host machine to
  the `/mnt/unstract/fs_input`. Replace the path with the path to your documents folder.
* When this command is called by the workflow execution subsystem, the path to the input files configured by the user in
  the UI is automatically mounted and loaded as a volumne in the `/mnt/unstract/fs_input` folder in the container.


================================================
File: docs/site/docs/Tools/_category_.json
================================================
{
    "position": 3,
    "label": "Tools",
    "collapsible": true,
    "collapsed": false,
    "link": {
        "type": "generated-index",
        "title": "Tools"
    },
    "customProps": {}
}


================================================
File: docs/site/src/components/HomepageFeatures/index.js
================================================
import React from 'react';
import clsx from 'clsx';
import styles from './styles.module.css';

const FeatureList = [
  {
    title: 'Easy to Use',
    Svg: require('@site/static/img/undraw_docusaurus_mountain.svg').default,
    description: (
      <>
        Docusaurus was designed from the ground up to be easily installed and
        used to get your website up and running quickly.
      </>
    ),
  },
  {
    title: 'Focus on What Matters',
    Svg: require('@site/static/img/undraw_docusaurus_tree.svg').default,
    description: (
      <>
        Docusaurus lets you focus on your docs, and we&apos;ll do the chores. Go
        ahead and move your docs into the <code>docs</code> directory.
      </>
    ),
  },
  {
    title: 'Powered by React',
    Svg: require('@site/static/img/undraw_docusaurus_react.svg').default,
    description: (
      <>
        Extend or customize your website layout by reusing React. Docusaurus can
        be extended while reusing the same header and footer.
      </>
    ),
  },
];

function Feature({Svg, title, description}) {
  return (
    <div className={clsx('col col--4')}>
      <div className="text--center">
        <Svg className={styles.featureSvg} role="img" />
      </div>
      <div className="text--center padding-horiz--md">
        <h3>{title}</h3>
        <p>{description}</p>
      </div>
    </div>
  );
}

export default function HomepageFeatures() {
  return (
    <section className={styles.features}>
      <div className="container">
        <div className="row">
          {FeatureList.map((props, idx) => (
            <Feature key={idx} {...props} />
          ))}
        </div>
      </div>
    </section>
  );
}


================================================
File: docs/site/src/components/HomepageFeatures/styles.module.css
================================================
.features {
  display: flex;
  align-items: center;
  padding: 2rem 0;
  width: 100%;
}

.featureSvg {
  height: 200px;
  width: 200px;
}


================================================
File: docs/site/src/css/custom.css
================================================
/**
 * Any CSS included here will be global. The classic template
 * bundles Infima by default. Infima is a CSS framework designed to
 * work well for content-centric websites.
 */

 @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500&family=Montserrat:wght@400;500&display=swap');

/* You can override the default Infima variables here. */
:root {
  --ifm-color-primary: #FF4D6D;
  --ifm-color-primary-dark: #29784c;
  --ifm-color-primary-darker: #277148;
  --ifm-color-primary-darkest: #205d3b;
  --ifm-color-primary-light: #33925d;
  --ifm-color-primary-lighter: #359962;
  --ifm-color-primary-lightest: #3cad6e;
  --ifm-code-font-size: 95%;
  --docusaurus-highlighted-code-line-bg: rgba(0, 0, 0, 0.1);
  --ifm-font-size-base: 15px;
  --ifm-font-family-base: 'Inter';
  --ifm-heading-font-family: 'Inter';
}

/* For readability concerns, you should choose a lighter palette in dark mode. */
[data-theme='dark'] {
  --ifm-color-primary: #FF4D6D;
  --ifm-color-primary-dark: #21af90;
  --ifm-color-primary-darker: #1fa588;
  --ifm-color-primary-darkest: #1a8870;
  --ifm-color-primary-light: #29d5b0;
  --ifm-color-primary-lighter: #32d8b4;
  --ifm-color-primary-lightest: #4fddbf;
  --docusaurus-highlighted-code-line-bg: rgba(0, 0, 0, 0.3);
  --ifm-font-size-base: 15px;
  --ifm-font-family-base: 'Inter';
  --ifm-heading-font-family: 'Inter';

}


================================================
File: docs/site/src/pages/index.js
================================================
import React from 'react';
import clsx from 'clsx';
import Link from '@docusaurus/Link';
import useDocusaurusContext from '@docusaurus/useDocusaurusContext';
import Layout from '@theme/Layout';
import HomepageFeatures from '@site/src/components/HomepageFeatures';
import { Redirect } from '@docusaurus/router';
import styles from './index.module.css';

function HomepageHeader() {
  const {siteConfig} = useDocusaurusContext();
  return (
    <header className={clsx('hero hero--primary', styles.heroBanner)}>
      <div className="container">
        <h1 className="hero__title">{siteConfig.title}</h1>
        <p className="hero__subtitle">{siteConfig.tagline}</p>
        <div className={styles.buttons}>
          <Link
            className="button button--secondary button--lg"
            to="/docs/intro">
            Docusaurus Tutorial - 5min ⏱️
          </Link>
        </div>
      </div>
    </header>
  );
}

export default function Home() {
  return <Redirect to="/docs/Introduction" />;
//  const {siteConfig} = useDocusaurusContext();
//  return (
//    <Layout
//      title={`Hello from ${siteConfig.title}`}
//      description="Description will go into a meta tag in <head />">
//      <HomepageHeader />
//      <main>
//        <HomepageFeatures />
//      </main>
//    </Layout>
//  );
}


================================================
File: docs/site/src/pages/index.module.css
================================================
/**
 * CSS files with the .module.css suffix will be treated as CSS modules
 * and scoped locally.
 */

.heroBanner {
  padding: 4rem 0;
  text-align: center;
  position: relative;
  overflow: hidden;
}

@media screen and (max-width: 996px) {
  .heroBanner {
    padding: 2rem;
  }
}

.buttons {
  display: flex;
  align-items: center;
  justify-content: center;
}


================================================
File: docs/site/src/pages/markdown-page.md
================================================
---
title: Markdown page example
---

# Markdown page example

You don't need React to write simple standalone pages.


================================================
File: src/unstract/sdk/__init__.py
================================================
__version__ = "0.60.0"


def get_sdk_version():
    """Returns the SDK version."""
    return __version__


================================================
File: src/unstract/sdk/adapter.py
================================================
import json
from typing import Any, Optional

import requests
from requests.exceptions import ConnectionError, HTTPError

from unstract.sdk.adapters.utils import AdapterUtils
from unstract.sdk.constants import AdapterKeys, LogLevel, ToolEnv
from unstract.sdk.exceptions import SdkError
from unstract.sdk.helper import SdkHelper
from unstract.sdk.platform import PlatformBase
from unstract.sdk.tool.base import BaseTool


class ToolAdapter(PlatformBase):
    """Class to handle Adapters for Unstract Tools.

    Notes:
        - PLATFORM_SERVICE_API_KEY environment variable is required.
    """

    def __init__(
        self,
        tool: BaseTool,
        platform_host: str,
        platform_port: str,
    ) -> None:
        """
        Args:
            tool (AbstractTool): Instance of AbstractTool
            platform_host (str): Host of platform service
            platform_port (str): Port of platform service

        Notes:
            - PLATFORM_SERVICE_API_KEY environment variable is required.
            - The platform_host and platform_port are the
                host and port of the platform service.
        """
        super().__init__(
            tool=tool, platform_host=platform_host, platform_port=platform_port
        )

    def _get_adapter_configuration(
        self,
        adapter_instance_id: str,
    ) -> dict[str, Any]:
        """Get Adapter
            1. Get the adapter config from platform service
            using the adapter_instance_id

        Args:
            adapter_instance_id (str): Adapter instance ID

        Returns:
            dict[str, Any]: Config stored for the adapter
        """
        url = f"{self.base_url}/adapter_instance"
        query_params = {AdapterKeys.ADAPTER_INSTANCE_ID: adapter_instance_id}
        headers = {"Authorization": f"Bearer {self.bearer_token}"}
        try:
            response = requests.get(url, headers=headers, params=query_params)
            response.raise_for_status()
            adapter_data: dict[str, Any] = response.json()

            # Removing name and type to avoid migration for already indexed records
            adapter_name = adapter_data.pop("adapter_name", "")
            adapter_type = adapter_data.pop("adapter_type", "")
            provider = adapter_data.get("adapter_id", "").split("|")[0]
            # TODO: Print metadata after redacting sensitive information
            self.tool.stream_log(
                f"Retrieved config for '{adapter_instance_id}', type: "
                f"'{adapter_type}', provider: '{provider}', name: '{adapter_name}'",
                level=LogLevel.DEBUG,
            )
        except ConnectionError:
            raise SdkError(
                "Unable to connect to platform service, please contact the admin."
            )
        except HTTPError as e:
            default_err = (
                "Error while calling the platform service, please contact the admin."
            )
            msg = AdapterUtils.get_msg_from_request_exc(
                err=e, message_key="error", default_err=default_err
            )
            raise SdkError(f"Error retrieving adapter. {msg}")
        return adapter_data

    @staticmethod
    def get_adapter_config(
        tool: BaseTool, adapter_instance_id: str
    ) -> Optional[dict[str, Any]]:
        """Get adapter spec by the help of unstract DB tool.

        This method first checks if the adapter_instance_id matches
        any of the public adapter keys. If it matches, the configuration
        is fetched from environment variables. Otherwise, it connects to the
        platform service to retrieve the configuration.

        Args:
            tool (AbstractTool): Instance of AbstractTool
            adapter_instance_id (str): ID of the adapter instance
        Required env variables:
            PLATFORM_HOST: Host of platform service
            PLATFORM_PORT: Port of platform service
        Returns:
            dict[str, Any]: Config stored for the adapter
        """
        # Check if the adapter ID matches any public adapter keys
        if SdkHelper.is_public_adapter(adapter_id=adapter_instance_id):
            adapter_metadata_config = tool.get_env_or_die(adapter_instance_id)
            adapter_metadata = json.loads(adapter_metadata_config)
            return adapter_metadata
        platform_host = tool.get_env_or_die(ToolEnv.PLATFORM_HOST)
        platform_port = tool.get_env_or_die(ToolEnv.PLATFORM_PORT)

        tool.stream_log(
            f"Retrieving config from DB for '{adapter_instance_id}'",
            level=LogLevel.DEBUG,
        )
        tool_adapter = ToolAdapter(
            tool=tool,
            platform_host=platform_host,
            platform_port=platform_port,
        )
        return tool_adapter._get_adapter_configuration(adapter_instance_id)


================================================
File: src/unstract/sdk/audit.py
================================================
from typing import Any, Union

import requests
from llama_index.core.callbacks import CBEventType, TokenCountingHandler

from unstract.sdk.constants import LogLevel, ToolEnv
from unstract.sdk.helper import SdkHelper
from unstract.sdk.tool.stream import StreamMixin
from unstract.sdk.utils.token_counter import TokenCounter


class Audit(StreamMixin):
    """The 'Audit' class is responsible for pushing usage data to the platform
    service.

    Methods:
        - push_usage_data: Pushes the usage data to the platform service.

    Attributes:
        None
    """

    def __init__(self, log_level: LogLevel = LogLevel.INFO) -> None:
        super().__init__(log_level)

    def push_usage_data(
        self,
        platform_api_key: str,
        token_counter: Union[TokenCountingHandler, TokenCounter] = None,
        model_name: str = "",
        event_type: CBEventType = None,
        kwargs: dict[Any, Any] = None,
    ) -> None:
        """Pushes the usage data to the platform service.

        Args:
            platform_api_key (str): The platform API key.
            token_counter (TokenCountingHandler, optional): The token counter
                object. Defaults to None.
            model_name (str, optional): The name of the model.
                Defaults to "".
            event_type (CBEventType, optional): The type of the event. Defaults
                to None.
            **kwargs: Optional keyword arguments.
                workflow_id (str, optional): The ID of the workflow.
                    Defaults to "".
                execution_id (str, optional): The ID of the execution. Defaults
                    to "".
                adapter_instance_id (str, optional): The adapter instance ID.
                    Defaults to "".
                run_id (str, optional): The run ID. Defaults to "".

        Returns:
            None

        Raises:
            requests.RequestException: If there is an error while pushing the
            usage details.
        """
        platform_host = self.get_env_or_die(ToolEnv.PLATFORM_HOST)
        platform_port = self.get_env_or_die(ToolEnv.PLATFORM_PORT)

        base_url = SdkHelper.get_platform_base_url(
            platform_host=platform_host, platform_port=platform_port
        )
        bearer_token = platform_api_key

        workflow_id = kwargs.get("workflow_id", "")
        execution_id = kwargs.get("execution_id", "")
        adapter_instance_id = kwargs.get("adapter_instance_id", "")
        run_id = kwargs.get("run_id", "")
        provider = kwargs.get("provider", "")
        llm_usage_reason = ""
        if event_type == "llm":
            llm_usage_reason = kwargs.get("llm_usage_reason", "")
        data = {
            "workflow_id": workflow_id,
            "execution_id": execution_id,
            "adapter_instance_id": adapter_instance_id,
            "run_id": run_id,
            "usage_type": event_type,
            "llm_usage_reason": llm_usage_reason,
            "model_name": model_name,
            "provider": provider,
            "embedding_tokens": token_counter.total_embedding_token_count,
            "prompt_tokens": token_counter.prompt_llm_token_count,
            "completion_tokens": token_counter.completion_llm_token_count,
            "total_tokens": token_counter.total_llm_token_count,
        }

        url = f"{base_url}/usage"
        headers = {"Authorization": f"Bearer {bearer_token}"}

        try:
            response = requests.post(url, headers=headers, json=data, timeout=30)
            if response.status_code != 200:
                self.stream_log(
                    log=(
                        "Error while pushing usage details: "
                        f"{response.status_code} {response.reason}",
                    ),
                    level=LogLevel.ERROR,
                )
            else:
                self.stream_log(
                    f"Successfully pushed usage details, {data}", level=LogLevel.DEBUG
                )

        except requests.RequestException as e:
            self.stream_log(
                log=f"Error while pushing usage details: {e}",
                level=LogLevel.ERROR,
            )

        finally:
            if isinstance(token_counter, TokenCountingHandler):
                token_counter.reset_counts()

    def push_page_usage_data(
        self,
        platform_api_key: str,
        page_count: int,
        file_size: int,
        file_type: str,
        kwargs: dict[Any, Any] = None,
    ) -> None:
        platform_host = self.get_env_or_die(ToolEnv.PLATFORM_HOST)
        platform_port = self.get_env_or_die(ToolEnv.PLATFORM_PORT)
        run_id = kwargs.get("run_id", "")
        file_name = kwargs.get("file_name", "")
        base_url = SdkHelper.get_platform_base_url(
            platform_host=platform_host, platform_port=platform_port
        )
        bearer_token = platform_api_key
        url = f"{base_url}/page-usage"
        headers = {"Authorization": f"Bearer {bearer_token}"}

        data = {
            "page_count": page_count,
            "file_name": file_name,
            "file_size": file_size,
            "file_type": file_type,
            "run_id": run_id,
        }

        try:
            response = requests.post(url, headers=headers, json=data, timeout=30)
            if response.status_code != 200:
                self.stream_log(
                    log=(
                        "Error while pushing page usage details: "
                        f"{response.status_code} {response.reason}",
                    ),
                    level=LogLevel.ERROR,
                )
            else:
                self.stream_log(
                    "Successfully pushed page usage details", level=LogLevel.DEBUG
                )

        except requests.RequestException as e:
            self.stream_log(
                log=f"Error while pushing page usage details: {e}",
                level=LogLevel.ERROR,
            )


================================================
File: src/unstract/sdk/cache.py
================================================
from typing import Any, Optional

import requests

from unstract.sdk.constants import LogLevel
from unstract.sdk.platform import PlatformBase
from unstract.sdk.tool.base import BaseTool


class ToolCache(PlatformBase):
    """Class to handle caching for Unstract Tools.

    Notes:
        - PLATFORM_SERVICE_API_KEY environment variable is required.
    """

    def __init__(
        self, tool: BaseTool, platform_host: str, platform_port: int
    ) -> None:
        """
        Args:
            tool (AbstractTool): Instance of AbstractTool
            platform_host (str): The host of the platform.
            platform_port (int): The port of the platform.

        Notes:
            - PLATFORM_SERVICE_API_KEY environment variable is required.
            - The platform_host and platform_port are the host and port of
                the platform service.
        """
        super().__init__(
            tool=tool, platform_host=platform_host, platform_port=platform_port
        )

    def set(self, key: str, value: str) -> bool:
        """Sets the value for a key in the cache.

        Args:
            key (str): The key.
            value (str): The value.

        Returns:
            bool: Whether the operation was successful.
        """

        url = f"{self.base_url}/cache"
        json = {"key": key, "value": value}
        headers = {"Authorization": f"Bearer {self.bearer_token}"}
        response = requests.post(url, json=json, headers=headers)

        if response.status_code == 200:
            self.tool.stream_log(f"Successfully cached data for key: {key}")
            return True
        else:
            self.tool.stream_log(
                f"Error while caching data for key: {key} / {response.reason}",
                level=LogLevel.ERROR,
            )
            return False

    def get(self, key: str) -> Optional[Any]:
        """Gets the value for a key in the cache.

        Args:
            key (str): The key.

        Returns:
            str: The value.
        """

        url = f"{self.base_url}/cache?key={key}"
        headers = {"Authorization": f"Bearer {self.bearer_token}"}
        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            self.tool.stream_log(
                f"Successfully retrieved cached data for key: {key}"
            )
            return response.text
        elif response.status_code == 404:
            self.tool.stream_log(
                f"Data not found for key: {key}", level=LogLevel.WARN
            )
            return None
        else:
            self.tool.stream_log(
                f"Error while retrieving cached data for key: "
                f"{key} / {response.reason}",
                level=LogLevel.ERROR,
            )
            return None

    def delete(self, key: str) -> bool:
        """Deletes the value for a key in the cache.

        Args:
            key (str): The key.

        Returns:
            bool: Whether the operation was successful.
        """
        url = f"{self.base_url}/cache?key={key}"
        headers = {"Authorization": f"Bearer {self.bearer_token}"}
        response = requests.delete(url, headers=headers)

        if response.status_code == 200:
            self.tool.stream_log(
                f"Successfully deleted cached data for key: {key}"
            )
            return True
        else:
            self.tool.stream_log(
                "Error while deleting cached data "
                f"for key: {key} / {response.reason}",
                level=LogLevel.ERROR,
            )
            return False


================================================
File: src/unstract/sdk/constants.py
================================================
from enum import Enum


class ToolEnv:
    """Environment variables used by tools.

    The 'ToolEnv' class represents a set of environment variables that are
    commonly used by tools.

    Attributes:
        PLATFORM_API_KEY (str): Platform service API key.
        PLATFORM_HOST (str): Platform service host.
        PLATFORM_PORT (str): Platform service port.
        EXECUTION_BY_TOOL (str): Implicitly set to 1 by the SDK if its executed
            by a tool.
    """

    PLATFORM_API_KEY = "PLATFORM_SERVICE_API_KEY"
    PLATFORM_HOST = "PLATFORM_SERVICE_HOST"
    PLATFORM_PORT = "PLATFORM_SERVICE_PORT"
    EXECUTION_BY_TOOL = "EXECUTION_BY_TOOL"
    EXECUTION_DATA_DIR = "EXECUTION_DATA_DIR"
    WORKFLOW_EXECUTION_FILE_STORAGE_CREDENTIALS = (
        "WORKFLOW_EXECUTION_FILE_STORAGE_CREDENTIALS"
    )


class ConnectorKeys:
    ID = "id"
    PROJECT_ID = "project_id"
    CONNECTOR_ID = "connector_id"
    TOOL_INSTANCE_ID = "tool_instance_id"
    CONNECTOR_METADATA = "connector_metadata"
    CONNECTOR_TYPE = "connector_type"


class AdapterKeys:
    ADAPTER_INSTANCE_ID = "adapter_instance_id"


class PromptStudioKeys:
    PROMPT_REGISTRY_ID = "prompt_registry_id"


class ConnectorType:
    INPUT = "INPUT"
    OUTPUT = "OUTPUT"


class LogType:
    LOG = "LOG"
    UPDATE = "UPDATE"


class LogStage:
    TOOL_RUN = "TOOL_RUN"


class LogState:
    """State of logs INPUT_UPDATE tag for update the FE input component
    OUTPUT_UPDATE tag for update the FE output component."""

    INPUT_UPDATE = "INPUT_UPDATE"
    OUTPUT_UPDATE = "OUTPUT_UPDATE"


class Connector:
    FILE_SYSTEM = "FILE_SYSTEM"
    DATABASE = "DATABASE"


class Command:
    SPEC = "SPEC"
    PROPERTIES = "PROPERTIES"
    ICON = "ICON"
    RUN = "RUN"
    VARIABLES = "VARIABLES"

    @classmethod
    def static_commands(cls) -> set[str]:
        return {cls.SPEC, cls.PROPERTIES, cls.ICON, cls.VARIABLES}


class UsageType:
    LLM_COMPLETE = "LLM_COMPLETE"
    RAG = "RAG"
    INDEXER = "INDEXER"


class LogLevel(Enum):
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARN = "WARN"
    ERROR = "ERROR"
    FATAL = "FATAL"


class PropKey:
    """Keys for the properties.json of tools."""

    INPUT = "input"
    OUTPUT = "output"
    RESULT = "result"
    TYPE = "type"
    RESTRICTIONS = "restrictions"
    MAX_FILE_SIZE = "maxFileSize"
    FILE_SIZE_REGEX = r"^(\d+)\s*([KkMmGgTt]B?)$"
    ALLOWED_FILE_TYPES = "allowedFileTypes"
    FUNCTION_NAME = "functionName"

    class OutputType:
        JSON = "JSON"
        TXT = "TXT"


class ToolExecKey:
    OUTPUT_DIR = "COPY_TO_FOLDER"
    METADATA_FILE = "METADATA.json"
    INFILE = "INFILE"
    SOURCE = "SOURCE"


class MetadataKey:
    SOURCE_NAME = "source_name"
    SOURCE_HASH = "source_hash"
    WORKFLOW_ID = "workflow_id"
    EXECUTION_ID = "execution_id"
    FILE_EXECUTION_ID = "file_execution_id"
    TAGS = "tags"
    ORG_ID = "organization_id"
    TOOL_META = "tool_metadata"
    TOOL_NAME = "tool_name"
    TOTAL_ELA_TIME = "total_elapsed_time"
    ELAPSED_TIME = "elapsed_time"
    OUTPUT = "output"
    OUTPUT_TYPE = "output_type"


class ToolSettingsKey:
    """A class representing the keys used in the tool settings.

    Attributes:
        LLM_ADAPTER_ID (str): The key for the LLM adapter ID.
        EMBEDDING_ADAPTER_ID (str): The key for the embedding adapter ID.
        VECTOR_DB_ADAPTER_ID (str): The key for the vector DB adapter ID.
        X2TEXT_ADAPTER_ID (str): The key for the X2Text adapter ID.
    """

    LLM_ADAPTER_ID = "llmAdapterId"
    EMBEDDING_ADAPTER_ID = "embeddingAdapterId"
    VECTOR_DB_ADAPTER_ID = "vectorDbAdapterId"
    X2TEXT_ADAPTER_ID = "x2TextAdapterId"
    ADAPTER_INSTANCE_ID = "adapter_instance_id"
    EMBEDDING_DIMENSION = "embedding_dimension"
    RUN_ID = "run_id"
    WORKFLOW_ID = "workflow_id"
    EXECUTION_ID = "execution_id"


class PublicAdapterKeys:
    PUBLIC_LLM_CONFIG = "PUBLIC_LLM_CONFIG"
    PUBLIC_EMBEDDING_CONFIG = "PUBLIC_EMBEDDING_CONFIG"
    PUBLIC_VECTOR_DB_CONFIG = "PUBLIC_VECTOR_DB_CONFIG"
    PUBLIC_X2TEXT_CONFIG = "PUBLIC_X2TEXT_CONFIG"


class MimeType:
    PDF = "application/pdf"
    TEXT = "text/plain"
    JSON = "application/json"


class UsageKwargs:
    RUN_ID = "run_id"
    FILE_NAME = "file_name"
    WORKFLOW_ID = "workflow_id"
    EXECUTION_ID = "execution_id"


================================================
File: src/unstract/sdk/embedding.py
================================================
from typing import Any, Optional

from deprecated import deprecated
from llama_index.core.base.embeddings.base import Embedding
from llama_index.core.callbacks import CallbackManager as LlamaIndexCallbackManager
from llama_index.core.embeddings import BaseEmbedding

from unstract.sdk.adapter import ToolAdapter
from unstract.sdk.adapters.constants import Common
from unstract.sdk.adapters.embedding import adapters
from unstract.sdk.constants import LogLevel, ToolEnv
from unstract.sdk.exceptions import EmbeddingError, SdkError
from unstract.sdk.helper import SdkHelper
from unstract.sdk.tool.base import BaseTool
from unstract.sdk.utils.callback_manager import CallbackManager


class Embedding:
    _TEST_SNIPPET = "Hello, I am Unstract"
    MAX_TOKENS = 1024 * 16
    embedding_adapters = adapters

    def __init__(
        self,
        tool: BaseTool,
        adapter_instance_id: Optional[str] = None,
        usage_kwargs: dict[Any, Any] = {},
    ):
        self._tool = tool
        self._adapter_instance_id = adapter_instance_id
        self._embedding_instance: BaseEmbedding = None
        self._length: int = None
        self._usage_kwargs = usage_kwargs
        self._initialise()

    def _initialise(self):
        if self._adapter_instance_id:
            self._embedding_instance = self._get_embedding()
            self._length: int = self._get_embedding_length()
            self._usage_kwargs["adapter_instance_id"] = self._adapter_instance_id

            if not SdkHelper.is_public_adapter(adapter_id=self._adapter_instance_id):
                platform_api_key = self._tool.get_env_or_die(ToolEnv.PLATFORM_API_KEY)
                CallbackManager.set_callback(
                    platform_api_key=platform_api_key,
                    model=self._embedding_instance,
                    kwargs=self._usage_kwargs,
                )

    def _get_embedding(self) -> BaseEmbedding:
        """Gets an instance of LlamaIndex's embedding object.

        Args:
            adapter_instance_id (str): UUID of the embedding adapter

        Returns:
            BaseEmbedding: Embedding instance
        """
        try:
            if not self._adapter_instance_id:
                raise EmbeddingError(
                    "Adapter instance ID not set. " "Initialisation failed"
                )

            embedding_config_data = ToolAdapter.get_adapter_config(
                self._tool, self._adapter_instance_id
            )
            embedding_adapter_id = embedding_config_data.get(Common.ADAPTER_ID)
            if embedding_adapter_id not in self.embedding_adapters:
                raise SdkError(
                    f"Embedding adapter not supported : " f"{embedding_adapter_id}"
                )

            embedding_adapter = self.embedding_adapters[embedding_adapter_id][
                Common.METADATA
            ][Common.ADAPTER]
            embedding_metadata = embedding_config_data.get(Common.ADAPTER_METADATA)
            embedding_adapter_class = embedding_adapter(embedding_metadata)
            self._usage_kwargs["provider"] = embedding_adapter_class.get_provider()
            return embedding_adapter_class.get_embedding_instance()
        except Exception as e:
            self._tool.stream_log(
                log=f"Error getting embedding: {e}", level=LogLevel.ERROR
            )
            raise EmbeddingError(f"Error getting embedding instance: {e}") from e

    def get_query_embedding(self, query: str) -> Embedding:
        return self._embedding_instance.get_query_embedding(query)

    def _get_embedding_length(self) -> int:
        embedding_list = self._embedding_instance._get_text_embedding(
            self._TEST_SNIPPET
        )
        embedding_dimension = len(embedding_list)
        return embedding_dimension

    def get_class_name(self) -> str:
        """Gets the class name of the Llama Index Embedding.

        Args:
            NA

            Returns:
                Class name
        """
        return self._embedding_instance.class_name()

    def get_callback_manager(self) -> LlamaIndexCallbackManager:
        """Gets the llama-index callback manager set on the model.

        Args:
            NA

            Returns:
                llama-index callback manager
        """
        return self._embedding_instance.callback_manager

    @deprecated("Use Embedding instead of ToolEmbedding")
    def get_embedding_length(self, embedding: BaseEmbedding) -> int:
        return self._get_embedding_length()

    @deprecated("Use Embedding instead of ToolEmbedding")
    def get_embedding(self, adapter_instance_id: str) -> BaseEmbedding:
        if not self._embedding_instance:
            self._adapter_instance_id = adapter_instance_id
            self._initialise()
        return self._embedding_instance


# Legacy
ToolEmbedding = Embedding


================================================
File: src/unstract/sdk/exceptions.py
================================================
from typing import Optional


def resolve_err_status_code(client_status_code: int) -> int:
    """Resolves the status code to return in case of errors.

    Returns a status code which should be returned from Unstract based on
    the status code from the client.

    - 502 is returned for 500 client errors
    - 4xx is returned for 4xx client errors

    Args:
        client_status_code (int): Status code from client library

    Returns:
        int: Status code that Unstract should return
    """
    if client_status_code == 500:
        return 502

    # 4xx status codes returned as is
    return client_status_code


class SdkError(Exception):
    DEFAULT_MESSAGE = "Something went wrong"
    actual_err: Optional[Exception] = None
    status_code: Optional[int] = None

    def __init__(
        self,
        message: str = DEFAULT_MESSAGE,
        status_code: Optional[int] = None,
        actual_err: Optional[Exception] = None,
    ):
        super().__init__(message)
        # Make it user friendly wherever possible
        self.message = message
        if actual_err:
            self.actual_err = actual_err

        # Setting status code for error
        if status_code:
            self.status_code = status_code
        elif actual_err:
            if hasattr(actual_err, "status_code"):  # Most providers
                self.status_code = resolve_err_status_code(actual_err.status_code)
            elif hasattr(actual_err, "http_status"):  # Few providers like Mistral
                self.status_code = resolve_err_status_code(actual_err.http_status)

    def __str__(self) -> str:
        return self.message


class IndexingError(SdkError):
    def __init__(self, message: str = "", **kwargs):
        if "404" in message:
            message = "Index not found. Please check vector DB settings."
        super().__init__(message, **kwargs)


class LLMError(SdkError):
    DEFAULT_MESSAGE = "Error ocurred related to LLM"


class EmbeddingError(SdkError):
    DEFAULT_MESSAGE = "Error ocurred related to embedding"


class VectorDBError(SdkError):
    DEFAULT_MESSAGE = "Error ocurred related to vector DB"


class X2TextError(SdkError):
    DEFAULT_MESSAGE = "Error ocurred related to text extractor"


class OCRError(SdkError):
    DEFAULT_MESSAGE = "Error ocurred related to OCR"


class RateLimitError(SdkError):
    DEFAULT_MESSAGE = "Running into rate limit errors, please try again later"


class FileStorageError(SdkError):
    DEFAULT_MESSAGE = (
        "Error while connecting with the storage. "
        "Please check the configuration credentials"
    )


class FileOperationError(SdkError):
    DEFAULT_MESSAGE = (
        "Error while performing operation on the file. "
        "Please check specific storage error for "
        "further information"
    )


================================================
File: src/unstract/sdk/helper.py
================================================
import logging

from unstract.sdk.constants import PublicAdapterKeys

logger = logging.getLogger(__name__)

class SdkHelper:
    def __init__(self) -> None:
        pass

    @staticmethod
    def get_platform_base_url(platform_host: str, platform_port: str) -> str:
        """Make base url from host and port.

        Args:
            platform_host (str): Host of platform service
            platform_port (str): Port of platform service

        Returns:
            str: URL to the platform service
        """
        if platform_host[-1] == "/":
            return f"{platform_host[:-1]}:{platform_port}"
        return f"{platform_host}:{platform_port}"

    @staticmethod
    def is_public_adapter(adapter_id: str) -> bool:
        """Check if the given adapter_id is one of the public adapter keys.

        This method iterates over the attributes of the PublicAdapterKeys class
        and checks if the provided adapter_id matches any of the attribute values.

        Args:
            adapter_id (str): The ID of the adapter to check.

        Returns:
            bool: True if the adapter_id matches any public adapter key,
            False otherwise.
        """
        try:
            for attr in dir(PublicAdapterKeys):
                if getattr(PublicAdapterKeys, attr) == adapter_id:
                    return True
            return False
        except Exception as e:
            logger.warning(
                f"Unable to determine if adapter_id: {adapter_id}"
                f"is public or not: {str(e)}"
            )
            return False


================================================
File: src/unstract/sdk/index.py
================================================
import json
import logging
from typing import Any, Callable, Optional

from deprecated import deprecated
from llama_index.core import Document
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.vector_stores import (
    FilterOperator,
    MetadataFilter,
    MetadataFilters,
    VectorStoreQuery,
    VectorStoreQueryResult,
)

from unstract.sdk.adapter import ToolAdapter
from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.vectordb.no_op.src.no_op_custom_vectordb import (
    NoOpCustomVectorDB,
)
from unstract.sdk.adapters.x2text.constants import X2TextConstants
from unstract.sdk.adapters.x2text.dto import TextExtractionResult
from unstract.sdk.adapters.x2text.llm_whisperer.src import LLMWhisperer
from unstract.sdk.adapters.x2text.llm_whisperer_v2.src import LLMWhispererV2
from unstract.sdk.constants import LogLevel
from unstract.sdk.embedding import Embedding
from unstract.sdk.exceptions import IndexingError, SdkError, VectorDBError, X2TextError
from unstract.sdk.file_storage import FileStorage, FileStorageProvider
from unstract.sdk.tool.base import BaseTool
from unstract.sdk.utils import ToolUtils
from unstract.sdk.utils.common_utils import capture_metrics, log_elapsed
from unstract.sdk.vector_db import VectorDB
from unstract.sdk.x2txt import X2Text

logger = logging.getLogger(__name__)


class Constants:
    TOP_K = 5


class Index:
    def __init__(
        self,
        tool: BaseTool,
        run_id: Optional[str] = None,
        capture_metrics: bool = False,
    ):
        # TODO: Inherit from StreamMixin and avoid using BaseTool
        self.tool = tool
        self._run_id = run_id
        self._capture_metrics = capture_metrics
        self._metrics = {}

    @capture_metrics
    def query_index(
        self,
        embedding_instance_id: str,
        vector_db_instance_id: str,
        doc_id: str,
        usage_kwargs: dict[Any, Any] = {},
    ):
        embedding = Embedding(
            tool=self.tool,
            adapter_instance_id=embedding_instance_id,
            usage_kwargs=usage_kwargs,
        )

        vector_db = VectorDB(
            tool=self.tool,
            adapter_instance_id=vector_db_instance_id,
            embedding=embedding,
        )

        try:
            self.tool.stream_log(
                f">>> Querying '{vector_db_instance_id}' for {doc_id}..."
            )
            try:
                doc_id_eq_filter = MetadataFilter.from_dict(
                    {
                        "key": "doc_id",
                        "operator": FilterOperator.EQ,
                        "value": doc_id,
                    }
                )
                filters = MetadataFilters(filters=[doc_id_eq_filter])
                q = VectorStoreQuery(
                    query_embedding=embedding.get_query_embedding(" "),
                    doc_ids=[doc_id],
                    filters=filters,
                    similarity_top_k=Constants.TOP_K,
                )
            except Exception as e:
                self.tool.stream_log(
                    f"Error while building vector DB query: {e}", level=LogLevel.ERROR
                )
                raise VectorDBError(
                    f"Failed to construct query for {vector_db}: {e}", actual_err=e
                )
            try:
                n: VectorStoreQueryResult = vector_db.query(query=q)
                if len(n.nodes) > 0:
                    self.tool.stream_log(f"Found {len(n.nodes)} nodes for {doc_id}")
                    all_text = ""
                    for node in n.nodes:
                        all_text += node.get_content()
                    return all_text
                else:
                    self.tool.stream_log(f"No nodes found for {doc_id}")
                    return None
            except Exception as e:
                self.tool.stream_log(
                    f"Error while executing vector DB query: {e}", level=LogLevel.ERROR
                )
                raise VectorDBError(
                    f"Failed to execute query on {vector_db}: {e}", actual_err=e
                )
        finally:
            vector_db.close()

    @log_elapsed(operation="EXTRACTION")
    def extract_text(
        self,
        x2text_instance_id: str,
        file_path: str,
        output_file_path: Optional[str] = None,
        enable_highlight: bool = False,
        usage_kwargs: dict[Any, Any] = {},
        process_text: Optional[Callable[[str], str]] = None,
        fs: FileStorage = FileStorage(FileStorageProvider.LOCAL),
        tags: Optional[list[str]] = None,
    ) -> str:
        """Extracts text from a document.

        Uses the configured service to perform the extraction
        - LLMWhisperer
        - Unstructured IO Community / Enterprise
        - Llama Parse

        Args:
            x2text_instance_id (str): UUID of the text extractor
            file_path (str): Path to the file
            output_file_path (Optional[str], optional): File path to write
                the extracted contents into. Defaults to None.
            enable_highlight (bool, optional): Flag to provide highlighting metadata.
                Defaults to False.
            usage_kwargs (dict[Any, Any], optional): Dict to capture usage.
                Defaults to {}.
            process_text (Optional[Callable[[str], str]], optional): Optional function
                to post-process the text. Defaults to None.
            tags: (Optional[list[str]], optional): Tags

        Raises:
            IndexingError: Errors during text extraction
        """
        self.tool.stream_log("Extracting text from input file")
        extracted_text = ""
        x2text = X2Text(
            tool=self.tool,
            adapter_instance_id=x2text_instance_id,
            usage_kwargs=usage_kwargs,
        )
        try:
            if enable_highlight and (isinstance(x2text.x2text_instance, LLMWhisperer) or isinstance(x2text.x2text_instance, LLMWhispererV2)):
                process_response: TextExtractionResult = x2text.process(
                    input_file_path=file_path,
                    output_file_path=output_file_path,
                    enable_highlight=enable_highlight,
                    tags=tags,
                    fs=fs,
                )
                whisper_hash_value = process_response.extraction_metadata.whisper_hash
                metadata = {X2TextConstants.WHISPER_HASH: whisper_hash_value}
                if hasattr(self.tool, 'update_exec_metadata'):
                    self.tool.update_exec_metadata(metadata)
            else:
                process_response: TextExtractionResult = x2text.process(
                    input_file_path=file_path,
                    output_file_path=output_file_path,
                    tags=tags,
                    fs=fs,
                )
            extracted_text = process_response.extracted_text
        # TODO: Handle prepend of context where error is raised and remove this
        except AdapterError as e:
            msg = f"Error from text extractor '{x2text.x2text_instance.get_name()}'. "
            msg += str(e)
            raise X2TextError(msg) from e
        if process_text:
            try:
                result = process_text(extracted_text)
                if isinstance(result, str):
                    extracted_text = result
                else:
                    logger.warning("'process_text' is expected to return an 'str'")
            except Exception as e:
                logger.error(
                    f"Error occured inside callable 'process_text': {e}\n"
                    "continuing processing..."
                )
        return extracted_text

    # TODO: Reduce the number of params by some dataclass
    @log_elapsed(operation="CHECK_AND_INDEX(overall)")
    @capture_metrics
    def index(
        self,
        tool_id: str,
        embedding_instance_id: str,
        vector_db_instance_id: str,
        x2text_instance_id: str,
        file_path: str,
        chunk_size: int,
        chunk_overlap: int,
        reindex: bool = False,
        file_hash: Optional[str] = None,
        output_file_path: Optional[str] = None,
        enable_highlight: bool = False,
        usage_kwargs: dict[Any, Any] = {},
        process_text: Optional[Callable[[str], str]] = None,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
        tags: Optional[list[str]] = None,
    ) -> str:
        """Indexes an individual file using the passed arguments.

        Args:
            tool_id (str): UUID of the tool (workflow_id in case it's called
                from workflow)
            embedding_instance_id (str): UUID of the embedding service configured
            vector_db_instance_id (str): UUID of the vector DB configured
            x2text_instance_id (str): UUID of the x2text adapter configured.
                This is to extract text from documents.
            file_path (str): Path to the file that needs to be indexed.
            chunk_size (int): Chunk size to be used for indexing
            chunk_overlap (int): Overlap in chunks to be used for indexing
            reindex (bool, optional): Flag to denote if document should be
                re-indexed if its already indexed. Defaults to False.
            file_hash (Optional[str], optional): SHA256 hash of the file.
                Defaults to None. If None, the hash is generated.
            output_file_path (Optional[str], optional): File path to write
                the extracted contents into. Defaults to None.
            fs (FileStorage): file storage object to perfrom file operations
            tags (Optional[list[str]], optional): List of tags to be associated with
                the indexed document.

        Returns:
            str: A unique ID for the file and indexing arguments combination
        """
        doc_id = self.generate_index_key(
            vector_db=vector_db_instance_id,
            embedding=embedding_instance_id,
            x2text=x2text_instance_id,
            chunk_size=str(chunk_size),
            chunk_overlap=str(chunk_overlap),
            file_path=file_path,
            file_hash=file_hash,
            fs=fs,
        )
        self.tool.stream_log(f"Checking if doc_id {doc_id} exists")
        embedding = Embedding(
            tool=self.tool,
            adapter_instance_id=embedding_instance_id,
            usage_kwargs=usage_kwargs,
        )

        vector_db = VectorDB(
            tool=self.tool,
            adapter_instance_id=vector_db_instance_id,
            embedding=embedding,
        )

        try:
            # Checking if document is already indexed against doc_id
            doc_id_eq_filter = MetadataFilter.from_dict(
                {"key": "doc_id", "operator": FilterOperator.EQ, "value": doc_id}
            )
            filters = MetadataFilters(filters=[doc_id_eq_filter])
            q = VectorStoreQuery(
                query_embedding=embedding.get_query_embedding(" "),
                doc_ids=[doc_id],
                filters=filters,
            )

            doc_id_found = False
            try:
                n: VectorStoreQueryResult = vector_db.query(query=q)
                if len(n.nodes) > 0:
                    doc_id_found = True
                    self.tool.stream_log(f"Found {len(n.nodes)} nodes for {doc_id}")
                else:
                    self.tool.stream_log(f"No nodes found for {doc_id}")
            except Exception as e:
                self.tool.stream_log(
                    f"Error querying {vector_db_instance_id}: {e}, proceeding to index",
                    level=LogLevel.ERROR,
                )

            if doc_id_found and not reindex:
                self.tool.stream_log(f"File was indexed already under {doc_id}")
                if output_file_path and not fs.exists(output_file_path):
                    # Added this as a workaround to handle extraction
                    # for documents uploaded twice in different projects.
                    # to be reconsidered after permanent fixes.
                    extracted_text = self.extract_text(
                        x2text_instance_id=x2text_instance_id,
                        file_path=file_path,
                        output_file_path=output_file_path,
                        enable_highlight=enable_highlight,
                        usage_kwargs=usage_kwargs,
                        process_text=process_text,
                        fs=fs,
                        tags=tags,
                    )
                return doc_id

            extracted_text = self.extract_text(
                x2text_instance_id=x2text_instance_id,
                file_path=file_path,
                output_file_path=output_file_path,
                enable_highlight=enable_highlight,
                usage_kwargs=usage_kwargs,
                process_text=process_text,
                tags=tags,
                fs=fs,
            )
            if not extracted_text:
                raise IndexingError("No text available to index")

            # For No-op adapters, addition of nodes to vectorDB should not happen
            # and this has to be handled in the adapter level. But there are a
            # few challenges considering callback manager and upstream Llama index
            # method invocations. Hence, making this check here and returning
            # the doc id to maintain the legacy flow of adapters.

            if isinstance(
                vector_db.get_vector_db(
                    adapter_instance_id=vector_db_instance_id, embedding_dimension=1
                ),
                (NoOpCustomVectorDB),
            ):
                return doc_id

            self.index_to_vector_db(
                vector_db=vector_db,
                embedding=embedding,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                doc_id=doc_id,
                text_to_idx=extracted_text,
                doc_id_found=doc_id_found,
            )
            return doc_id
        finally:
            vector_db.close()

    @log_elapsed(operation="INDEXING")
    def index_to_vector_db(
        self,
        vector_db: VectorDB,
        embedding: Embedding,
        chunk_size: int,
        chunk_overlap: int,
        text_to_idx: str,
        doc_id: str,
        doc_id_found: bool,
    ):
        self.tool.stream_log("Indexing file...")
        full_text = [
            {
                "section": "full",
                "text_contents": text_to_idx,
            }
        ]
        # Check if chunking is required
        documents = []
        for item in full_text:
            text = item["text_contents"]
            document = Document(
                text=text,
                doc_id=doc_id,
                metadata={"section": item["section"]},
            )
            document.id_ = doc_id
            documents.append(document)
        self.tool.stream_log(f"Number of documents: {len(documents)}")

        if doc_id_found:
            # Delete the nodes for the doc_id
            try:
                vector_db.delete(ref_doc_id=doc_id)
                self.tool.stream_log(f"Deleted nodes for {doc_id}")
            except Exception as e:
                self.tool.stream_log(
                    f"Error deleting nodes for {doc_id}: {e}",
                    level=LogLevel.ERROR,
                )
                raise SdkError(f"Error deleting nodes for {doc_id}: {e}") from e

        try:
            if chunk_size == 0:
                parser = SentenceSplitter.from_defaults(
                    chunk_size=len(documents[0].text) + 10,
                    chunk_overlap=0,
                    callback_manager=embedding.get_callback_manager(),
                )
                nodes = parser.get_nodes_from_documents(documents, show_progress=True)
                node = nodes[0]
                node.embedding = embedding.get_query_embedding(" ")
                vector_db.add(doc_id, nodes=[node])
                self.tool.stream_log("Added node to vector db")
            else:
                self.tool.stream_log("Adding nodes to vector db...")
                # TODO: Phase 2:
                # Post insertion to VDB, use query using doc_id and
                # store all the VDB ids to a table against the doc_id
                # During deletion for cases where metadata filtering
                # does not work, these ids can be used for direct deletion
                # This new table will also act like an audit trail for
                # all nodes that were added to the VDB by Unstract
                # Once this is in place, the overridden implementation
                # of prefixing ids with doc_id before adding to VDB
                # can be removed
                vector_db.index_document(
                    documents,
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap,
                    show_progress=True,
                )
        except Exception as e:
            self.tool.stream_log(
                f"Error adding nodes to vector db: {e}",
                level=LogLevel.ERROR,
            )
            raise IndexingError(str(e)) from e

        self.tool.stream_log("File has been indexed successfully")
        return

    def generate_index_key(
        self,
        vector_db: str,
        embedding: str,
        x2text: str,
        chunk_size: str,
        chunk_overlap: str,
        file_path: Optional[str] = None,
        file_hash: Optional[str] = None,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> str:
        """Generates a unique ID useful for identifying files during indexing.

        Args:
            vector_db (str): UUID of the vector DB adapter
            embedding (str): UUID of the embedding adapter
            x2text (str): UUID of the X2Text adapter
            chunk_size (str): Chunk size for indexing
            chunk_overlap (str): Chunk overlap for indexing
            file_path (Optional[str]): Path to the file that needs to be indexed.
                Defaults to None. One of file_path or file_hash needs to be specified.
            file_hash (Optional[str], optional): SHA256 hash of the file.
                Defaults to None. If None, the hash is generated with file_path.
            fs (FileStorage): file storage object to perfrom file operations

        Returns:
            str: Key representing unique ID for a file
        """
        if not file_path and not file_hash:
            raise ValueError("One of `file_path` or `file_hash` need to be provided")

        if not file_hash:
            file_hash = fs.get_hash_from_file(path=file_path)

        # Whole adapter config is used currently even though it contains some keys
        # which might not be relevant to indexing. This is easier for now than
        # marking certain keys of the adapter config as necessary.
        index_key = {
            "file_hash": file_hash,
            "vector_db_config": ToolAdapter.get_adapter_config(self.tool, vector_db),
            "embedding_config": ToolAdapter.get_adapter_config(self.tool, embedding),
            "x2text_config": ToolAdapter.get_adapter_config(self.tool, x2text),
            # Typed and hashed as strings since the final hash is persisted
            # and this is required to be backward compatible
            "chunk_size": str(chunk_size),
            "chunk_overlap": str(chunk_overlap),
        }
        # JSON keys are sorted to ensure that the same key gets hashed even in
        # case where the fields are reordered.
        hashed_index_key = ToolUtils.hash_str(json.dumps(index_key, sort_keys=True))
        return hashed_index_key

    def get_metrics(self):
        return self._metrics

    def clear_metrics(self):
        self._metrics = {}

    @deprecated(version="0.45.0", reason="Use generate_index_key() instead")
    def generate_file_id(
        self,
        tool_id: str,
        vector_db: str,
        embedding: str,
        x2text: str,
        chunk_size: str,
        chunk_overlap: str,
        file_path: Optional[str] = None,
        file_hash: Optional[str] = None,
    ) -> str:
        return self.generate_index_key(
            vector_db,
            embedding,
            x2text,
            chunk_size,
            chunk_overlap,
            file_path,
            file_hash,
            fs=FileStorage(provider=FileStorageProvider.LOCAL),
        )

    @deprecated(version="0.50.0", reason="Use index() instead")
    def index_file(
        self,
        tool_id: str,
        embedding_type: str,
        vector_db: str,
        x2text_adapter: str,
        file_path: str,
        chunk_size: int,
        chunk_overlap: int,
        reindex: bool = False,
        file_hash: Optional[str] = None,
        output_file_path: Optional[str] = None,
    ) -> str:
        return self.index(
            tool_id=tool_id,
            embedding_instance_id=embedding_type,
            vector_db_instance_id=vector_db,
            x2text_instance_id=x2text_adapter,
            file_path=file_path,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            reindex=reindex,
            file_hash=file_hash,
            output_file_path=output_file_path,
        )

    @deprecated("Deprecated class and method. Use Index and query_index() instead")
    def get_text_from_index(
        self, embedding_type: str, vector_db: str, doc_id: str
    ) -> Optional[str]:
        return self.query_index(
            embedding_instance_id=embedding_type,
            vector_db_instance_id=vector_db,
            doc_id=doc_id,
        )


# Legacy
ToolIndex = Index


================================================
File: src/unstract/sdk/llm.py
================================================
import logging
import re
from typing import Any, Callable, Optional

from deprecated import deprecated
from llama_index.core.base.llms.types import CompletionResponseGen
from llama_index.core.llms import LLM as LlamaIndexLLM
from llama_index.core.llms import CompletionResponse
from openai import APIError as OpenAIAPIError
from openai import RateLimitError as OpenAIRateLimitError

from unstract.sdk.adapter import ToolAdapter
from unstract.sdk.adapters.constants import Common
from unstract.sdk.adapters.llm import adapters
from unstract.sdk.adapters.llm.exceptions import parse_llm_err
from unstract.sdk.adapters.llm.llm_adapter import LLMAdapter
from unstract.sdk.constants import LogLevel, ToolEnv
from unstract.sdk.exceptions import LLMError, RateLimitError, SdkError
from unstract.sdk.helper import SdkHelper
from unstract.sdk.tool.base import BaseTool
from unstract.sdk.utils.callback_manager import CallbackManager
from unstract.sdk.utils.common_utils import capture_metrics

logger = logging.getLogger(__name__)


class LLM:
    """Interface to handle all LLM interactions."""

    json_regex = re.compile(r"\[(?:.|\n)*\]|\{(?:.|\n)*\}")
    llm_adapters = adapters
    MAX_TOKENS = 1024 * 4
    RESPONSE = "response"

    def __init__(
        self,
        tool: BaseTool,
        adapter_instance_id: Optional[str] = None,
        usage_kwargs: dict[Any, Any] = {},
        capture_metrics: bool = False,
    ):
        """Creates an instance of this LLM class.

        Args:
            tool (BaseTool): Instance of BaseTool to expose function to stream logs
            adapter_instance_id (Optional[str], optional): UUID of the adapter in
                Unstract. Defaults to None.
            usage_kwargs (dict[Any, Any], optional): Dict to capture token usage with
                callbacks. Defaults to {}.
        """
        self._tool = tool
        self._adapter_instance_id = adapter_instance_id
        self._llm_instance: LlamaIndexLLM = None
        self._usage_kwargs = usage_kwargs
        self._capture_metrics = capture_metrics
        self._run_id = usage_kwargs.get("run_id")
        self._usage_reason = usage_kwargs.get("llm_usage_reason")
        self._metrics = {}
        self._initialise()

    def _initialise(self):
        if self._adapter_instance_id:
            self._llm_instance = self._get_llm(self._adapter_instance_id)
            self._usage_kwargs["adapter_instance_id"] = self._adapter_instance_id

            if not SdkHelper.is_public_adapter(adapter_id=self._adapter_instance_id):
                platform_api_key = self._tool.get_env_or_die(ToolEnv.PLATFORM_API_KEY)
                CallbackManager.set_callback(
                    platform_api_key=platform_api_key,
                    model=self._llm_instance,
                    kwargs=self._usage_kwargs,
                )

    @capture_metrics
    def complete(
        self,
        prompt: str,
        extract_json: bool = True,
        process_text: Optional[Callable[[str], str]] = None,
        **kwargs: Any,
    ) -> dict[str, Any]:
        """Generates a completion response for the given prompt and captures
        metrics if run_id is provided.

        Args:
            prompt (str): The input text prompt for generating the completion.
            extract_json (bool, optional): If set to True, the response text is
                processed using a regex to extract JSON content from it. If no JSON is
                found, the text is returned as it is. Defaults to True.
            process_text (Optional[Callable[[str], str]], optional): A callable that
                processes the generated text and extracts specific information.
                Defaults to None.
            **kwargs (Any): Additional arguments passed to the completion function.

        Returns:
            dict[str, Any]: A dictionary containing the result of the completion,
                any processed output, and the captured metrics (if applicable).
        """
        try:
            response: CompletionResponse = self._llm_instance.complete(prompt, **kwargs)
            process_text_output = {}
            if extract_json:
                match = LLM.json_regex.search(response.text)
                if match:
                    response.text = match.group(0)
            if process_text:
                try:
                    process_text_output = process_text(response, extract_json)
                    if not isinstance(process_text_output, dict):
                        process_text_output = {}
                except Exception as e:
                    logger.error(f"Error occurred inside function 'process_text': {e}")
                    process_text_output = {}
            response_data = {LLM.RESPONSE: response, **process_text_output}
            return response_data
        except Exception as e:
            raise parse_llm_err(e, self._llm_adapter_class) from e

    def get_metrics(self):
        return self._metrics

    def get_usage_reason(self):
        return self._usage_reason

    def stream_complete(
        self,
        prompt: str,
        **kwargs: Any,
    ) -> CompletionResponseGen:
        try:
            response: CompletionResponseGen = self._llm_instance.stream_complete(
                prompt, **kwargs
            )
            return response
        except Exception as e:
            raise parse_llm_err(e, self._llm_adapter_class) from e

    def _get_llm(self, adapter_instance_id: str) -> LlamaIndexLLM:
        """Returns the LLM object for the tool.

        Returns:
            LLM: The LLM object for the tool.
            (llama_index.llms.base.LLM)
        """
        try:
            if not self._adapter_instance_id:
                raise LLMError("Adapter instance ID not set. " "Initialisation failed")

            llm_config_data = ToolAdapter.get_adapter_config(
                self._tool, self._adapter_instance_id
            )

            llm_adapter_id = llm_config_data.get(Common.ADAPTER_ID)
            if llm_adapter_id not in self.llm_adapters:
                raise SdkError(f"LLM adapter not supported : " f"{llm_adapter_id}")

            llm_adapter = self.llm_adapters[llm_adapter_id][Common.METADATA][
                Common.ADAPTER
            ]
            llm_metadata = llm_config_data.get(Common.ADAPTER_METADATA)
            self._llm_adapter_class: LLMAdapter = llm_adapter(llm_metadata)
            self._usage_kwargs["provider"] = self._llm_adapter_class.get_provider()
            llm_instance: LLM = self._llm_adapter_class.get_llm_instance()
            return llm_instance
        except Exception as e:
            self._tool.stream_log(
                log=f"Unable to get llm instance: {e}", level=LogLevel.ERROR
            )
            raise LLMError(f"Error getting llm instance: {e}") from e

    def get_max_tokens(self, reserved_for_output: int = 0) -> int:
        """Returns the maximum number of tokens that can be used for the LLM.

        Args:
            reserved_for_output (int): The number of tokens reserved for the
                                        output.
                The default is 0.

            Returns:
                int: The maximum number of tokens that can be used for the LLM.
        """
        return self.MAX_TOKENS - reserved_for_output

    def set_max_tokens(self, max_tokens: int) -> None:
        """Set the maximum number of tokens that can be used for the LLM.

        Args:
            max_tokens (int): The number of tokens to be used at the maximum

            Returns:
                None
        """
        self._llm_instance.max_tokens = max_tokens

    def get_class_name(self) -> str:
        """Gets the class name of the Llama Index LLM.

        Args:
            NA

            Returns:
                Class name
        """
        return self._llm_instance.class_name()

    def get_model_name(self) -> str:
        """Gets the name of the LLM model.

        Args:
            NA

        Returns:
            LLM model name
        """
        return self._llm_instance.model

    @deprecated("Use LLM instead of ToolLLM")
    def get_llm(self, adapter_instance_id: Optional[str] = None) -> LlamaIndexLLM:
        if not self._llm_instance:
            self._adapter_instance_id = adapter_instance_id
            self._initialise()
        return self._llm_instance

    @classmethod
    @deprecated("Instantiate LLM and call complete() instead")
    def run_completion(
        cls,
        llm: LlamaIndexLLM,
        platform_api_key: str,
        prompt: str,
        retries: int = 3,
        **kwargs: Any,
    ) -> Optional[dict[str, Any]]:
        # Setup callback manager to collect Usage stats
        CallbackManager.set_callback_manager(
            platform_api_key=platform_api_key, llm=llm, **kwargs
        )
        # Removing specific keys from kwargs
        new_kwargs = kwargs.copy()
        for key in [
            "workflow_id",
            "execution_id",
            "adapter_instance_id",
            "run_id",
        ]:
            new_kwargs.pop(key, None)

        try:
            response: CompletionResponse = llm.complete(prompt, **new_kwargs)
            match = cls.json_regex.search(response.text)
            if match:
                response.text = match.group(0)
            return {"response": response}
        # TODO: Handle for all LLM providers
        except OpenAIAPIError as e:
            msg = "OpenAI error: "
            msg += e.message
            if hasattr(e, "body") and isinstance(e.body, dict) and "message" in e.body:
                msg += e.body["message"]
            if isinstance(e, OpenAIRateLimitError):
                raise RateLimitError(msg)
            raise LLMError(msg) from e


# Legacy
ToolLLM = LLM


================================================
File: src/unstract/sdk/metrics_mixin.py
================================================
import logging
import os
import time
import uuid
from typing import Any

from redis import StrictRedis

logger = logging.getLogger(__name__)


class MetricsMixin:
    TIME_TAKEN_KEY = "time_taken(s)"

    def __init__(self, run_id):
        """Initialize the MetricsMixin class.

        Args:
            run_id (str): Unique identifier for the run.
        """
        self.run_id = run_id
        self.op_id = str(uuid.uuid4())  # Unique identifier for this instance
        self.redis_client = None
        try:
            # Initialize Redis client
            self.redis_client = StrictRedis(
                host=os.getenv("REDIS_HOST", "unstract-redis"),
                port=int(os.getenv("REDIS_PORT", 6379)),
                username=os.getenv("REDIS_USER", "default"),
                password=os.getenv("REDIS_PASSWORD", ""),
                db=1,
                decode_responses=True,
            )
        except Exception as e:
            logger.error(
                "Failed to initialize Redis client" f" for run_id={run_id}: {e}"
            )

        self.redis_key = f"metrics:{self.run_id}:{self.op_id}"

        # Set the start time immediately upon initialization
        self.set_start_time()

    def set_start_time(self, ttl=86400):
        """Store the current timestamp in Redis when the instance is
        created."""
        if self.redis_client is None:
            logger.error("Redis client is not initialized. Cannot set start time.")
            return
        self.redis_client.set(self.redis_key, time.time(), ex=ttl)

    def collect_metrics(self) -> dict[str, Any]:
        """Calculate the time taken since the timestamp was set and delete the
        Redis key.

        Returns:
            dict: The calculated time taken and the associated run_id and op_id.
        """

        if self.redis_client is None:
            logger.error("Redis client is not initialized. Cannot collect metrics.")
            return {self.TIME_TAKEN_KEY: None}

        if not self.redis_client.exists(self.redis_key):
            return {self.TIME_TAKEN_KEY: None}

        start_time = float(self.redis_client.get(self.redis_key))
        time_taken = round(time.time() - start_time, 3)

        # Delete the Redis key after use
        self.redis_client.delete(self.redis_key)

        return {self.TIME_TAKEN_KEY: time_taken}


================================================
File: src/unstract/sdk/ocr.py
================================================
from abc import ABCMeta
from typing import Optional

from deprecated import deprecated

from unstract.sdk.adapter import ToolAdapter
from unstract.sdk.adapters.constants import Common
from unstract.sdk.adapters.ocr import adapters
from unstract.sdk.adapters.ocr.ocr_adapter import OCRAdapter
from unstract.sdk.constants import LogLevel
from unstract.sdk.exceptions import OCRError
from unstract.sdk.tool.base import BaseTool


class OCR(metaclass=ABCMeta):
    def __init__(
        self,
        tool: BaseTool,
        adapter_instance_id: Optional[str] = None,
    ):
        self._tool = tool
        self._ocr_adapters = adapters
        self._adapter_instance_id = adapter_instance_id
        self._ocr_instance: OCRAdapter = None
        self._initialise(adapter_instance_id)

    def _initialise(self, adapter_instance_id):
        if self._adapter_instance_id:
            self._ocr_instance: OCRAdapter = self._get_ocr()

    def _get_ocr(self) -> Optional[OCRAdapter]:
        try:
            if not self._adapter_instance_id:
                raise OCRError("Adapter instance ID not set. " "Initialisation failed")
            ocr_config = ToolAdapter.get_adapter_config(
                self._tool, self._adapter_instance_id
            )
            ocr_adapter_id = ocr_config.get(Common.ADAPTER_ID)
            if ocr_adapter_id in self._ocr_adapters:
                ocr_adapter = self._ocr_adapters[ocr_adapter_id][Common.METADATA][
                    Common.ADAPTER
                ]
                ocr_metadata = ocr_config.get(Common.ADAPTER_METADATA)
                self._ocr_instance = ocr_adapter(ocr_metadata)

                return self._ocr_instance

        except Exception as e:
            self._tool.stream_log(
                log=f"Unable to get OCR adapter {self._adapter_instance_id}: {e}",
                level=LogLevel.ERROR,
            )
            return None

    def process(
        self, input_file_path: str, output_file_path: Optional[str] = None
    ) -> str:
        return self._ocr_instance.process(input_file_path, output_file_path)

    @deprecated("Instantiate OCR and call process() instead")
    def get_x2text(self, adapter_instance_id: str) -> OCRAdapter:
        if not self._ocr_instance:
            self._adapter_instance_id = adapter_instance_id
            self._ocr_instance: OCRAdapter = self._get_ocr()
        return self._ocr_instance


================================================
File: src/unstract/sdk/platform.py
================================================
from typing import Any, Optional

import requests

from unstract.sdk.constants import LogLevel, ToolEnv
from unstract.sdk.helper import SdkHelper
from unstract.sdk.tool.base import BaseTool


class PlatformBase:
    """Base class to handle interactions with Unstract's platform service.

    Notes:
        - PLATFORM_SERVICE_API_KEY environment variable is required.
    """

    def __init__(
        self,
        tool: BaseTool,
        platform_host: str,
        platform_port: str,
    ) -> None:
        """
        Args:
            tool (AbstractTool): Instance of AbstractTool
            platform_host (str): Host of platform service
            platform_port (str): Port of platform service

        Notes:
            - PLATFORM_SERVICE_API_KEY environment variable is required.
        """
        self.tool = tool
        self.base_url = SdkHelper.get_platform_base_url(platform_host, platform_port)
        self.bearer_token = tool.get_env_or_die(ToolEnv.PLATFORM_API_KEY)


class PlatformHelper(PlatformBase):
    """Implementation of `UnstractPlatformBase` to interact with platform
    service.

    Notes:
        - PLATFORM_SERVICE_API_KEY environment variable is required.
    """

    def __init__(self, tool: BaseTool, platform_host: str, platform_port: str):
        """Constructor of the implementation of `UnstractPlatformBase`

        Args:
            tool (AbstractTool): Instance of AbstractTool
            platform_host (str): Host of platform service
            platform_port (str): Port of platform service
        """
        super().__init__(
            tool=tool, platform_host=platform_host, platform_port=platform_port
        )

    def get_platform_details(self) -> Optional[dict[str, Any]]:
        """Obtains platform details associated with the platform key.

        Currently helps fetch organization ID related to the key.

        Returns:
            Optional[dict[str, Any]]: Dictionary containing the platform details
        """
        url = f"{self.base_url}/platform_details"
        headers = {"Authorization": f"Bearer {self.bearer_token}"}
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            self.tool.stream_log(
                (
                    "Error while retrieving platform details: "
                    f"[{response.status_code}] {response.reason}"
                ),
                level=LogLevel.ERROR,
            )
            return None
        else:
            platform_details: dict[str, Any] = response.json().get("details")
            return platform_details


================================================
File: src/unstract/sdk/prompt.py
================================================
import logging
from typing import Any, Optional

import requests
from requests import ConnectionError, RequestException, Response

from unstract.sdk.constants import LogLevel, MimeType, PromptStudioKeys, ToolEnv
from unstract.sdk.helper import SdkHelper
from unstract.sdk.tool.base import BaseTool
from unstract.sdk.utils.common_utils import log_elapsed

logger = logging.getLogger(__name__)


class PromptTool:
    """Class to handle prompt service methods for Unstract Tools."""

    def __init__(
        self,
        tool: BaseTool,
        prompt_host: str,
        prompt_port: str,
        is_public_call: bool = False,
    ) -> None:
        """
        Args:
            tool (AbstractTool): Instance of AbstractTool
            prompt_host (str): Host of platform service
            prompt_host (str): Port of platform service
        """
        self.tool = tool
        self.base_url = SdkHelper.get_platform_base_url(prompt_host, prompt_port)
        self.is_public_call = is_public_call
        if not is_public_call:
            self.bearer_token = tool.get_env_or_die(ToolEnv.PLATFORM_API_KEY)

    @log_elapsed(operation="ANSWER_PROMPTS")
    def answer_prompt(
        self, payload: dict[str, Any], params: Optional[dict[str, str]] = None
    ) -> dict[str, Any]:
        url_path = "answer-prompt"
        if self.is_public_call:
            url_path = "answer-prompt-public"
        return self._post_call(
            url_path=url_path,
            payload=payload,
            params=params,
        )

    def single_pass_extraction(
        self, payload: dict[str, Any], params: Optional[dict[str, str]] = None
    ) -> dict[str, Any]:
        return self._post_call(
            url_path="single-pass-extraction",
            payload=payload,
            params=params,
        )

    def summarize(
        self, payload: dict[str, Any], params: Optional[dict[str, str]] = None
    ) -> dict[str, Any]:
        return self._post_call(url_path="summarize", payload=payload, params=params)

    def _post_call(
        self,
        url_path: str,
        payload: dict[str, Any],
        params: Optional[dict[str, str]] = None,
    ) -> dict[str, Any]:
        """Invokes and communicates to prompt service to fetch response for the
        prompt.

        Args:
            url_path (str): URL path to the service endpoint
            payload (dict): Payload to send in the request body
            params (dict, optional): Query parameters to include in the request

        Returns:
            dict: Response from the prompt service

            Sample Response:
            {
                "status": "OK",
                "error": "",
                "cost": 0,
                structure_output : {}
            }
        """
        result: dict[str, Any] = {
            "status": "ERROR",
            "error": "",
            "cost": 0,
            "structure_output": "",
            "status_code": 500,
        }
        url: str = f"{self.base_url}/{url_path}"
        headers: dict[str, str] = {}
        if not self.is_public_call:
            headers = {"Authorization": f"Bearer {self.bearer_token}"}
        response: Response = Response()
        try:
            response = requests.post(
                url=url, json=payload, params=params, headers=headers
            )
            response.raise_for_status()
            result["status"] = "OK"
            result["structure_output"] = response.text
            result["status_code"] = 200
        except ConnectionError as connect_err:
            msg = "Unable to connect to prompt service. Please contact admin."
            self._stringify_and_stream_err(connect_err, msg)
            result["error"] = msg
        except RequestException as e:
            # Extract error information from the response if available
            error_message = str(e)
            content_type = response.headers.get("Content-Type", "").lower()
            if MimeType.JSON in content_type:
                response_json = response.json()
                if "error" in response_json:
                    error_message = response_json["error"]
            elif response.text:
                error_message = response.text
            result["error"] = error_message
            result["status_code"] = response.status_code
            self.tool.stream_log(
                f"Error while fetching response for prompt: {result['error']}",
                level=LogLevel.ERROR,
            )
        return result

    def _stringify_and_stream_err(self, err: RequestException, msg: str) -> None:
        error_message = str(err)
        trace = f"{msg}: {error_message}"
        self.tool.stream_log(trace, level=LogLevel.ERROR)
        logger.error(trace)

    @staticmethod
    def get_exported_tool(
        tool: BaseTool, prompt_registry_id: str
    ) -> Optional[dict[str, Any]]:
        """Get exported custom tool by the help of unstract DB tool.

        Args:
            prompt_registry_id (str): ID of the prompt_registry_id
            tool (AbstractTool): Instance of AbstractTool
        Required env variables:
            PLATFORM_HOST: Host of platform service
            PLATFORM_PORT: Port of platform service
        """
        platform_host = tool.get_env_or_die(ToolEnv.PLATFORM_HOST)
        platform_port = tool.get_env_or_die(ToolEnv.PLATFORM_PORT)
        base_url = SdkHelper.get_platform_base_url(platform_host, platform_port)
        bearer_token = tool.get_env_or_die(ToolEnv.PLATFORM_API_KEY)
        url = f"{base_url}/custom_tool_instance"
        query_params = {PromptStudioKeys.PROMPT_REGISTRY_ID: prompt_registry_id}
        headers = {"Authorization": f"Bearer {bearer_token}"}
        response = requests.get(url, headers=headers, params=query_params)
        if response.status_code == 200:
            return response.json()
        elif response.status_code == 404:
            tool.stream_error_and_exit(
                f"Exported tool '{prompt_registry_id}' is not found"
            )
            return None
        else:
            tool.stream_error_and_exit(
                f"Error while retrieving tool metadata "
                "for the exported tool: "
                f"{prompt_registry_id} / {response.reason}"
            )
            return None


================================================
File: src/unstract/sdk/vector_db.py
================================================
import logging
from collections.abc import Sequence
from typing import Any, Optional, Union

from deprecated import deprecated
from llama_index.core import StorageContext, VectorStoreIndex
from llama_index.core.indices.base import IndexType
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.schema import BaseNode, Document
from llama_index.core.vector_stores.types import (
    BasePydanticVectorStore,
    VectorStore,
    VectorStoreQueryResult,
)

from unstract.sdk.adapter import ToolAdapter
from unstract.sdk.adapters.constants import Common
from unstract.sdk.adapters.vectordb import adapters
from unstract.sdk.adapters.vectordb.constants import VectorDbConstants
from unstract.sdk.adapters.vectordb.exceptions import parse_vector_db_err
from unstract.sdk.constants import LogLevel, ToolEnv
from unstract.sdk.embedding import Embedding
from unstract.sdk.exceptions import SdkError, VectorDBError
from unstract.sdk.helper import SdkHelper
from unstract.sdk.platform import PlatformHelper
from unstract.sdk.tool.base import BaseTool

logger = logging.getLogger(__name__)


class VectorDB:
    """Class to handle VectorDB for Unstract Tools."""

    vector_db_adapters = adapters
    DEFAULT_EMBEDDING_DIMENSION = 1536
    EMBEDDING_INSTANCE_ERROR = (
        "Vector DB does not have an embedding initialised."
        "Migrate to VectorDB instead of deprecated ToolVectorDB "
        "and pass in an Embedding to proceed"
    )

    def __init__(
        self,
        tool: BaseTool,
        adapter_instance_id: Optional[str] = None,
        embedding: Optional[Embedding] = None,
    ):
        self._tool = tool
        self._adapter_instance_id = adapter_instance_id
        self._vector_db_instance = None
        self._embedding_instance = None
        self._embedding_dimension = VectorDB.DEFAULT_EMBEDDING_DIMENSION
        self._initialise(embedding)

    def _initialise(self, embedding: Optional[Embedding] = None):
        if embedding:
            self._embedding_instance = embedding._embedding_instance
            self._embedding_dimension = embedding._length
        if self._adapter_instance_id:
            self._vector_db_instance: Union[
                BasePydanticVectorStore, VectorStore
            ] = self._get_vector_db()

    def _get_org_id(self) -> str:
        platform_helper = PlatformHelper(
            tool=self._tool,
            platform_host=self._tool.get_env_or_die(ToolEnv.PLATFORM_HOST),
            platform_port=self._tool.get_env_or_die(ToolEnv.PLATFORM_PORT),
        )
        # fetch org id from bearer token
        platform_details = platform_helper.get_platform_details()
        if not platform_details:
            # Errors are logged by the SDK itself
            raise SdkError("Error getting platform details")
        account_id = platform_details.get("organization_id")
        return account_id

    def _get_vector_db(self) -> Union[BasePydanticVectorStore, VectorStore]:
        """Gets an instance of LlamaIndex's VectorStore.

        Returns:
            Union[BasePydanticVectorStore, VectorStore]: Vector store instance
        """
        try:
            if not self._adapter_instance_id:
                raise VectorDBError(
                    "Adapter instance ID not set. Initialisation failed"
                )

            vector_db_config = ToolAdapter.get_adapter_config(
                self._tool, self._adapter_instance_id
            )

            vector_db_adapter_id = vector_db_config.get(Common.ADAPTER_ID)
            if vector_db_adapter_id not in self.vector_db_adapters:
                raise SdkError(
                    f"VectorDB adapter not supported : " f"{vector_db_adapter_id}"
                )

            vector_db_adapter = self.vector_db_adapters[vector_db_adapter_id][
                Common.METADATA
            ][Common.ADAPTER]
            vector_db_metadata = vector_db_config.get(Common.ADAPTER_METADATA)
            # Adding the collection prefix and embedding type
            # to the metadata

            if not SdkHelper.is_public_adapter(adapter_id=self._adapter_instance_id):
                org = self._get_org_id()
                vector_db_metadata[VectorDbConstants.VECTOR_DB_NAME] = org

            vector_db_metadata[
                VectorDbConstants.EMBEDDING_DIMENSION
            ] = self._embedding_dimension

            self.vector_db_adapter_class = vector_db_adapter(vector_db_metadata)
            return self.vector_db_adapter_class.get_vector_db_instance()
        except Exception as e:
            self._tool.stream_log(
                log=f"Unable to get vector_db {self._adapter_instance_id}: {e}",
                level=LogLevel.ERROR,
            )
            raise VectorDBError(f"Error getting vectorDB instance: {e}") from e

    def index_document(
        self,
        documents: Sequence[Document],
        chunk_size: int = 1024,
        chunk_overlap: int = 128,
        show_progress: bool = False,
        **index_kwargs,
    ) -> IndexType:
        if not self._embedding_instance:
            raise VectorDBError(self.EMBEDDING_INSTANCE_ERROR)
        storage_context = self.get_storage_context()
        parser = SentenceSplitter.from_defaults(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            callback_manager=self._embedding_instance.callback_manager,
        )
        return VectorStoreIndex.from_documents(
            documents,
            storage_context=storage_context,
            show_progress=show_progress,
            embed_model=self._embedding_instance,
            transformations=[parser],
            callback_manager=self._embedding_instance.callback_manager,
            **index_kwargs,
        )

    @deprecated(version="0.47.0", reason="Use index_document() instead")
    def get_vector_store_index_from_storage_context(
        self,
        documents: Sequence[Document],
        storage_context: Optional[StorageContext] = None,
        show_progress: bool = False,
        callback_manager=None,
        **kwargs,
    ) -> IndexType:
        if not self._embedding_instance:
            raise VectorDBError(self.EMBEDDING_INSTANCE_ERROR)
        parser = kwargs.get("node_parser")
        return VectorStoreIndex.from_documents(
            documents,
            storage_context=storage_context,
            show_progress=show_progress,
            embed_model=self._embedding_instance,
            node_parser=parser,
            callback_manager=self._embedding_instance.callback_manager,
        )

    def get_vector_store_index(self, **kwargs: Any) -> VectorStoreIndex:
        if not self._embedding_instance:
            raise VectorDBError(self.EMBEDDING_INSTANCE_ERROR)
        return VectorStoreIndex.from_vector_store(
            vector_store=self._vector_db_instance,
            embed_model=self._embedding_instance,
            callback_manager=self._embedding_instance.callback_manager,
        )

    def get_storage_context(self) -> StorageContext:
        return StorageContext.from_defaults(vector_store=self._vector_db_instance)

    def query(self, query) -> VectorStoreQueryResult:
        try:
            return self._vector_db_instance.query(query=query)
        except Exception as e:
            raise parse_vector_db_err(e, self.vector_db_adapter_class) from e

    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
        if not self.vector_db_adapter_class:
            raise VectorDBError("Vector DB is not initialised properly")
        self.vector_db_adapter_class.delete(
            ref_doc_id=ref_doc_id, delete_kwargs=delete_kwargs
        )

    def add(
        self,
        ref_doc_id,
        nodes: list[BaseNode],
    ) -> list[str]:
        if not self.vector_db_adapter_class:
            raise VectorDBError("Vector DB is not initialised properly")
        self.vector_db_adapter_class.add(
            ref_doc_id=ref_doc_id,
            nodes=nodes,
        )

    def close(self, **kwargs):
        if not self.vector_db_adapter_class:
            raise VectorDBError("Vector DB is not initialised properly")
        self.vector_db_adapter_class.close()

    def get_class_name(self) -> str:
        """Gets the class name of the Llama Index Vector DB.

        Args:
            NA

            Returns:
                Class name
        """
        return self._vector_db_instance.class_name()

    @deprecated("Use VectorDB instead of ToolVectorDB")
    def get_vector_db(
        self, adapter_instance_id: str, embedding_dimension: int
    ) -> Union[BasePydanticVectorStore, VectorStore]:
        if not self._vector_db_instance:
            self._adapter_instance_id = adapter_instance_id
            self._initialise()
        return self._vector_db_instance


# Legacy
ToolVectorDB = VectorDB


================================================
File: src/unstract/sdk/x2txt.py
================================================
import io
from abc import ABCMeta
from typing import Any, Optional

import pdfplumber
from deprecated import deprecated

from unstract.sdk.adapter import ToolAdapter
from unstract.sdk.adapters.constants import Common
from unstract.sdk.adapters.x2text import adapters
from unstract.sdk.adapters.x2text.constants import X2TextConstants
from unstract.sdk.adapters.x2text.dto import TextExtractionResult
from unstract.sdk.adapters.x2text.llm_whisperer.src import LLMWhisperer
from unstract.sdk.adapters.x2text.llm_whisperer.src.constants import WhispererConfig
from unstract.sdk.adapters.x2text.x2text_adapter import X2TextAdapter
from unstract.sdk.audit import Audit
from unstract.sdk.constants import LogLevel, MimeType, ToolEnv
from unstract.sdk.exceptions import X2TextError
from unstract.sdk.file_storage import FileStorage, FileStorageProvider
from unstract.sdk.helper import SdkHelper
from unstract.sdk.tool.base import BaseTool
from unstract.sdk.utils import ToolUtils


class X2Text(metaclass=ABCMeta):
    def __init__(
        self,
        tool: BaseTool,
        adapter_instance_id: Optional[str] = None,
        usage_kwargs: dict[Any, Any] = {},
    ):
        self._tool = tool
        self._x2text_adapters = adapters
        self._adapter_instance_id = adapter_instance_id
        self._x2text_instance: X2TextAdapter = None
        self._usage_kwargs = usage_kwargs
        self._initialise()

    @property
    def x2text_instance(self):
        return self._x2text_instance

    def _initialise(self):
        if self._adapter_instance_id:
            self._x2text_instance = self._get_x2text()

    def _get_x2text(self) -> X2TextAdapter:
        try:
            if not self._adapter_instance_id:
                raise X2TextError("Adapter instance ID not set. Initialisation failed")

            x2text_config = ToolAdapter.get_adapter_config(
                self._tool, self._adapter_instance_id
            )

            x2text_adapter_id = x2text_config.get(Common.ADAPTER_ID)
            if x2text_adapter_id in self._x2text_adapters:
                x2text_adapter = self._x2text_adapters[x2text_adapter_id][
                    Common.METADATA
                ][Common.ADAPTER]
                x2text_metadata = x2text_config.get(Common.ADAPTER_METADATA)
                # Add x2text service host, port and platform_service_key
                x2text_metadata[
                    X2TextConstants.X2TEXT_HOST
                ] = self._tool.get_env_or_die(X2TextConstants.X2TEXT_HOST)
                x2text_metadata[
                    X2TextConstants.X2TEXT_PORT
                ] = self._tool.get_env_or_die(X2TextConstants.X2TEXT_PORT)

                if not SdkHelper.is_public_adapter(
                    adapter_id=self._adapter_instance_id
                ):
                    x2text_metadata[
                        X2TextConstants.PLATFORM_SERVICE_API_KEY
                    ] = self._tool.get_env_or_die(
                        X2TextConstants.PLATFORM_SERVICE_API_KEY
                    )

                self._x2text_instance = x2text_adapter(x2text_metadata)

                return self._x2text_instance

        except Exception as e:
            self._tool.stream_log(
                log=f"Unable to get x2text adapter {self._adapter_instance_id}: {e}",
                level=LogLevel.ERROR,
            )
            raise X2TextError(f"Error getting text extractor: {e}") from e

    def process(
        self,
        input_file_path: str,
        output_file_path: Optional[str] = None,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
        **kwargs: dict[Any, Any],
    ) -> TextExtractionResult:
        mime_type = fs.mime_type(input_file_path)
        text_extraction_result: TextExtractionResult = None
        if mime_type == MimeType.TEXT:
            extracted_text = fs.read(path=input_file_path, mode="r", encoding="utf-8")
            text_extraction_result = TextExtractionResult(
                extracted_text=extracted_text, extraction_metadata=None
            )
        text_extraction_result = self._x2text_instance.process(
            input_file_path, output_file_path, fs, **kwargs
        )
        # The will be executed each and every time text extraction takes place
        self.push_usage_details(input_file_path, mime_type, fs=fs)
        return text_extraction_result

    @deprecated("Instantiate X2Text and call process() instead")
    def get_x2text(self, adapter_instance_id: str) -> X2TextAdapter:
        if not self._x2text_instance:
            self._adapter_instance_id = adapter_instance_id
            self._initialise()
        return self._x2text_instance

    def push_usage_details(
        self,
        input_file_path: str,
        mime_type: str,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> None:
        file_size = ToolUtils.get_file_size(input_file_path, fs)

        self._x2text_instance

        if mime_type == MimeType.PDF:
            pdf_contents = io.BytesIO(fs.read(path=input_file_path, mode="rb"))
            with pdfplumber.open(pdf_contents) as pdf:
                # calculate the number of pages
                page_count = len(pdf.pages)
            if isinstance(self._x2text_instance, LLMWhisperer):
                self._x2text_instance.config.get(WhispererConfig.PAGES_TO_EXTRACT)
                page_count = ToolUtils.calculate_page_count(
                    self._x2text_instance.config.get(WhispererConfig.PAGES_TO_EXTRACT),
                    page_count,
                )
            Audit().push_page_usage_data(
                platform_api_key=self._tool.get_env_or_die(ToolEnv.PLATFORM_API_KEY),
                file_size=file_size,
                file_type=mime_type,
                page_count=page_count,
                kwargs=self._usage_kwargs,
            )
        else:
            # We are allowing certain image types,and raw texts. We will consider them
            # as single page documents as there in no concept of page numbers.
            Audit().push_page_usage_data(
                platform_api_key=self._tool.get_env_or_die(ToolEnv.PLATFORM_API_KEY),
                file_size=file_size,
                file_type=mime_type,
                page_count=1,
                kwargs=self._usage_kwargs,
            )


================================================
File: src/unstract/sdk/adapters/__init__.py
================================================
import logging
from logging import NullHandler
from typing import Any

logging.getLogger(__name__).addHandler(NullHandler())

AdapterDict = dict[str, dict[str, Any]]



================================================
File: src/unstract/sdk/adapters/adapterkit.py
================================================
import logging
from typing import Any

from singleton_decorator import singleton

from unstract.sdk.adapters import AdapterDict
from unstract.sdk.adapters.base import Adapter
from unstract.sdk.adapters.constants import Common
from unstract.sdk.adapters.embedding import adapters as embedding_adapters
from unstract.sdk.adapters.llm import adapters as llm_adapters
from unstract.sdk.adapters.ocr import adapters as ocr_adapters
from unstract.sdk.adapters.vectordb import adapters as vectordb_adapters
from unstract.sdk.adapters.x2text import adapters as x2text_adapters

logger = logging.getLogger(__name__)


# Declaring this class as a Singleton to avoid initialising
# adapters list everytime
@singleton
class Adapterkit:
    def __init__(self) -> None:
        self._adapters: AdapterDict = (
            embedding_adapters
            | llm_adapters
            | vectordb_adapters
            | x2text_adapters
            | ocr_adapters
        )

    @property
    def adapters(self) -> AdapterDict:
        return self._adapters

    def get_adapter_class_by_adapter_id(self, adapter_id: str) -> Adapter:
        if adapter_id in self._adapters:
            adapter_class: Adapter = self._adapters[adapter_id][
                Common.METADATA
            ][Common.ADAPTER]
            return adapter_class
        else:
            raise RuntimeError(f"Couldn't obtain adapter for {adapter_id}")

    def get_adapter_by_id(
        self, adapter_id: str, *args: Any, **kwargs: Any
    ) -> Adapter:
        """Instantiates and returns a adapter.

        Args:
            adapter_id (str): Identifies adapter to create

        Raises:
            RuntimeError: If the ID is invalid/adapter is missing

        Returns:
            Adapter: Concrete impl of the `Adapter` base
        """
        adapter_class: Adapter = self.get_adapter_class_by_adapter_id(
            adapter_id
        )
        return adapter_class(*args, **kwargs)

    def get_adapters_list(self) -> list[dict[str, Any]]:
        adapters = []
        for adapter_id, adapter_registry_metadata in self._adapters.items():
            m: Adapter = adapter_registry_metadata[Common.METADATA][
                Common.ADAPTER
            ]
            _id = m.get_id()
            name = m.get_name()
            adapter_type = m.get_adapter_type().name
            json_schema = m.get_json_schema()
            desc = m.get_description()
            icon = m.get_icon()
            adapters.append(
                {
                    "id": _id,
                    "name": name,
                    "class_name": m.__name__,
                    "description": desc,
                    "icon": icon,
                    "adapter_type": adapter_type,
                    "json_schema": json_schema,
                }
            )
        return adapters


================================================
File: src/unstract/sdk/adapters/base.py
================================================
import logging
from abc import ABC, abstractmethod

from unstract.sdk.adapters.enums import AdapterTypes

logger = logging.getLogger(__name__)


class Adapter(ABC):
    def __init__(self, name: str):
        self.name = name

    @staticmethod
    @abstractmethod
    def get_id() -> str:
        return ""

    @staticmethod
    @abstractmethod
    def get_name() -> str:
        return ""

    @staticmethod
    @abstractmethod
    def get_description() -> str:
        return ""

    @staticmethod
    @abstractmethod
    def get_icon() -> str:
        return ""

    @classmethod
    def get_json_schema(cls) -> str:
        schema_path = getattr(cls, 'SCHEMA_PATH', None)
        if schema_path is None:
            raise ValueError(f"SCHEMA_PATH not defined for {cls.__name__}")
        with open(schema_path) as f:
            return f.read()

    @staticmethod
    @abstractmethod
    def get_adapter_type() -> AdapterTypes:
        return ""

    @abstractmethod
    def test_connection(self) -> bool:
        """Override to test connection for a adapter.

        Returns:
            bool: Flag indicating if the credentials are valid or not
        """
        pass


================================================
File: src/unstract/sdk/adapters/constants.py
================================================
class Common:
    METADATA = "metadata"
    MODULE = "module"
    ADAPTER = "adapter"
    SRC_FOLDER = "src"
    ADAPTER_METADATA = "adapter_metadata"
    ICON = "icon"
    ADAPTER_ID = "adapter_id"
    ADAPTER_TYPE = "adapter_type"
    DEFAULT_ERR_MESSAGE = "Something went wrong"


================================================
File: src/unstract/sdk/adapters/enums.py
================================================
from enum import Enum


class AdapterTypes(Enum):
    UNKNOWN = "UNKNOWN"
    LLM = "LLM"
    EMBEDDING = "EMBEDDING"
    VECTOR_DB = "VECTOR_DB"
    OCR = "OCR"
    X2TEXT = "X2TEXT"


================================================
File: src/unstract/sdk/adapters/exceptions.py
================================================
from unstract.sdk.exceptions import SdkError


# TODO: Remove redundant AdapterError classes and use SdkError itself
class AdapterError(SdkError):
    pass


class LLMError(AdapterError):
    pass


class ExtractorError(AdapterError):
    pass


================================================
File: src/unstract/sdk/adapters/registry.py
================================================
from abc import ABC, abstractmethod
from typing import Any


class AdapterRegistry(ABC):
    def __init__(self, name: str):
        self.name = name

    @staticmethod
    @abstractmethod
    def register_adapters(adapters: dict[str, Any]) -> None:
        pass


================================================
File: src/unstract/sdk/adapters/utils.py
================================================
import logging
import warnings
from pathlib import Path

import filetype
import magic
from requests import Response
from requests.exceptions import RequestException

from unstract.sdk.adapters.constants import Common
from unstract.sdk.constants import MimeType
from unstract.sdk.file_storage import FileStorage, FileStorageProvider

logger = logging.getLogger(__name__)


class AdapterUtils:
    @staticmethod
    def get_msg_from_request_exc(
        err: RequestException,
        message_key: str,
        default_err: str = Common.DEFAULT_ERR_MESSAGE,
    ) -> str:
        """Gets the message from the RequestException.

        Args:
            err_response (Response): Error response from the exception
            message_key (str): Key from response containing error message

        Returns:
            str: Error message returned by the server
        """
        if not hasattr(err, "response"):
            return default_err

        err_response: Response = err.response  # type: ignore
        err_content_type = err_response.headers.get("Content-Type")

        if not err_content_type:
            logger.warning(
                f"Content-Type header not found in {err_response}, "
                f"returning {default_err}"
            )
            return default_err

        if err_content_type == MimeType.JSON:
            err_json = err_response.json()
            if message_key in err_json:
                return str(err_json[message_key])
            else:
                logger.warning(
                    f"Unable to parse error with key '{message_key}' for "
                    f"'{err_json}', returning '{default_err}' instead."
                )
        elif err_content_type == MimeType.TEXT:
            return err_response.text  # type: ignore
        else:
            logger.warning(
                f"Unhandled err_response type '{err_content_type}' "
                f"for {err_response}, returning {default_err}"
            )
        return default_err

    # TODO: get_file_mime_type() to be removed once migrated to FileStorage
    # FileStorage has mime_type() which could be used instead.
    @staticmethod
    def get_file_mime_type(
        input_file: Path,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> str:
        """Gets the file MIME type for an input file. Uses libmagic to perform
        the same.

        Args:
            input_file (Path): Path object of the input file

        Returns:
            str: MIME type of the file
        """
        # Adding the following DeprecationWarning manually as the package "deprecated"
        # does not support deprecation on static methods.
        warnings.warn(
            "`get_file_mime_type` is deprecated. "
            "Use `FileStorage mime_type()` instead.",
            DeprecationWarning,
        )
        sample_contents = fs.read(path=input_file, mode="rb", length=100)
        input_file_mime = magic.from_buffer(sample_contents, mime=True)
        return input_file_mime

    @staticmethod
    def guess_extention(
        input_file_path: str,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> str:
        """Returns the extention of the file passed.

        Args:
            input_file_path (str): String holding the path

        Returns:
            str: File extention
        """
        # Adding the following DeprecationWarning manually as the package "deprecated"
        # does not support deprecation on static methods.
        warnings.warn(
            "`guess_extention` is deprecated. "
            "Use `FileStorage guess_extension()` instead.",
            DeprecationWarning,
        )
        input_file_extention = ""
        sample_contents = fs.read(path=input_file_path, mode="rb", length=100)
        if sample_contents:
            file_type = filetype.guess(sample_contents)
            input_file_extention = file_type.EXTENSION
        return input_file_extention


================================================
File: src/unstract/sdk/adapters/embedding/__init__.py
================================================
from unstract.sdk.adapters import AdapterDict
from unstract.sdk.adapters.embedding.register import EmbeddingRegistry

adapters: AdapterDict = {}
EmbeddingRegistry.register_adapters(adapters)


================================================
File: src/unstract/sdk/adapters/embedding/embedding_adapter.py
================================================
from abc import ABC, abstractmethod
from typing import Any

from llama_index.core import MockEmbedding
from llama_index.core.embeddings import BaseEmbedding

from unstract.sdk.adapters.base import Adapter
from unstract.sdk.adapters.enums import AdapterTypes

from unstract.sdk.adapters.embedding.helper import EmbeddingHelper

class EmbeddingAdapter(Adapter, ABC):
    def __init__(self, name: str):
        super().__init__(name)
        self.name = name

    @staticmethod
    def get_id() -> str:
        return ""

    @staticmethod
    def get_name() -> str:
        return ""

    @staticmethod
    def get_description() -> str:
        return ""

    @staticmethod
    @abstractmethod
    def get_provider() -> str:
        pass

    @staticmethod
    def get_icon() -> str:
        return ""

    @staticmethod
    def get_adapter_type() -> AdapterTypes:
        return AdapterTypes.EMBEDDING

    def get_embedding_instance(self, embed_config: dict[str, Any]) -> BaseEmbedding:
        """Instantiate the llama index BaseEmbedding class.

        Returns:
            BaseEmbedding: llama index implementation of the Embedding
            Raises exceptions for any error
        """
        return MockEmbedding(embed_dim=1)
    
    def test_connection(self) -> bool:
        embedding = self.get_embedding_instance()
        test_result: bool = EmbeddingHelper.test_embedding_instance(embedding)
        return test_result

================================================
File: src/unstract/sdk/adapters/embedding/helper.py
================================================
import logging
from typing import Any, Optional

from llama_index.core.embeddings import BaseEmbedding

from unstract.sdk.adapters.exceptions import AdapterError

logger = logging.getLogger(__name__)


class EmbeddingConstants:
    DEFAULT_EMBED_BATCH_SIZE = 10
    EMBED_BATCH_SIZE = "embed_batch_size"


class EmbeddingHelper:
    @staticmethod
    def get_embedding_batch_size(config: dict[str, Any]) -> int:
        if config.get(EmbeddingConstants.EMBED_BATCH_SIZE) is None:
            embedding_batch_size = EmbeddingConstants.DEFAULT_EMBED_BATCH_SIZE
        else:
            embedding_batch_size = int(
                config.get(
                    EmbeddingConstants.EMBED_BATCH_SIZE,
                    EmbeddingConstants.DEFAULT_EMBED_BATCH_SIZE,
                )
            )
        return embedding_batch_size

    @staticmethod
    def test_embedding_instance(embedding: Optional[BaseEmbedding]) -> bool:
        try:
            if embedding is None:
                return False
            response = embedding._get_text_embedding("This is a test")
            if len(response) != 0:
                return True
            else:
                return False
        except Exception as e:
            logger.error(f"Error occured while testing adapter {e}")
            raise AdapterError(str(e))


================================================
File: src/unstract/sdk/adapters/embedding/register.py
================================================
import logging
import os
from importlib import import_module
from typing import Any

from unstract.sdk.adapters.constants import Common
from unstract.sdk.adapters.embedding.embedding_adapter import EmbeddingAdapter
from unstract.sdk.adapters.registry import AdapterRegistry

logger = logging.getLogger(__name__)


class EmbeddingRegistry(AdapterRegistry):
    @staticmethod
    def register_adapters(adapters: dict[str, Any]) -> None:
        current_directory = os.path.dirname(os.path.abspath(__file__))
        package = "unstract.sdk.adapters.embedding"

        for adapter in os.listdir(current_directory):
            adapter_path = os.path.join(current_directory, adapter, Common.SRC_FOLDER)
            # Check if the item is a directory and not
            # a special directory like __pycache__
            if os.path.isdir(adapter_path) and not adapter.startswith("__"):
                EmbeddingRegistry._build_adapter_list(adapter, package, adapters)
        if len(adapters) == 0:
            logger.warning("No embedding adapter found.")

    @staticmethod
    def _build_adapter_list(
        adapter: str, package: str, adapters: dict[str, Any]
    ) -> None:
        try:
            full_module_path = f"{package}.{adapter}.{Common.SRC_FOLDER}"
            module = import_module(full_module_path)
            metadata = getattr(module, Common.METADATA, {})
            if metadata.get("is_active", False):
                adapter_class: EmbeddingAdapter = metadata[Common.ADAPTER]
                adapter_id = adapter_class.get_id()
                if not adapter_id or (adapter_id in adapters):
                    logger.warning(f"Duplicate Id : {adapter_id}")
                else:
                    adapters[adapter_id] = {
                        Common.MODULE: module,
                        Common.METADATA: metadata,
                    }
        except ModuleNotFoundError as exception:
            logger.warning(f"Unable to import embedding adapters : {exception}")


================================================
File: src/unstract/sdk/adapters/embedding/azure_open_ai/README.md
================================================
# Unstract Azure OpenAI Embedding Adapter

This package consists of the functionalities required to adapt with Azure OpenAI Embedding 
Version supported


================================================
File: src/unstract/sdk/adapters/embedding/azure_open_ai/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-azure-open-ai-embedding"
version = "0.0.1"
description = "Azure OpenAI Embedding"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/embedding/azure_open_ai/src/__init__.py
================================================
from .azure_open_ai import AzureOpenAI

metadata = {
    "name": AzureOpenAI.__name__,
    "version": "1.0.0",
    "adapter": AzureOpenAI,
    "description": "AzureOpenAI embedding adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/embedding/azure_open_ai/src/azure_open_ai.py
================================================
import os
from typing import Any

import httpx
from llama_index.core.embeddings import BaseEmbedding
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding

from unstract.sdk.adapters.embedding.embedding_adapter import EmbeddingAdapter
from unstract.sdk.adapters.embedding.helper import EmbeddingHelper
from unstract.sdk.adapters.exceptions import AdapterError


class Constants:
    ADAPTER_NAME = "adapter_name"
    MODEL = "model"
    API_KEY = "api_key"
    API_VERSION = "api_version"
    AZURE_ENDPOINT = "azure_endpoint"
    DEPLOYMENT_NAME = "deployment_name"
    API_TYPE = "azure"
    TIMEOUT = "timeout"
    DEFAULT_TIMEOUT = 240


class AzureOpenAI(EmbeddingAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("AzureOpenAIEmbedding")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "azureopenai|9770f3f6-f8ba-4fa0-bb3a-bef48a00e66f"

    @staticmethod
    def get_name() -> str:
        return "AzureOpenAIEmbedding"

    @staticmethod
    def get_description() -> str:
        return "AzureOpenAI Embedding"

    @staticmethod
    def get_provider() -> str:
        return "azure"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/AzureopenAI.png"

    def get_embedding_instance(self) -> BaseEmbedding:
        try:
            embedding_batch_size = EmbeddingHelper.get_embedding_batch_size(
                config=self.config
            )
            timeout = int(self.config.get(Constants.TIMEOUT, Constants.DEFAULT_TIMEOUT))
            httpx_timeout = httpx.Timeout(timeout, connect=60.0)
            httpx_client = httpx.Client(timeout=httpx_timeout)
            embedding: BaseEmbedding = AzureOpenAIEmbedding(
                model=str(self.config.get(Constants.MODEL)),
                deployment_name=str(self.config.get(Constants.DEPLOYMENT_NAME)),
                api_key=str(self.config.get(Constants.API_KEY)),
                api_version=str(self.config.get(Constants.API_VERSION)),
                azure_endpoint=str(self.config.get(Constants.AZURE_ENDPOINT)),
                embed_batch_size=embedding_batch_size,
                api_type=Constants.API_TYPE,
                timeout=timeout,
                http_client=httpx_client,
            )
            return embedding
        except Exception as e:
            raise AdapterError(str(e))

     


================================================
File: src/unstract/sdk/adapters/embedding/azure_open_ai/src/static/json_schema.json
================================================
{
  "title": "Azure OpenAI Embedding",
  "type": "object",
  "required": [
    "adapter_name",
    "model",
    "deployment_name",
    "api_key",
    "azure_endpoint",
    "api_version"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: azure-emb-1"
    },
    "model": {
      "type": "string",
      "title": "Model",
      "default": "",
      "description": "Provide the name of the model you defined in Azure console. Example text-embedding-ada-002"
    },
    "deployment_name": {
      "type": "string",
      "title": "Deployment Name",
      "default": "",
      "description": "Provide the name of the deployment you defined in Azure console"
    },
    "api_key": {
      "type": "string",
      "title": "API Key",
      "format": "password",
      "description": "Provide the API key"
    },
    "api_version": {
      "type": "string",
      "title": "API Version",
      "default": "2023-05-15",
      "description": "Provide the API version. Refer to the documentation provided by Azure. Example: 2023-05-15"
    },
    "azure_endpoint": {
      "type": "string",
      "title": "Azure Endpoint",
      "default": "",
      "format": "uri",
      "description": "Provide the Azure endpoint. Example: https://<your-deployment>.openai.azure.com/"
    },
    "embed_batch_size": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Embedding Batch Size",
      "default": 5
    },
    "timeout": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Timeout",
      "default": 240,
      "description": "Timeout for each request in seconds"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/embedding/bedrock/README.md
================================================
# Unstract Bedrock Embeddings

================================================
File: src/unstract/sdk/adapters/embedding/bedrock/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-bedrock-embedding"
version = "0.0.1"
description = "Bedrock Embedding"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/embedding/bedrock/src/__init__.py
================================================
from .bedrock import Bedrock

metadata = {
    "name": Bedrock.__name__,
    "version": "1.0.0",
    "adapter": Bedrock,
    "description": "Bedrock embedding adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/embedding/bedrock/src/bedrock.py
================================================
import os
from typing import Any

from llama_index.core.embeddings import BaseEmbedding
from llama_index.embeddings.bedrock import BedrockEmbedding

from unstract.sdk.adapters.embedding.embedding_adapter import EmbeddingAdapter
from unstract.sdk.adapters.embedding.helper import EmbeddingHelper
from unstract.sdk.adapters.exceptions import AdapterError

class Constants:
    MODEL = "model"
    TIMEOUT = "timeout"
    MAX_RETRIES = "max_retries"
    SECRET_ACCESS_KEY = "aws_secret_access_key"
    ACCESS_KEY_ID = "aws_access_key_id"
    REGION_NAME = "region_name"
    DEFAULT_TIMEOUT = 240
    DEFAULT_MAX_RETRIES = 3

class Bedrock(EmbeddingAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("Bedrock")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "bedrock|88199741-8d7e-4e8c-9d92-d76b0dc20c91"

    @staticmethod
    def get_name() -> str:
        return "Bedrock"

    @staticmethod
    def get_description() -> str:
        return "Bedrock Embedding"

    @staticmethod
    def get_provider() -> str:
        return "bedrock"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/Bedrock.png"

    def get_embedding_instance(self) -> BaseEmbedding:
        try:
            embedding_batch_size = EmbeddingHelper.get_embedding_batch_size(
                config=self.config
            )
            embedding: BaseEmbedding = BedrockEmbedding(
                model_name=self.config.get(Constants.MODEL),
                aws_access_key_id=self.config.get(Constants.ACCESS_KEY_ID),
                aws_secret_access_key=self.config.get(Constants.SECRET_ACCESS_KEY),
                region_name=self.config.get(Constants.REGION_NAME),
                timeout=float(
                    self.config.get(Constants.TIMEOUT, Constants.DEFAULT_TIMEOUT)
                ),
                max_retries=int(
                    self.config.get(Constants.MAX_RETRIES, Constants.DEFAULT_MAX_RETRIES)
                ),
                embed_batch_size=embedding_batch_size,
            )
            return embedding
        except Exception as e:
            raise AdapterError(str(e))
        
     

================================================
File: src/unstract/sdk/adapters/embedding/bedrock/src/static/json_schema.json
================================================
{
    "title": "Bedrock Embeddings",
    "type": "object",
    "required": [
      "aws_secret_access_key",
      "region_name",
      "aws_access_key_id",
      "model",
      "adapter_name"
    ],
    "properties": {
      "adapter_name": {
        "type": "string",
        "title": "Name",
        "default": "",
        "description": "Provide a unique name for this adapter instance. Example: Bedrock-Embedding-1"
      },
      "model": {
        "type": "string",
        "title": "Model",
        "default": "amazon.titan-embed-text-v2:0",
        "description": "Model name. Refer to [Bedrock's documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html) for the list of available models."
      },
      "aws_access_key_id": {
        "type": "string",
        "title": "AWS Access Key ID",
        "description": "Provide your AWS Access Key ID",
        "format": "password"
      },
      "aws_secret_access_key": {
        "type": "string",
        "title": "AWS Secret Access Key",
        "description": "Provide your AWS Secret Access Key",
        "format": "password"
      },
      "region_name": {
        "type": "string",
        "title": "AWS Region name",
        "description": "Provide the AWS Region name where the service is running. Eg. us-east-1"
      },
      "embed_batch_size": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Embedding Batch Size",
      "default": 10
      },
      "max_retries": {
        "type": "number",
        "minimum": 0,
        "multipleOf": 1,
        "title": "Max Retries",
        "default": 5,
        "description": "Maximum number of retries to attempt when a request fails."
      },
      "timeout": {
        "type": "number",
        "minimum": 0,
        "multipleOf": 1,
        "title": "Timeout",
        "default": 900,
        "description": "Timeout in seconds"
      }
    }
  }
  

================================================
File: src/unstract/sdk/adapters/embedding/hugging_face/README.md
================================================
# Unstract Hugging Face Embedding Adapter


================================================
File: src/unstract/sdk/adapters/embedding/hugging_face/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-hugging-face-embedding"
version = "0.0.1"
description = "HuggingFace Embedding"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/embedding/hugging_face/src/__init__.py
================================================
from .hugging_face import HuggingFace

metadata = {
    "name": HuggingFace.__name__,
    "version": "1.0.0",
    "adapter": HuggingFace,
    "description": "HuggingFace embedding adapter",
    "is_active": False,
}


================================================
File: src/unstract/sdk/adapters/embedding/hugging_face/src/hugging_face.py
================================================
import os
from typing import Any, Optional

from llama_index.core.embeddings import BaseEmbedding
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

from unstract.sdk.adapters.embedding.embedding_adapter import EmbeddingAdapter
from unstract.sdk.adapters.embedding.helper import EmbeddingHelper
from unstract.sdk.adapters.exceptions import AdapterError


class Constants:
    ADAPTER_NAME = "adapter_name"
    MODEL = "model_name"
    TOKENIZER_NAME = "tokenizer_name"
    MAX_LENGTH = "max_length"
    NORMALIZE = "normalize"


class HuggingFace(EmbeddingAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("HuggingFace")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "huggingface|90ec9ec2-1768-4d69-8fb1-c88b95de5e5a"

    @staticmethod
    def get_name() -> str:
        return "HuggingFace"

    @staticmethod
    def get_description() -> str:
        return "HuggingFace Embedding"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/huggingface.png"

    def get_embedding_instance(self) -> BaseEmbedding:
        try:
            embedding_batch_size = EmbeddingHelper.get_embedding_batch_size(
                config=self.config
            )
            max_length: Optional[int] = (
                int(self.config.get(Constants.MAX_LENGTH, 0))
                if self.config.get(Constants.MAX_LENGTH)
                else None
            )
            embedding: BaseEmbedding = HuggingFaceEmbedding(
                model_name=str(self.config.get(Constants.MODEL)),
                tokenizer_name=str(self.config.get(Constants.TOKENIZER_NAME)),
                normalize=bool(self.config.get(Constants.NORMALIZE)),
                embed_batch_size=embedding_batch_size,
                max_length=max_length,
            )

            return embedding
        except Exception as e:
            raise AdapterError(str(e))

     


================================================
File: src/unstract/sdk/adapters/embedding/hugging_face/src/static/json_schema.json
================================================
{
  "title": "Hugging Face Embedding",
  "type": "object",
  "required": [
    "adapter_name",
    "model_name"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: hf-emb-1"
    },
    "model_name": {
      "type": "string",
      "title": "Model Name",
      "default": "BAAI/bge-small-en-v1.5",
      "description": "Provide the name of the model to use. Example: BAAI/bge-small-en-v1.5"
    },
    "tokenizer_name": {
      "type": "string",
      "title": "Tokenizer Name",
      "default": ""
    },
    "max_length": {
      "type": "integer",
      "title": "Max Length"
    },
    "normalize": {
      "type": "boolean",
      "title": "Normalize",
      "default": true
    },
    "embed_batch_size": {
      "type": "integer",
      "title": "Embed Batch Size"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/embedding/no_op/README.md
================================================
# Unstract NoOp Embedding adapter for load testing

An embedding adapter that does not perform any operation. Waits for the configured time before returning a response. This can be useful to perform tests on the system


================================================
File: src/unstract/sdk/adapters/embedding/no_op/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-no-op-embedding"
version = "0.0.1"
description = "NoOp Embedding"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/embedding/no_op/src/__init__.py
================================================
from .no_op_embedding import NoOpEmbedding

metadata = {
    "name": NoOpEmbedding.__name__,
    "version": "1.0.0",
    "adapter": NoOpEmbedding,
    "description": "NoOp embedding adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/embedding/no_op/src/no_op_custom_embedding.py
================================================
from typing import Any

from llama_index.core import MockEmbedding


class NoOpCustomEmbedding(MockEmbedding):

    embed_dim: int

    def __init__(self, embed_dim: int, wait_time: float, **kwargs: Any) -> None:
        """Init params."""
        super().__init__(embed_dim=embed_dim, **kwargs, wait_time=wait_time)


================================================
File: src/unstract/sdk/adapters/embedding/no_op/src/no_op_embedding.py
================================================
import os
import time
from typing import Any

from llama_index.core.embeddings import BaseEmbedding

from unstract.sdk.adapters.embedding.embedding_adapter import EmbeddingAdapter
from unstract.sdk.adapters.embedding.no_op.src.no_op_custom_embedding import (
    NoOpCustomEmbedding,
)


class NoOpEmbedding(EmbeddingAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("NoOpCustomEmbedding")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "noOpEmbedding|ff223003-fee8-4079-b288-e86215e6b39a"

    @staticmethod
    def get_name() -> str:
        return "No Op Embedding"

    @staticmethod
    def get_description() -> str:
        return "No Op Embedding"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/noOpEmbedding.png"

    @staticmethod
    def get_provider() -> str:
        return "NoOp"

    def get_embedding_instance(self) -> BaseEmbedding:
        embedding: BaseEmbedding = NoOpCustomEmbedding(
            embed_dim=1, wait_time=self.config.get("wait_time")
        )
        return embedding

    def test_connection(self) -> bool:
        time.sleep(self.config.get("wait_time"))
        return True


================================================
File: src/unstract/sdk/adapters/embedding/no_op/src/static/json_schema.json
================================================
{
  "title": "No Op Embedding",
  "type": "object",
  "required": [
    "adapter_name",
    "wait_time"
  ],
  "description": "No Op Embedding does not perform any operation, its used to test the performance of the system in the absence of 3rd party induced latencies",
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "no-op-embedding",
      "description": "Provide a unique name for this No Op adapter instance. Example: no-op-instance-1"
    },
    "wait_time": {
      "type": "number",
      "title": "Wait time (in seconds)",
      "default": "0",
      "description": "Provide the time to wait (in seconds) before returning the response"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/embedding/ollama/README.md
================================================
# Unstract Ollama Embedding Adapter

For official documentation on Ollama embeddings, please refer to https://ollama.com/blog/embedding-models

To use the Ollama embedding adapter in the local setup, the preferred model needs to be run locally. 

Ollama can also be run using the docker setup. Please refer to https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image


================================================
File: src/unstract/sdk/adapters/embedding/ollama/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-ollama-embedding"
version = "0.0.1"
description = "Ollama Embedding"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/embedding/ollama/src/__init__.py
================================================
from .ollama import Ollama

metadata = {
    "name": Ollama.__name__,
    "version": "1.0.0",
    "adapter": Ollama,
    "description": "Ollama embedding adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/embedding/ollama/src/ollama.py
================================================
import os
from typing import Any

from llama_index.core.embeddings import BaseEmbedding
from llama_index.embeddings.ollama import OllamaEmbedding

from unstract.sdk.adapters.embedding.embedding_adapter import EmbeddingAdapter
from unstract.sdk.adapters.embedding.helper import EmbeddingHelper
from unstract.sdk.adapters.exceptions import AdapterError


class Constants:
    MODEL = "model_name"
    ADAPTER_NAME = "adapter_name"
    BASE_URL = "base_url"


class Ollama(EmbeddingAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("Ollama")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "ollama|d58d7080-55a9-4542-becd-8433528e127b"

    @staticmethod
    def get_name() -> str:
        return "Ollama"

    @staticmethod
    def get_description() -> str:
        return "Ollama Embedding"

    @staticmethod
    def get_provider() -> str:
        return "ollama"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/ollama.png"

    def get_embedding_instance(self) -> BaseEmbedding:
        try:
            embedding_batch_size = EmbeddingHelper.get_embedding_batch_size(
                config=self.config
            )
            embedding: BaseEmbedding = OllamaEmbedding(
                model_name=str(self.config.get(Constants.MODEL)),
                base_url=str(self.config.get(Constants.BASE_URL)),
                embed_batch_size=embedding_batch_size,
            )
            return embedding
        except Exception as e:
            raise AdapterError(str(e))

     


================================================
File: src/unstract/sdk/adapters/embedding/ollama/src/static/json_schema.json
================================================
{
  "title": "Ollama Embedding",
  "type": "object",
  "required": [
    "adapter_name",
    "base_url",
    "model_name"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: ollama-emb-1"
    },
    "model_name": {
      "type": "string",
      "title": "Model Name",
      "default": "mxbai-embed-large",
      "description": "Provide the name of the model to use for embedding. Example: mxbai-embed-large"
    },
    "base_url": {
      "type": "string",
      "title": "Base URL",
      "default": "",
      "description": "Provide the base URL where Ollama server is running. Example: `http://docker.host.internal:11434` or `http://localhost:11434`"
    },
    "embed_batch_size": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Embed Batch Size",
      "default": 10
    }
  }
}


================================================
File: src/unstract/sdk/adapters/embedding/open_ai/README.md
================================================
# Unstract OpenAI Embeddings


================================================
File: src/unstract/sdk/adapters/embedding/open_ai/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-open-ai-embedding"
version = "0.0.1"
description = "OpenAI Embedding"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/embedding/open_ai/src/__init__.py
================================================
from .open_ai import OpenAI

metadata = {
    "name": OpenAI.__name__,
    "version": "1.0.0",
    "adapter": OpenAI,
    "description": "OpenAI embedding adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/embedding/open_ai/src/open_ai.py
================================================
import os
from typing import Any

import httpx
from llama_index.core.embeddings import BaseEmbedding
from llama_index.embeddings.openai import OpenAIEmbedding

from unstract.sdk.adapters.embedding.embedding_adapter import EmbeddingAdapter
from unstract.sdk.adapters.embedding.helper import EmbeddingHelper
from unstract.sdk.adapters.exceptions import AdapterError


class Constants:
    API_KEY = "api_key"
    MODEL = "model"
    API_BASE_VALUE = "https://api.openai.com/v1/"
    API_BASE_KEY = "api_base"
    ADAPTER_NAME = "adapter_name"
    API_TYPE = "openai"
    TIMEOUT = "timeout"
    DEFAULT_TIMEOUT = 240
    DEFAULT_MODEL = "text-embedding-ada-002"


class OpenAI(EmbeddingAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("OpenAI")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "openai|717a0b0e-3bbc-41dc-9f0c-5689437a1151"

    @staticmethod
    def get_name() -> str:
        return "OpenAI"

    @staticmethod
    def get_description() -> str:
        return "OpenAI LLM"

    @staticmethod
    def get_provider() -> str:
        return "openai"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/OpenAI.png"

    def get_embedding_instance(self) -> BaseEmbedding:
        try:
            timeout = int(self.config.get(Constants.TIMEOUT, Constants.DEFAULT_TIMEOUT))
            httpx_timeout = httpx.Timeout(10.0, connect=60.0)
            httpx_client = httpx.Client(timeout=httpx_timeout)
            embedding: BaseEmbedding = OpenAIEmbedding(
                api_key=str(self.config.get(Constants.API_KEY)),
                api_base=str(
                    self.config.get(Constants.API_BASE_KEY, Constants.API_BASE_VALUE)
                ),
                model=str(self.config.get(Constants.MODEL, Constants.DEFAULT_MODEL)),
                api_type=Constants.API_TYPE,
                timeout=timeout,
                http_client=httpx_client,
            )
            return embedding
        except Exception as e:
            raise AdapterError(str(e))

     


================================================
File: src/unstract/sdk/adapters/embedding/open_ai/src/static/json_schema.json
================================================
{
  "title": "OpenAI Embedding",
  "type": "object",
  "required": [
    "adapter_name",
    "api_key"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: openai-emb-1"
    },
    "model": {
      "type": "string",
      "title": "Model",
      "default": "text-embedding-ada-002",
      "description": "Provide the name of the model."
    },
    "api_key": {
      "type": "string",
      "title": "API Key",
      "default": "",
      "format": "password"
    },
    "api_base": {
      "type": "string",
      "title": "API Base",
      "format": "uri",
      "default": "https://api.openai.com/v1/"
    },
    "embed_batch_size": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Embed Batch Size",
      "default": 10
    },
    "timeout": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Timeout",
      "default": 240,
      "description": "Timeout in seconds"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/embedding/palm/README.md
================================================
# Unstract Pa LM Embedding Adapter


================================================
File: src/unstract/sdk/adapters/embedding/palm/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-palm-embedding"
version = "0.0.1"
description = "PaLM Embedding"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/embedding/palm/src/__init__.py
================================================
from .palm import PaLM

metadata = {
    "name": PaLM.__name__,
    "version": "1.0.0",
    "adapter": PaLM,
    "description": "PaLM embedding adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/embedding/palm/src/palm.py
================================================
import os
from typing import Any

from llama_index.core.embeddings import BaseEmbedding
from llama_index.embeddings.google import GooglePaLMEmbedding

from unstract.sdk.adapters.embedding.embedding_adapter import EmbeddingAdapter
from unstract.sdk.adapters.embedding.helper import EmbeddingHelper
from unstract.sdk.adapters.exceptions import AdapterError


class Constants:
    MODEL = "model_name"
    API_KEY = "api_key"
    ADAPTER_NAME = "adapter_name"


class PaLM(EmbeddingAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("Palm")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "palm|a3fc9fda-f02f-405f-bb26-8bd2ace4317e"

    @staticmethod
    def get_name() -> str:
        return "Palm"

    @staticmethod
    def get_description() -> str:
        return "PaLM Embedding"

    @staticmethod
    def get_provider() -> str:
        return "palm"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/PaLM.png"

    def get_embedding_instance(self) -> BaseEmbedding:
        try:
            embedding_batch_size = EmbeddingHelper.get_embedding_batch_size(
                config=self.config
            )
            embedding: BaseEmbedding = GooglePaLMEmbedding(
                model_name=str(self.config.get(Constants.MODEL)),
                api_key=str(self.config.get(Constants.API_KEY)),
                embed_batch_size=embedding_batch_size,
            )
            return embedding
        except Exception as e:
            raise AdapterError(str(e))

     


================================================
File: src/unstract/sdk/adapters/embedding/palm/src/static/json_schema.json
================================================
{
  "title": "Palm Embedding",
  "type": "object",
  "required": [
    "adapter_name",
    "model_name",
    "api_key"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: palm-emb-1"
    },
    "model_name": {
      "type": "string",
      "title": "Model Name",
      "default": "models/embedding-gecko-001",
      "description": "Provide the name of the model to use for embedding. Example: `models/embedding-gecko-001`"
    },
    "api_key": {
      "type": "string",
      "title": "API Key",
      "format": "password"
    },
    "embed_batch_size": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Embed Batch Size",
      "default": 10
    }
  }
}


================================================
File: src/unstract/sdk/adapters/embedding/qdrant_fast_embed/README.md
================================================
# Unstract Qdrant FastEmbed Embedding Adapter


================================================
File: src/unstract/sdk/adapters/embedding/qdrant_fast_embed/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-qdrant-fastembed-embedding"
version = "0.0.1"
description = "Qdrant FastEmbed Embedding"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/embedding/qdrant_fast_embed/src/__init__.py
================================================
from .qdrant_fast_embed import QdrantFastEmbedM

metadata = {
    "name": QdrantFastEmbedM.__name__,
    "version": "1.0.0",
    "adapter": QdrantFastEmbedM,
    "description": "QdrantFastEmbed embedding adapter",
    "is_active": False,
}


================================================
File: src/unstract/sdk/adapters/embedding/qdrant_fast_embed/src/qdrant_fast_embed.py
================================================
import os
from typing import Any

from llama_index.core.embeddings import BaseEmbedding
from llama_index.embeddings.fastembed import FastEmbedEmbedding

from unstract.sdk.adapters.embedding.embedding_adapter import EmbeddingAdapter
from unstract.sdk.adapters.embedding.helper import EmbeddingHelper
from unstract.sdk.adapters.exceptions import AdapterError


class Constants:
    MODEL = "model_name"
    ADAPTER_NAME = "adapter_name"


class QdrantFastEmbedM(EmbeddingAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("QdrantFastEmbedM")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "qdrantfastembed|31e83eee-a416-4c07-9c9c-02392d5bcf7f"

    @staticmethod
    def get_name() -> str:
        return "QdrantFastEmbedM"

    @staticmethod
    def get_description() -> str:
        return "QdrantFastEmbedM LLM"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/qdrant.png"

    def get_embedding_instance(self) -> BaseEmbedding:
        try:
            embedding: BaseEmbedding = FastEmbedEmbedding(
                model_name=str(self.config.get(Constants.MODEL))
            )
            return embedding
        except Exception as e:
            raise AdapterError(str(e))

     


================================================
File: src/unstract/sdk/adapters/embedding/qdrant_fast_embed/src/static/json_schema.json
================================================
{
  "title": "Qdrant FastEmbed Embedding",
  "type": "object",
  "required": [
    "adapter_name",
    "model_name"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: hf-fe-emb-1"
    },
    "model_name": {
      "type": "string",
      "title": "Model",
      "default": "BAAI/bge-small-en-v1.5",
      "description": "The name of the model to use. Example: `BAAI/bge-small-en-v1.5`"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/embedding/vertex_ai/README.md
================================================
# Unstract Vertex AI Embeddings

================================================
File: src/unstract/sdk/adapters/embedding/vertex_ai/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-vertex-embedding"
version = "0.0.1"
description = "Vertex Embedding"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/embedding/vertex_ai/src/__init__.py
================================================
from .vertex_ai import VertexAIEmbedding

metadata = {
    "name": VertexAIEmbedding.__name__,
    "version": "1.0.0",
    "adapter": VertexAIEmbedding,
    "description": "VertexAI embedding adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/embedding/vertex_ai/src/vertex_ai.py
================================================
import json
import logging
import os
from typing import Any, Optional

from google.auth.transport import requests as google_requests
from google.oauth2.service_account import Credentials

from llama_index.core.embeddings import BaseEmbedding
from llama_index.embeddings.vertex import VertexTextEmbedding

from unstract.sdk.adapters.embedding.embedding_adapter import EmbeddingAdapter
from unstract.sdk.adapters.embedding.helper import EmbeddingHelper
from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.exceptions import EmbeddingError

class Constants:
    MODEL = "model"
    PROJECT = "project"
    JSON_CREDENTIALS = "json_credentials"
    EMBED_MODE = "embed_mode"

class VertexAIEmbedding(EmbeddingAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("Bedrock")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "vertexai|457a256b-e74f-4251-98a0-8864aafb42a5"

    @staticmethod
    def get_name() -> str:
        return "VertextAI"

    @staticmethod
    def get_description() -> str:
        return "VertexAI Embedding"

    @staticmethod
    def get_provider() -> str:
        return "vertexai"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/VertexAI.png"
    
    def get_embedding_instance(self) -> BaseEmbedding:
        try:
            embedding_batch_size = EmbeddingHelper.get_embedding_batch_size(
                config=self.config
            )
            input_credentials = self.config.get(Constants.JSON_CREDENTIALS, "{}")
            json_credentials = json.loads(input_credentials)

            credentials = Credentials.from_service_account_info(
                info=json_credentials,
                scopes=["https://www.googleapis.com/auth/cloud-platform"],
            )  # type: ignore
            credentials.refresh(google_requests.Request())

            embedding: BaseEmbedding = VertexTextEmbedding(
                model_name=self.config.get(Constants.MODEL),
                project=self.config.get(Constants.PROJECT),
                credentials=credentials,
                embed_mode=self.config.get(Constants.EMBED_MODE),
                embed_batch_size=embedding_batch_size,
            )
            return embedding
        except json.JSONDecodeError:
                raise EmbeddingError(
                    "Credentials is not a valid service account JSON, "
                    "please provide a valid JSON."
                )
        except Exception as e:
            raise AdapterError(str(e))
        
     

================================================
File: src/unstract/sdk/adapters/embedding/vertex_ai/src/static/json_schema.json
================================================
{
    "title": "Vertex AI Embedding",
    "type": "object",
    "required": [
      "adapter_name",
      "project",
      "json_credentials",
      "model"
    ],
    "properties": {
      "adapter_name": {
        "type": "string",
        "title": "Name",
        "default": "",
        "description": "Provide a unique name for this adapter instance. Example: vertexai-group-1"
      },
      "json_credentials": {
        "type": "string",
        "title": "Credentials",
        "default": "",
        "description": "Refer [GCloud docs](https://developers.google.com/workspace/guides/create-credentials#create_credentials_for_a_service_account) on adding keys for service account."
      },
      "model": {
        "type": "string",
        "title": "Model",
        "default": "",
        "description": "Provide the name of the model you defined for Vertex AI."
      },
      "project": {
        "type": "string",
        "title": "Project",
        "default": "",
        "description": "Provide the name of the deployment or project you defined for Vertex AI"
      },
      "embed_batch_size": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Embedding Batch Size",
      "default": 10
      },
      "embed_mode": {
        "type": "string",
        "title": "Embed Mode",
        "description": "Embedding mode for Vertex AI's embedding model. Refer to [Vertext AI's documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/task-types) for more details.",
        "enum": [
            "default",
            "classification",
            "clustering",
            "similarity",
            "retrieval"
          ],
          "default": "default"
      }
    }
  }
  

================================================
File: src/unstract/sdk/adapters/llm/__init__.py
================================================
from unstract.sdk.adapters import AdapterDict
from unstract.sdk.adapters.llm.register import LLMRegistry

adapters: AdapterDict = {}
LLMRegistry.register_adapters(adapters)


================================================
File: src/unstract/sdk/adapters/llm/constants.py
================================================
class LLMKeys:
    DEFAULT_TIMEOUT = 900
    DEFAULT_MAX_RETRIES = 3


================================================
File: src/unstract/sdk/adapters/llm/exceptions.py
================================================
from anthropic import APIError as AnthropicAPIError
from google.api_core.exceptions import GoogleAPICallError
from mistralai.models import SDKError as MistralError
from openai import APIError as OpenAIAPIError
from vertexai.generative_models import ResponseValidationError

from unstract.sdk.adapters.exceptions import LLMError
from unstract.sdk.adapters.llm.anthropic.src import AnthropicLLM
from unstract.sdk.adapters.llm.llm_adapter import LLMAdapter
from unstract.sdk.adapters.llm.mistral.src import MistralLLM
from unstract.sdk.adapters.llm.open_ai.src import OpenAILLM
from unstract.sdk.adapters.llm.palm.src import PaLMLLM
from unstract.sdk.adapters.llm.vertex_ai.src import VertexAILLM


def parse_llm_err(e: Exception, llm_adapter: LLMAdapter) -> LLMError:
    """Parses the exception from LLM provider.

    Helps parse the LLM error and wraps it with our
    custom exception object to contain a user friendly message.

    Args:
        e (Exception): Error from LLM provider

    Returns:
        LLMError: Unstract's LLMError object
    """
    # Avoid wrapping LLMError objects again
    if isinstance(e, LLMError):
        return e

    if isinstance(e, ResponseValidationError):
        err = VertexAILLM.parse_llm_err(e)
    elif isinstance(e, OpenAIAPIError):
        err = OpenAILLM.parse_llm_err(e)
    elif isinstance(e, AnthropicAPIError):
        err = AnthropicLLM.parse_llm_err(e)
    elif isinstance(e, MistralError):
        err = MistralLLM.parse_llm_err(e)
    elif isinstance(e, GoogleAPICallError):
        err = PaLMLLM.parse_llm_err(e)
    else:
        err = LLMError(str(e), actual_err=e)

    msg = f"Error from LLM provider '{llm_adapter.get_name()}'."

    # Add a code block only for errors from clients
    if err.actual_err:
        msg += f"\n```\n{str(err)}\n```"
    else:
        msg += str(err)
    err.message = msg
    return err


================================================
File: src/unstract/sdk/adapters/llm/llm_adapter.py
================================================
import logging
import re
from abc import ABC, abstractmethod
from typing import Optional

from llama_index.core.llms import LLM, MockLLM
from llama_index.llms.openai.utils import O1_MODELS

from unstract.sdk.adapters.base import Adapter
from unstract.sdk.adapters.enums import AdapterTypes
from unstract.sdk.adapters.exceptions import LLMError

logger = logging.getLogger(__name__)


class LLMAdapter(Adapter, ABC):
    def __init__(self, name: str):
        super().__init__(name)
        self.name = name

    @staticmethod
    def get_id() -> str:
        return ""

    @staticmethod
    def get_name() -> str:
        return ""

    @staticmethod
    def get_description() -> str:
        return ""

    @staticmethod
    @abstractmethod
    def get_provider() -> str:
        pass

    @staticmethod
    def get_icon() -> str:
        return ""

    @staticmethod
    def get_adapter_type() -> AdapterTypes:
        return AdapterTypes.LLM

    @staticmethod
    def parse_llm_err(e: Exception) -> LLMError:
        """Parse the error from an LLM provider.

        Helps parse errors from a provider and wraps with custom exception.

        Args:
            e (Exception): Exception from LLM provider

        Returns:
            LLMError: Error to be sent to the user
        """
        return LLMError(str(e), actual_err=e)

    def get_llm_instance(self) -> LLM:
        """Instantiate the llama index LLM class.

        Returns:
            LLM: llama index implementation of the LLM
            Raises exceptions for any error
        """
        return MockLLM()

    @staticmethod
    def _test_llm_instance(llm: Optional[LLM]) -> bool:
        if llm is None:
            raise LLMError(
                message="Unable to connect to LLM, please recheck the configuration",
                status_code=400,
            )
        # Get completion kwargs based on model capabilities
        completion_kwargs = {}
        if hasattr(llm, 'model') and getattr(llm, 'model') not in O1_MODELS:
            completion_kwargs['temperature'] = 0.003
            
        response = llm.complete(
            "The capital of Tamilnadu is ",
            **completion_kwargs
        )
        response_lower_case: str = response.text.lower()
        find_match = re.search("chennai", response_lower_case)
        if find_match:
            return True
        else:
            msg = (
                "LLM based test failed. The credentials was valid however a sane "
                "response was not obtained from the LLM provider, please recheck "
                "the configuration."
            )
            raise LLMError(message=msg, status_code=400)

    def test_connection(self) -> bool:
        try:
            test_result: bool = self._test_llm_instance(llm=self.get_llm_instance())
        except Exception as e:
            # Avoids circular import errors
            from unstract.sdk.adapters.llm.exceptions import parse_llm_err

            raise parse_llm_err(e, llm_adapter=self) from e
        return test_result

    def get_context_window_size(self) -> int:
        """Get the context window size supported by the LLM.

        Note: None of the derived classes implement this method

        Returns:
            int: Context window size supported by the LLM
        """
        context_window_size: int = 0
        llm = self.get_llm_instance()
        if llm:
            context_window_size = llm.metadata.context_window
        return context_window_size


================================================
File: src/unstract/sdk/adapters/llm/register.py
================================================
import logging
import os
from importlib import import_module
from typing import Any

from unstract.sdk.adapters.constants import Common
from unstract.sdk.adapters.llm.llm_adapter import LLMAdapter
from unstract.sdk.adapters.registry import AdapterRegistry

logger = logging.getLogger(__name__)


class LLMRegistry(AdapterRegistry):
    @staticmethod
    def register_adapters(adapters: dict[str, Any]) -> None:
        current_directory = os.path.dirname(os.path.abspath(__file__))
        package = "unstract.sdk.adapters.llm"

        for adapter in os.listdir(current_directory):
            adapter_path = os.path.join(current_directory, adapter, Common.SRC_FOLDER)
            # Check if the item is a directory and not a
            # special directory like _pycache__
            if os.path.isdir(adapter_path) and not adapter.startswith("__"):
                LLMRegistry._build_adapter_list(adapter, package, adapters)
        if len(adapters) == 0:
            logger.warning("No llm adapter found.")

    @staticmethod
    def _build_adapter_list(
        adapter: str, package: str, adapters: dict[str, Any]
    ) -> None:
        try:
            full_module_path = f"{package}.{adapter}.{Common.SRC_FOLDER}"
            module = import_module(full_module_path)
            metadata = getattr(module, Common.METADATA, {})
            if metadata.get("is_active", False):
                adapter_class: LLMAdapter = metadata[Common.ADAPTER]
                adapter_id = adapter_class.get_id()
                if not adapter_id or (adapter_id in adapters):
                    logger.warning(f"Duplicate Id : {adapter_id}")
                else:
                    adapters[adapter_id] = {
                        Common.MODULE: module,
                        Common.METADATA: metadata,
                    }
        except ModuleNotFoundError as exception:
            logger.warning(f"Unable to import llm adapters : {exception}")


================================================
File: src/unstract/sdk/adapters/llm/anthropic/README.md
================================================
# Unstract Anthropic LLM Adapter


================================================
File: src/unstract/sdk/adapters/llm/anthropic/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-anthropic-llm"
version = "0.0.1"
description = "Anthropic LLM"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/llm/anthropic/src/__init__.py
================================================
from .anthropic import AnthropicLLM

metadata = {
    "name": AnthropicLLM.__name__,
    "version": "1.0.0",
    "adapter": AnthropicLLM,
    "description": "Anthropic LLM adapter",
    "is_active": True,
}

__all__ = ["AnthropicLLM"]


================================================
File: src/unstract/sdk/adapters/llm/anthropic/src/anthropic.py
================================================
import os
from typing import Any

from anthropic import APIError
from llama_index.core.llms import LLM
from llama_index.llms.anthropic import Anthropic
from llama_index.llms.anthropic.base import DEFAULT_ANTHROPIC_MAX_TOKENS

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.llm.constants import LLMKeys
from unstract.sdk.adapters.llm.llm_adapter import LLMAdapter
from unstract.sdk.exceptions import LLMError

from .exceptions import parse_anthropic_err


class Constants:
    MODEL = "model"
    API_KEY = "api_key"
    TIMEOUT = "timeout"
    MAX_RETRIES = "max_retries"
    MAX_TOKENS = "max_tokens"


class AnthropicLLM(LLMAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("Anthropic")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "anthropic|90ebd4cd-2f19-4cef-a884-9eeb6ac0f203"

    @staticmethod
    def get_name() -> str:
        return "Anthropic"

    @staticmethod
    def get_description() -> str:
        return "Anthropic LLM"

    @staticmethod
    def get_provider() -> str:
        return "anthropic"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/Anthropic.png"

    def get_llm_instance(self) -> LLM:
        max_tokens = int(
            self.config.get(Constants.MAX_TOKENS, DEFAULT_ANTHROPIC_MAX_TOKENS)
        )
        try:
            llm: LLM = Anthropic(
                model=str(self.config.get(Constants.MODEL)),
                api_key=str(self.config.get(Constants.API_KEY)),
                timeout=float(
                    self.config.get(Constants.TIMEOUT, LLMKeys.DEFAULT_TIMEOUT)
                ),
                max_retries=int(
                    self.config.get(Constants.MAX_RETRIES, LLMKeys.DEFAULT_MAX_RETRIES)
                ),
                temperature=0,
                max_tokens=max_tokens,
            )
            return llm
        except Exception as e:
            raise AdapterError(str(e))

    @staticmethod
    def parse_llm_err(e: APIError) -> LLMError:
        """Parse the error from Anthropic.

        Helps parse errors from Anthropic and wraps with custom exception.

        Args:
            e (AnthropicAPIError): Exception from Anthropic

        Returns:
            LLMError: Error to be sent to the user
        """
        if hasattr(e, "body") and isinstance(e.body, dict) and "error" in e.body:
            err = e.body["error"]
            msg = parse_anthropic_err(err)
        else:
            msg = e.message

        return LLMError(msg, actual_err=e)


================================================
File: src/unstract/sdk/adapters/llm/anthropic/src/exceptions.py
================================================
from typing import Any


class ErrorType:
    AUTH_ERR = "authentication_error"
    PERMISSION_ERR = "permission_error"
    NOT_FOUND_ERR = "not_found_error"
    API_ERR = "api_error"
    OVERLOADED_ERR = "overloaded_error"
    RATE_LIMIT_ERR = "rate_limit_error"
    LARGE_REQUEST_ERR = "request_too_large"


def parse_anthropic_err(err_body: dict[str, Any]) -> str:
    """Parses error from Anthropic.

    Refer https://docs.anthropic.com/en/api/errors#http-errors

    Args:
        err_body (dict[str, Any]): Error from Anthropic to parse

    Returns:
        str: Parsed error from Anthropic
    """
    err_type = err_body.get("type")
    msg: str

    if err_type == ErrorType.AUTH_ERR:
        msg = "Authentication error, please check the provided API key"
    elif err_type == ErrorType.PERMISSION_ERR:
        msg = "Your API key does not have permission to use the specified resource"
    elif err_type == ErrorType.NOT_FOUND_ERR:
        err_msg = err_body.get("message")
        msg = f"The requested resource was not found, '{err_msg}'"
    elif err_type == ErrorType.RATE_LIMIT_ERR:
        msg = (
            "Your Anthropic account has hit a rate limit, "
            "please try again after some time"
        )
    elif err_type == ErrorType.LARGE_REQUEST_ERR:
        err_msg = err_body.get("message")
        msg = (
            f"Request exceeded the maximum allowed number of bytes, '{err_msg}'. "
            "Try reducing the number of prompts or reduce the context size by chunking."
        )
    elif err_type == ErrorType.API_ERR:
        msg = "An unexpected error has occurred internal to Anthropic's systems"
    elif err_type == ErrorType.OVERLOADED_ERR:
        msg = (
            "Anthropic's API is temporarily overloaded, please "
            "try again after some time"
        )
    else:
        msg = err_body.get("message", "Anthropic error")
    return msg


================================================
File: src/unstract/sdk/adapters/llm/anthropic/src/static/json_schema.json
================================================
{
  "title": "Anthropic LLM",
  "type": "object",
  "required": [
    "api_key",
    "model",
    "adapter_name"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: anthropic-group-1"
    },
    "model": {
      "type": "string",
      "title": "Model",
      "default": "claude-instant-1.2",
      "description": "Model name. Refer to Anthropic's documentation for the list of available models."
    },
    "api_key": {
      "type": "string",
      "title": "API Key",
      "default": "",
      "description": "API Key",
      "format": "password"
    },
    "max_tokens": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "default": 512,
      "title": "Maximum Output Tokens",
      "description": "Maximum number of output tokens to limit LLM replies, the maximum possible differs from model to model."
    },
    "max_retries": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Max Retries",
      "default": 3,
      "format": "number",
      "description": "Maximum number of retries"
    },
    "timeout": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Timeout",
      "default": 900,
      "description": "Timeout in seconds"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/llm/any_scale/README.md
================================================
# Unstract Replicate LLM Adapter


================================================
File: src/unstract/sdk/adapters/llm/any_scale/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-any-scale-llm"
version = "0.0.1"
description = "AnyScale LLM"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/llm/any_scale/src/__init__.py
================================================
from .anyscale import AnyScaleLLM

metadata = {
    "name": AnyScaleLLM.__name__,
    "version": "1.0.0",
    "adapter": AnyScaleLLM,
    "description": "AnyScale LLM adapter",
    "is_active": True,
}

__all__ = ["AnyScaleLLM"]


================================================
File: src/unstract/sdk/adapters/llm/any_scale/src/anyscale.py
================================================
import os
from typing import Any

from llama_index.core.constants import DEFAULT_NUM_OUTPUTS
from llama_index.core.llms import LLM
from llama_index.llms.anyscale import Anyscale

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.llm.constants import LLMKeys
from unstract.sdk.adapters.llm.llm_adapter import LLMAdapter


class Constants:
    MODEL = "model"
    API_KEY = "api_key"
    API_BASE = "api_base"
    MAX_RETRIES = "max_retries"
    ADDITIONAL_KWARGS = "additional_kwargs"
    MAX_TOKENS = "max_tokens"


class AnyScaleLLM(LLMAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("AnyScale")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "anyscale|adec9815-eabc-4207-9389-79cb89952639"

    @staticmethod
    def get_name() -> str:
        return "AnyScale"

    @staticmethod
    def get_description() -> str:
        return "AnyScale LLM"

    @staticmethod
    def get_provider() -> str:
        return "anyscale"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/anyscale.png"

    def get_llm_instance(self) -> LLM:
        try:
            max_tokens = int(self.config.get(Constants.MAX_TOKENS, DEFAULT_NUM_OUTPUTS))
            llm: LLM = Anyscale(
                model=str(self.config.get(Constants.MODEL)),
                api_key=str(self.config.get(Constants.API_KEY)),
                api_base=str(self.config.get(Constants.API_BASE)),
                additional_kwargs=self.config.get(Constants.ADDITIONAL_KWARGS),
                max_retries=int(
                    self.config.get(Constants.MAX_RETRIES, LLMKeys.DEFAULT_MAX_RETRIES)
                ),
                temperature=0,
                max_tokens=max_tokens,
            )
            return llm
        except Exception as e:
            raise AdapterError(str(e))


================================================
File: src/unstract/sdk/adapters/llm/any_scale/src/static/json_schema.json
================================================
{
  "title": "AnyScale LLM",
  "type": "object",
  "required": [
    "api_key",
    "adapter_name"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: anyscale-group-1"
    },
    "model": {
      "type": "string",
      "title": "Model",
      "default": "mistralai/Mistral-7B-Instruct-v0.1",
      "description": "Model name. Example: mistralai/Mistral-7B-Instruct-v0.1"
    },
    "additional_kwargs": {
      "type": "string",
      "title": "Additional kwargs",
      "default": "",
      "description": "Additional kwargs to pass to the model."
    },
    "api_base": {
      "type": "string",
      "title": "API Base",
      "default": "https://api.endpoints.anyscale.com/v1",
      "format": "uri",
      "description": "Base URL for the AnyScale API. Change it from the default if you are using a custom deployment."
    },
    "api_key": {
      "type": "string",
      "title": "API Key",
      "format": "password",
      "description": "API Key for the AnyScale API."
    },
    "max_tokens": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Maximum Output Tokens",
      "default": 256,
      "description": "Maximum number of output tokens to limit LLM replies, maximum possible varies from model to model."
    },
    "max_retries": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Max Retries",
      "default": 5,
      "description": "Maximum number of retries to attempt when a request fails."
    }
  }
}


================================================
File: src/unstract/sdk/adapters/llm/azure_open_ai/README.md
================================================
# Unstract Azure OpenAI LLM Adapter


================================================
File: src/unstract/sdk/adapters/llm/azure_open_ai/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-azure-open-ai-llm"
version = "0.0.1"
description = "Azure OpenAI LLM"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/llm/azure_open_ai/src/__init__.py
================================================
from .azure_open_ai import AzureOpenAILLM

metadata = {
    "name": AzureOpenAILLM.__name__,
    "version": "1.0.0",
    "adapter": AzureOpenAILLM,
    "description": "AzureOpenAI LLM adapter",
    "is_active": True,
}

__all__ = ["AzureOpenAILLM"]


================================================
File: src/unstract/sdk/adapters/llm/azure_open_ai/src/azure_open_ai.py
================================================
import os
from typing import Any

from llama_index.core.llms import LLM
from llama_index.llms.azure_openai import AzureOpenAI

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.llm.constants import LLMKeys
from unstract.sdk.adapters.llm.llm_adapter import LLMAdapter


class Constants:
    MODEL = "model"
    DEPLOYMENT_NAME = "deployment_name"
    API_KEY = "api_key"
    API_VERSION = "api_version"
    MAX_RETRIES = "max_retries"
    MAX_TOKENS = "max_tokens"
    AZURE_ENDPONT = "azure_endpoint"
    API_TYPE = "azure"
    TIMEOUT = "timeout"
    DEFAULT_MODEL = "gpt-35-turbo"


class AzureOpenAILLM(LLMAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("AzureOpenAI")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "azureopenai|592d84b9-fe03-4102-a17e-6b391f32850b"

    @staticmethod
    def get_name() -> str:
        return "AzureOpenAI"

    @staticmethod
    def get_description() -> str:
        return "AzureOpenAI LLM"

    @staticmethod
    def get_provider() -> str:
        return "azure"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/AzureopenAI.png"

    def get_llm_instance(self) -> LLM:
        max_retries = int(
            self.config.get(Constants.MAX_RETRIES, LLMKeys.DEFAULT_MAX_RETRIES)
        )
        max_tokens = self.config.get(Constants.MAX_TOKENS)
        max_tokens = int(max_tokens) if max_tokens else None
        try:
            llm: LLM = AzureOpenAI(
                model=self.config.get(Constants.MODEL, Constants.DEFAULT_MODEL),
                deployment_name=str(self.config.get(Constants.DEPLOYMENT_NAME)),
                api_key=str(self.config.get(Constants.API_KEY)),
                api_version=str(self.config.get(Constants.API_VERSION)),
                azure_endpoint=str(self.config.get(Constants.AZURE_ENDPONT)),
                api_type=Constants.API_TYPE,
                temperature=0,
                timeout=float(
                    self.config.get(Constants.TIMEOUT, LLMKeys.DEFAULT_TIMEOUT)
                ),
                max_retries=max_retries,
                max_tokens=max_tokens,
            )
            return llm
        except Exception as e:
            raise AdapterError(str(e))


================================================
File: src/unstract/sdk/adapters/llm/azure_open_ai/src/static/json_schema.json
================================================
{
  "title": "Azure OpenAI LLM",
  "type": "object",
  "required": [
    "adapter_name",
    "deployment_name",
    "api_key",
    "azure_endpoint"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: azure-group-1"
    },
    "model": {
      "type": "string",
      "title": "Model",
      "default": "",
      "description": "Provide the name of the model you defined in Azure console"
    },
    "deployment_name": {
      "type": "string",
      "title": "Deployment Name",
      "default": "",
      "description": "Provide the name of the deployment you defined in Azure console"
    },
    "api_key": {
      "type": "string",
      "title": "API Key",
      "format": "password",
      "description": "Provide the API key"
    },
    "api_version": {
      "type": "string",
      "title": "API Version",
      "default": "2023-05-15",
      "description": "Provide the API version. Refer to the documentation provided by Azure. Example: 2023-05-15"
    },
    "azure_endpoint": {
      "type": "string",
      "title": "Azure Endpoint",
      "default": "",
      "format": "uri",
      "description": "Provide the Azure endpoint. Example: https://<your-deployment>.openai.azure.com/"
    },
    "max_tokens": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Maximum Output Tokens",
      "description": "Maximum number of output tokens to limit LLM replies, leave it empty to use the maximum possible for the selected model."
    },
    "max_retries": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Max Retries",
      "default": 5,
      "description": "Maximum number of retries to attempt when a request fails."
    },
    "timeout": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Timeout",
      "default": 900,
      "description": "Timeout in seconds"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/llm/bedrock/README.md
================================================
# Unstract Bedrock LLM Adapter


================================================
File: src/unstract/sdk/adapters/llm/bedrock/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-bedrock-llm"
version = "0.0.1"
description = "Bedrock LLM"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/llm/bedrock/src/__init__.py
================================================
from .bedrock import BedrockLLM

metadata = {
    "name": BedrockLLM.__name__,
    "version": "1.0.0",
    "adapter": BedrockLLM,
    "description": "Bedrock LLM adapter",
    "is_active": True,
}

__all__ = ["BedrockLLM"]


================================================
File: src/unstract/sdk/adapters/llm/bedrock/src/bedrock.py
================================================
import os
from typing import Any, Optional

from llama_index.core.llms import LLM
from llama_index.llms.bedrock import Bedrock

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.llm.constants import LLMKeys
from unstract.sdk.adapters.llm.llm_adapter import LLMAdapter


class Constants:
    MODEL = "model"
    API_KEY = "api_key"
    TIMEOUT = "timeout"
    MAX_RETRIES = "max_retries"
    SECRET_ACCESS_KEY = "aws_secret_access_key"
    ACCESS_KEY_ID = "aws_access_key_id"
    REGION_NAME = "region_name"
    CONTEXT_SIZE = "context_size"
    MAX_TOKENS = "max_tokens"
    DEFAULT_MAX_TOKENS = 512  # Default at llama-index


class BedrockLLM(LLMAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("Bedrock")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "bedrock|8d18571f-5e96-4505-bd28-ad0379c64064"

    @staticmethod
    def get_name() -> str:
        return "Bedrock"

    @staticmethod
    def get_description() -> str:
        return "Bedrock LLM"

    @staticmethod
    def get_provider() -> str:
        return "bedrock"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/Bedrock.png"  

    def get_llm_instance(self) -> LLM:
        try:
            context_size: Optional[int] = (
                int(self.config.get(Constants.CONTEXT_SIZE, 0))
                if self.config.get(Constants.CONTEXT_SIZE)
                else None
            )
            max_tokens = int(
                self.config.get(Constants.MAX_TOKENS, Constants.DEFAULT_MAX_TOKENS)
            )
            llm: LLM = Bedrock(
                model=self.config.get(Constants.MODEL),
                aws_access_key_id=self.config.get(Constants.ACCESS_KEY_ID),
                aws_secret_access_key=self.config.get(Constants.SECRET_ACCESS_KEY),
                region_name=self.config.get(Constants.REGION_NAME),
                timeout=float(
                    self.config.get(Constants.TIMEOUT, LLMKeys.DEFAULT_TIMEOUT)
                ),
                max_retries=int(
                    self.config.get(Constants.MAX_RETRIES, LLMKeys.DEFAULT_MAX_RETRIES)
                ),
                temperature=0,
                context_size=context_size,
                max_tokens=max_tokens,
            )
            return llm
        except Exception as e:
            raise AdapterError(str(e))

================================================
File: src/unstract/sdk/adapters/llm/bedrock/src/static/json_schema.json
================================================
{
  "title": "Bedrock LLM",
  "type": "object",
  "required": [
    "aws_secret_access_key",
    "region_name",
    "aws_access_key_id",
    "model",
    "adapter_name"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: Bedrock-1"
    },
    "model": {
      "type": "string",
      "title": "Model",
      "default": "amazon.titan-text-express-v1",
      "description": "Model name. Refer to [Bedrock's documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html) for the list of available models."
    },
    "aws_access_key_id": {
      "type": "string",
      "title": "AWS Access Key ID",
      "description": "Provide your AWS Access Key ID",
      "format": "password"
    },
    "aws_secret_access_key": {
      "type": "string",
      "title": "AWS Secret Access Key",
      "description": "Provide your AWS Secret Access Key",
      "format": "password"
    },
    "region_name": {
      "type": "string",
      "title": "AWS Region name",
      "description": "Provide the AWS Region name where the service is running. Eg. us-east-1"
    },
    "context_size": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Context Size",
      "description": "The maximum number of context (input) tokens for the model. For setting default in supported models, leave this empty."
    },
    "max_tokens": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "default": 512,
      "title": "Maximum Output Tokens",
      "description": "Maximum number of output tokens to limit LLM replies, the maximum possible differs from model to model."
    },
    "max_retries": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Max Retries",
      "default": 5,
      "description": "Maximum number of retries to attempt when a request fails."
    },
    "timeout": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Timeout",
      "default": 900,
      "description": "Timeout in seconds"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/llm/mistral/README.md
================================================
# Unstract Mistral AI LLM Adapter


================================================
File: src/unstract/sdk/adapters/llm/mistral/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-mistral-llm"
version = "0.0.1"
description = "Mistral AI LLM"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/llm/mistral/src/__init__.py
================================================
from .mistral import MistralLLM

metadata = {
    "name": MistralLLM.__name__,
    "version": "1.0.0",
    "adapter": MistralLLM,
    "description": "Mistral LLM adapter",
    "is_active": True,
}

__all__ = ["MistralLLM"]


================================================
File: src/unstract/sdk/adapters/llm/mistral/src/mistral.py
================================================
import os
from typing import Any

from llama_index.core.llms import LLM
from llama_index.llms.mistralai import MistralAI
from llama_index.llms.mistralai.base import DEFAULT_MISTRALAI_MAX_TOKENS
from mistralai.models import SDKError as MistralError

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.llm.constants import LLMKeys
from unstract.sdk.adapters.llm.llm_adapter import LLMAdapter
from unstract.sdk.exceptions import LLMError


class Constants:
    MODEL = "model"
    API_KEY = "api_key"
    TIMEOUT = "timeout"
    MAX_RETRIES = "max_retries"
    MAX_TOKENS = "max_tokens"


class MistralLLM(LLMAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("Mistral")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "mistral|00f766a5-6d6d-47ea-9f6c-ddb1e8a94e82"

    @staticmethod
    def get_name() -> str:
        return "Mistral AI"

    @staticmethod
    def get_description() -> str:
        return "Mistral AI LLM"

    @staticmethod
    def get_provider() -> str:
        return "mistral"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/Mistral%20AI.png"

    def get_llm_instance(self) -> LLM:
        max_retries = int(
            self.config.get(Constants.MAX_RETRIES, LLMKeys.DEFAULT_MAX_RETRIES)
        )
        max_tokens = int(
            self.config.get(Constants.MAX_RETRIES, DEFAULT_MISTRALAI_MAX_TOKENS)
        )
        try:
            llm: LLM = MistralAI(
                model=str(self.config.get(Constants.MODEL)),
                api_key=str(self.config.get(Constants.API_KEY)),
                temperature=0,
                timeout=float(
                    self.config.get(Constants.TIMEOUT, LLMKeys.DEFAULT_TIMEOUT)
                ),
                max_retries=max_retries,
                max_tokens=max_tokens,
            )
            return llm
        except Exception as e:
            raise AdapterError(str(e))

    @staticmethod
    def parse_llm_err(e: MistralError) -> LLMError:
        """Parse the error from MistralAI.

        Helps parse errors from MistralAI and wraps with custom exception.

        Args:
            e (OpenAIAPIError): Exception from MistralAI

        Returns:
            LLMError: Error to be sent to the user
        """
        if e.message and e.message.find('"message":"Unauthorized"'):
            return LLMError(
                "Incorrect API key, please check the API key provided.",
                actual_err=e,
                status_code=401,
            )
        return LLMError(str(e), actual_err=e)


================================================
File: src/unstract/sdk/adapters/llm/mistral/src/static/json_schema.json
================================================
{
  "title": "Mistral AI LLM",
  "type": "object",
  "required": [
    "adapter_name",
    "api_key",
    "model"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: mistral-group-1"
    },
    "model": {
      "type": "string",
      "title": "Model",
      "default": "mistral-medium",
      "description": "Provide the model name to be used. Example: mistral-tiny, mistral-small, mistral-medium"
    },
    "api_key": {
      "type": "string",
      "title": "API Key",
      "format": "password",
      "description": "API Key for Mistral AI LLM"
    },
    "max_retries": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Max Retries",
      "default": 5,
      "description": "Maximum number of retries to attempt when a request fails."
    },
    "max_tokens": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "default": 512,
      "title": "Maximum Output Tokens",
      "description": "Maximum number of output tokens to limit LLM replies, the maximum possible differs from model to model."
    },
    "timeout": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Timeout",
      "default": 900,
      "description": "Timeout in seconds"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/llm/no_op/README.md
================================================
# Unstract NoOp LLM adapter for load testing

An LLM adapter that does not perform any operation. Waits for the configured time before returning a response. This can be useful to perform tests on the system


================================================
File: src/unstract/sdk/adapters/llm/no_op/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-no-op-llm"
version = "0.0.1"
description = "noOp LLM"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/llm/no_op/src/__init__.py
================================================
from unstract.sdk.adapters.llm.no_op.src.no_op_llm import NoOpLLM

metadata = {
    "name": NoOpLLM.__name__,
    "version": "1.0.0",
    "adapter": NoOpLLM,
    "description": "NoOp LLM adapter",
    "is_active": True,
}

__all__ = ["NoOpLLM"]


================================================
File: src/unstract/sdk/adapters/llm/no_op/src/no_op_custom_llm.py
================================================
import time
from typing import Any

from llama_index.core.base.llms.types import (
    CompletionResponse,
    CompletionResponseGen,
    LLMMetadata,
)
from llama_index.core.llms.custom import CustomLLM


class NoOpCustomLLM(CustomLLM):
    wait_time: float

    def __init__(
        self,
        wait_time: float,
    ) -> None:
        wait_time = wait_time
        super().__init__(wait_time=wait_time)

    @classmethod
    def class_name(cls) -> str:
        return "NoOpLLM"

    def _generate_text(self) -> str:
        # Returns a JSON here to support for all enforce types.
        return '{ "response":"This is a sample response from a NoOp LLM Adapter."}'

    def complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -> CompletionResponse:
        time.sleep(self.wait_time)
        response_text = self._generate_text()

        return CompletionResponse(
            text=response_text,
        )

    def stream_complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -> CompletionResponseGen:

        def gen_response() -> CompletionResponseGen:
            response_text = self._generate_text()
            yield CompletionResponse(
                text=response_text,
                delta=response_text,
            )

        time.sleep(self.wait_time)

        return gen_response()

    @property
    def metadata(self) -> LLMMetadata:
        """Method to fetch LLM metadata. Overriden to extent Base class.

        Returns:
            LLMMetadata
        """
        return LLMMetadata(num_output=-1)


================================================
File: src/unstract/sdk/adapters/llm/no_op/src/no_op_llm.py
================================================
import logging
import os
import time
from typing import Any

from llama_index.core.llms import LLM

from unstract.sdk.adapters.llm.llm_adapter import LLMAdapter
from unstract.sdk.adapters.llm.no_op.src.no_op_custom_llm import NoOpCustomLLM

logger = logging.getLogger(__name__)


class NoOpLLM(LLMAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("NoOpLlm")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "noOpLlm|f673a5a2-90f9-40f5-94c0-9fbc663b7553"

    @staticmethod
    def get_name() -> str:
        return "No Op LLM"

    @staticmethod
    def get_description() -> str:
        return "No Op LLM"

    @staticmethod
    def get_provider() -> str:
        return "noOpLlm"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/noOpLlm.png"

    def get_llm_instance(self) -> LLM:
        llm: LLM = NoOpCustomLLM(wait_time=self.config.get("wait_time"))
        return llm

    def test_connection(self) -> bool:
        llm = self.get_llm_instance()
        if not llm:
            return False
        llm.complete(
            "The capital of Tamilnadu is ",
            temperature=0.003,
        )
        time.sleep(self.config.get("wait_time"))
        return True


================================================
File: src/unstract/sdk/adapters/llm/no_op/src/static/json_schema.json
================================================
{
  "title": "No Op LLM",
  "type": "object",
  "required": [
    "adapter_name",
    "wait_time"
  ],
  "description": "No Op LLM does not perform any operation, its used to test the performance of the system in the absence of 3rd party induced latencies",
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this No Op adapter instance. Example: no-op-instance-1"
    },
    "wait_time": {
      "type": "number",
      "title": "Wait time (in seconds)",
      "default": "0",
      "description": "Provide the time to wait (in seconds) before returning the response"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/llm/ollama/README.md
================================================
# Unstract Ollama AI LLM Adapter


================================================
File: src/unstract/sdk/adapters/llm/ollama/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-ollama-llm"
version = "0.0.1"
description = "Ollama AI LLM"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/llm/ollama/src/__init__.py
================================================
from .ollama import OllamaLLM

metadata = {
    "name": OllamaLLM.__name__,
    "version": "1.0.0",
    "adapter": OllamaLLM,
    "description": "Ollama LLM adapter",
    "is_active": True,
}

__all__ = ["OllamaLLM"]


================================================
File: src/unstract/sdk/adapters/llm/ollama/src/ollama.py
================================================
import logging
import os
import re
from typing import Any

from httpx import ConnectError, HTTPStatusError
from llama_index.core.llms import LLM
from llama_index.llms.ollama import Ollama

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.llm.constants import LLMKeys
from unstract.sdk.adapters.llm.llm_adapter import LLMAdapter

logger = logging.getLogger(__name__)


class Constants:
    MODEL = "model"
    API_KEY = "api_key"
    TIMEOUT = "timeout"
    BASE_URL = "base_url"
    JSON_MODE = "json_mode"
    CONTEXT_WINDOW = "context_window"
    MODEL_MISSING_ERROR = "try pulling it first"


class OllamaLLM(LLMAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("Ollama")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "ollama|4b8bd31a-ce42-48d4-9d69-f29c12e0f276"

    @staticmethod
    def get_name() -> str:
        return "Ollama"

    @staticmethod
    def get_description() -> str:
        return "Ollama AI LLM"

    @staticmethod
    def get_provider() -> str:
        return "ollama"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/ollama.png"

    def get_llm_instance(self) -> LLM:
        try:
            llm: LLM = Ollama(
                model=str(self.config.get(Constants.MODEL)),
                base_url=str(self.config.get(Constants.BASE_URL)),
                request_timeout=float(
                    self.config.get(Constants.TIMEOUT, LLMKeys.DEFAULT_TIMEOUT)
                ),
                json_mode=False,
                context_window=int(self.config.get(Constants.CONTEXT_WINDOW, 3900)),
                temperature=0.01,
            )
            return llm

        except ConnectError as connec_err:
            logger.error(f"Ollama server not running : {connec_err}")
            raise AdapterError(
                "Unable to connect to Ollama`s Server, "
                "please check if the server is up and running or"
                "if it is accepting connections."
            )
        except Exception as exc:
            logger.error(f"Error occured while getting llm instance:{exc}")
            raise AdapterError(str(exc))

    def test_connection(self) -> bool:
        try:
            llm = self.get_llm_instance()
            if not llm:
                return False
            response = llm.complete(
                "The capital of Tamilnadu is ",
                temperature=0.003,
            )
            response_lower_case: str = response.text.lower()
            find_match = re.search("chennai", response_lower_case)
            if find_match:
                return True
            else:
                return False
        except HTTPStatusError as http_err:
            if http_err.response:
                if (
                    http_err.response.status_code == 404
                    and Constants.MODEL_MISSING_ERROR in http_err.response.text
                ):
                    logger.error(
                        f"Error occured while sending requst to the model{http_err}"
                    )
                    raise AdapterError(
                        "Model under use is not found. Try pulling it first."
                    )
            raise AdapterError(
                f"Some issue while communicating with the model. "
                f"Details : {http_err.response.text}"
            )

        except Exception as e:
            logger.error(f"Error occured while testing adapter {e}")
            raise AdapterError(str(e))


================================================
File: src/unstract/sdk/adapters/llm/ollama/src/static/json_schema.json
================================================
{
  "title": "Ollama AI LLM",
  "type": "object",
  "required": [
    "adapter_name",
    "base_url",
    "model"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this LLM adapter instance. Example: ollama-instance-1"
    },
    "model": {
      "type": "string",
      "title": "Model",
      "default": "",
      "description": "Provide the model name to be used. Example:llama2, llama3, mistral"
    },
    "base_url": {
      "type": "string",
      "title": "Base URL",
      "default": "",
      "description": "Provide the base URL where Ollama server is running. Example: http://docker.host.internal:11434 or http://localhost:11434"
    },
    "context_window": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Context window",
      "default":3900,
      "description": "The maximum number of context tokens for the model."
    },
    "request_timeout": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Request Timeout",
      "default": 900,
      "description": "Request timeout in seconds"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/llm/open_ai/README.md
================================================
# Unstract OpenAI LLM Adapter


================================================
File: src/unstract/sdk/adapters/llm/open_ai/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-open-ai-llm"
version = "0.0.1"
description = "OpenAI LLM"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/llm/open_ai/src/__init__.py
================================================
from .open_ai import OpenAILLM

metadata = {
    "name": OpenAILLM.__name__,
    "version": "1.0.0",
    "adapter": OpenAILLM,
    "description": "OpenAI LLM adapter",
    "is_active": True,
}

__all__ = ["OpenAILLM"]


================================================
File: src/unstract/sdk/adapters/llm/open_ai/src/open_ai.py
================================================
import os
from typing import Any

from llama_index.core.llms import LLM
from llama_index.llms.openai import OpenAI
from llama_index.llms.openai.utils import O1_MODELS
from openai import APIError as OpenAIAPIError

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.llm.constants import LLMKeys
from unstract.sdk.adapters.llm.llm_adapter import LLMAdapter
from unstract.sdk.exceptions import LLMError

class Constants:
    MODEL = "model"
    API_KEY = "api_key"
    MAX_RETRIES = "max_retries"
    ADAPTER_NAME = "adapter_name"
    TIMEOUT = "timeout"
    API_BASE = "api_base"
    API_VERSION = "api_version"
    MAX_TOKENS = "max_tokens"


class OpenAILLM(LLMAdapter):
    
    def __init__(self, settings: dict[str, Any]):
        super().__init__("OpenAI")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "openai|502ecf49-e47c-445c-9907-6d4b90c5cd17"

    @staticmethod
    def get_name() -> str:
        return "OpenAI"

    @staticmethod
    def get_description() -> str:
        return "OpenAI LLM"

    @staticmethod
    def get_provider() -> str:
        return "openai"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/OpenAI.png"

    def get_llm_instance(self) -> LLM:
        try:
            max_tokens = self.config.get(Constants.MAX_TOKENS)
            max_tokens = int(max_tokens) if max_tokens else None
            model = str(self.config.get(Constants.MODEL))

            llm_kwargs = {
                "model": model,
                "api_key": str(self.config.get(Constants.API_KEY)),
                "api_base": str(self.config.get(Constants.API_BASE)),
                "api_version": str(self.config.get(Constants.API_VERSION)),
                "max_retries": int(self.config.get(Constants.MAX_RETRIES, LLMKeys.DEFAULT_MAX_RETRIES)),
                "api_type": "openai",
                "timeout": float(self.config.get(Constants.TIMEOUT, LLMKeys.DEFAULT_TIMEOUT)),
                "max_tokens": max_tokens,
            }

            # O-series models default to temperature=1, ignoring passed values, so it's not set explicitly.
            if model not in O1_MODELS:
                llm_kwargs["temperature"] = 0

            llm = OpenAI(**llm_kwargs)
            return llm
        except Exception as e:
            raise AdapterError(str(e))

    @staticmethod
    def parse_llm_err(e: OpenAIAPIError) -> LLMError:
        """Parse the error from OpenAI.

        Helps parse errors from OpenAI and wraps with custom exception.

        Args:
            e (OpenAIAPIError): Exception from OpenAI

        Returns:
            LLMError: Error to be sent to the user
        """
        if hasattr(e, "body") and isinstance(e.body, dict) and "message" in e.body:
            msg = e.body["message"]
        else:
            msg = e.message

        status_code = e.status_code if hasattr(e, "status_code") else None
        return LLMError(msg, actual_err=e, status_code=status_code)


================================================
File: src/unstract/sdk/adapters/llm/open_ai/src/static/json_schema.json
================================================
{
  "title": "OpenAI LLM",
  "type": "object",
  "required": [
    "adapter_name",
    "api_key",
    "api_base"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: openai-group-1"
    },
    "api_key": {
      "type": "string",
      "title": "API Key",
      "format": "password",
      "description": "Your OpenAI API key."
    },
    "model": {
      "type": "string",
      "title": "Model",
      "default": "gpt-3.5-turbo",
      "description": "The model to use for the API request. Refer to https://platform.openai.com/docs/models"
    },
    "api_base": {
      "type": "string",
      "format": "url",
      "title": "API Base",
      "default": "https://api.openai.com/v1",
      "description": "Provide the OpenAI endpoint. Example: https://api.openai.com/v1"
    },
    "api_version": {
      "type": "string",
      "title": "API Version",
      "default": "2023-05-15",
      "description": "Provide the API version. Refer to the documentation provided by OpenAI. Example: 2023-05-15"
    },
    "max_tokens": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Maximum Output Tokens",
      "description": "Maximum number of output tokens to limit LLM replies, leave it empty to use the maximum possible for the selected model."
    },
    "max_retries": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Max Retries",
      "default": 5,
      "description": "The maximum number of times to retry a request if it fails."
    },
    "timeout": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Timeout",
      "default": 900,
      "description": "Timeout in seconds"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/llm/palm/README.md
================================================
# Unstract Pa LLM Adapter


================================================
File: src/unstract/sdk/adapters/llm/palm/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-palm-llm"
version = "0.0.1"
description = "Pa LLM"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/llm/palm/src/__init__.py
================================================
from .palm import PaLMLLM

metadata = {
    "name": PaLMLLM.__name__,
    "version": "1.0.0",
    "adapter": PaLMLLM,
    "description": "Palm LLM adapter",
    "is_active": True,
}

__all__ = ["PaLMLLM"]


================================================
File: src/unstract/sdk/adapters/llm/palm/src/palm.py
================================================
import os
from typing import Any, Optional

from google.api_core.exceptions import GoogleAPICallError
from llama_index.core.llms import LLM
from llama_index.llms.palm import PaLM

from unstract.sdk.adapters.llm.llm_adapter import LLMAdapter
from unstract.sdk.exceptions import LLMError


class Constants:
    MODEL = "model_name"
    API_KEY = "api_key"
    NUM_OUTPUT = "num_output"
    API_TYPE = "palm"
    DEFAULT_MAX_TOKENS = 1024


class PaLMLLM(LLMAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("PaLM")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "palm|af7c8ee7-3d01-47c5-9b81-5ffd7546014b"

    @staticmethod
    def get_name() -> str:
        return "Palm"

    @staticmethod
    def get_description() -> str:
        return "Palm LLM"

    @staticmethod
    def get_provider() -> str:
        return "palm"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/PaLM.png"

     

    def get_llm_instance(self) -> LLM:
        try:
            num_output: Optional[int] = (
                int(self.config.get(Constants.NUM_OUTPUT, Constants.DEFAULT_MAX_TOKENS))
                if self.config.get(Constants.NUM_OUTPUT) is not None
                else None
            )
            llm: LLM = PaLM(
                model=str(self.config.get(Constants.MODEL)),
                api_key=str(self.config.get(Constants.API_KEY)),
                num_output=num_output,
                api_type=Constants.API_TYPE,
                temperature=0,
            )
            return llm
        except Exception as e:
            # To avoid circular import errors
            from unstract.sdk.adapters.llm.exceptions import parse_llm_err

            raise parse_llm_err(e, llm_adapter=self)

    @staticmethod
    def parse_llm_err(e: GoogleAPICallError) -> LLMError:
        """Parse the error from PaLM.

        Helps parse errors from PaLM and wraps with custom exception.

        Args:
            e (OpenAIAPIError): Exception from PaLM

        Returns:
            LLMError: Error to be sent to the user
        """
        return LLMError(f"{e.message}", actual_err=e)


================================================
File: src/unstract/sdk/adapters/llm/palm/src/static/json_schema.json
================================================
{
  "title": "PaLM LLM",
  "type": "object",
  "required": [
    "adapter_name",
    "api_key"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: palm-group-1"
    },
    "api_key": {
      "type": "string",
      "title": "API Key",
      "format": "password",
      "description": "Your API key for the Palm API"
    },
    "model_name": {
      "type": "string",
      "title": "Model Name",
      "default": "models/text-bison-001",
      "description": "The name of the model to use for this adapter instance. Refer https://ai.google.dev/models/palm. Leave it empty to use the default model."
    },
    "num_output": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Max output tokens",
      "description": "The number of tokens to generate. This is limited by the maximum supported by the model and will vary from model to model. The higher the number, the longer the response will be. Leave it empty to use the model's maximum."
    }
  }
}


================================================
File: src/unstract/sdk/adapters/llm/replicate/README.md
================================================
# Unstract Replicate LLM Adapter


================================================
File: src/unstract/sdk/adapters/llm/replicate/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-replicate-llm"
version = "0.0.1"
description = "Replicate LLM"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/llm/replicate/src/__init__.py
================================================
from .replicate import ReplicateLLM

metadata = {
    "name": ReplicateLLM.__name__,
    "version": "1.0.0",
    "adapter": ReplicateLLM,
    "description": "Replicate LLM adapter",
    "is_active": True,
}

__all__ = ["ReplicateLLM"]


================================================
File: src/unstract/sdk/adapters/llm/replicate/src/replicate.py
================================================
import os
from typing import Any

from llama_index.core.llms import LLM
from llama_index.llms.replicate import Replicate

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.llm.llm_adapter import LLMAdapter


class Constants:
    MODEL = "model"
    API_KEY = "api_key"


class ReplicateLLM(LLMAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("Replicate")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "replicate|2715ce84-05af-4ab4-b8e9-67ac3211b81e"

    @staticmethod
    def get_name() -> str:
        return "Replicate"

    @staticmethod
    def get_description() -> str:
        return "Replicate LLM"

    @staticmethod
    def get_provider() -> str:
        return "replicate"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/Replicate.png"

     

    @staticmethod
    def can_write() -> bool:
        return True

    @staticmethod
    def can_read() -> bool:
        return True

    def get_llm_instance(self) -> LLM:
        try:
            llm: LLM = Replicate(
                model=str(self.config.get(Constants.MODEL)),
                prompt_key=str(self.config.get(Constants.API_KEY)),
                temperature=0,
            )
            return llm
        except Exception as e:
            raise AdapterError(str(e))


================================================
File: src/unstract/sdk/adapters/llm/replicate/src/static/json_schema.json
================================================
{
  "title": "Replicate LLM",
  "type": "object",
  "required": [
    "adapter_name",
    "api_key"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: replicate-group-1"
    },
    "model": {
      "type": "string",
      "title": "Model",
      "default": "mistralai/mistral-7b-instruct-v0.1:83b6a56e7c828e667f21fd596c338fd4f0039b46bcfa18d973e8e70e455fda70",
      "description": "Provide the model name. Example: mistralai/mistral-7b-instruct-v0.1:83b6a56e7c828e667f21fd596c338fd4f0039b46bcfa18d973e8e70e455fda70"
    },
    "api_key": {
      "type": "string",
      "title": "API Key",
      "format": "password",
      "description": "Provide the API key for the model."
    }
  }
}


================================================
File: src/unstract/sdk/adapters/llm/vertex_ai/README.md
================================================
# Unstract VertexAI LLM Adapter
Vertex AI is a machine learning (ML) platform that lets you train and deploy ML models and AI applications. Vertex AI combines data engineering, data science, and ML engineering workflows, enabling team collaboration using a common toolset.


================================================
File: src/unstract/sdk/adapters/llm/vertex_ai/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-vertex-ai-llm"
version = "0.0.1"
description = "Vertex AI LLM"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"


================================================
File: src/unstract/sdk/adapters/llm/vertex_ai/src/__init__.py
================================================
from .vertex_ai import VertexAILLM

metadata = {
    "name": VertexAILLM.__name__,
    "version": "1.0.0",
    "adapter": VertexAILLM,
    "description": "VertexAI LLM adapter",
    "is_active": True,
}

__all__ = ["VertexAILLM"]


================================================
File: src/unstract/sdk/adapters/llm/vertex_ai/src/vertex_ai.py
================================================
import json
import logging
import os
from typing import Any, Optional

from google.auth.transport import requests as google_requests
from google.oauth2.service_account import Credentials
from llama_index.core.llms import LLM
from llama_index.llms.vertex import Vertex
from vertexai.generative_models import Candidate, FinishReason, ResponseValidationError
from vertexai.generative_models._generative_models import (
    HarmBlockThreshold,
    HarmCategory,
)

from unstract.sdk.adapters.exceptions import LLMError
from unstract.sdk.adapters.llm.constants import LLMKeys
from unstract.sdk.adapters.llm.llm_adapter import LLMAdapter

logger = logging.getLogger(__name__)


class Constants:
    MODEL = "model"
    PROJECT = "project"
    JSON_CREDENTIALS = "json_credentials"
    MAX_RETRIES = "max_retries"
    MAX_TOKENS = "max_tokens"
    DEFAULT_MAX_TOKENS = 2048
    BLOCK_ONLY_HIGH = "BLOCK_ONLY_HIGH"


class SafetySettingsConstants:
    SAFETY_SETTINGS = "safety_settings"
    DANGEROUS_CONTENT = "dangerous_content"
    HATE_SPEECH = "hate_speech"
    HARASSMENT = "harassment"
    SEXUAL_CONTENT = "sexual_content"
    OTHER = "other"


UNSTRACT_VERTEX_SAFETY_THRESHOLD_MAPPING: dict[str, HarmBlockThreshold] = {
    "HARM_BLOCK_THRESHOLD_UNSPECIFIED": HarmBlockThreshold.HARM_BLOCK_THRESHOLD_UNSPECIFIED,  # noqa: E501
    "BLOCK_LOW_AND_ABOVE": HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    "BLOCK_MEDIUM_AND_ABOVE": HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    "BLOCK_ONLY_HIGH": HarmBlockThreshold.BLOCK_ONLY_HIGH,
    "BLOCK_NONE": HarmBlockThreshold.BLOCK_NONE,
    "OFF": HarmBlockThreshold.OFF,
}


class VertexAILLM(LLMAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("VertexAILLM")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "vertexai|78fa17a5-a619-47d4-ac6e-3fc1698fdb55"

    @staticmethod
    def get_name() -> str:
        return "VertexAI"

    @staticmethod
    def get_description() -> str:
        return "Vertex Gemini LLM"

    @staticmethod
    def get_provider() -> str:
        return "vertex_ai"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/VertexAI.png"

     

    def get_llm_instance(self) -> LLM:
        input_credentials = self.config.get(Constants.JSON_CREDENTIALS, "{}")
        try:
            json_credentials = json.loads(input_credentials)
        except json.JSONDecodeError:
            raise LLMError(
                "Credentials is not a valid service account JSON, "
                "please provide a valid JSON."
            )

        credentials = Credentials.from_service_account_info(
            info=json_credentials,
            scopes=["https://www.googleapis.com/auth/cloud-platform"],
        )  # type: ignore
        credentials.refresh(google_requests.Request())  # type: ignore
        max_retries = int(
            self.config.get(Constants.MAX_RETRIES, LLMKeys.DEFAULT_MAX_RETRIES)
        )
        max_tokens = int(
            self.config.get(Constants.MAX_TOKENS, Constants.DEFAULT_MAX_TOKENS)
        )

        safety_settings_default_config: dict[str, str] = {
            SafetySettingsConstants.DANGEROUS_CONTENT: Constants.BLOCK_ONLY_HIGH,
            SafetySettingsConstants.HATE_SPEECH: Constants.BLOCK_ONLY_HIGH,
            SafetySettingsConstants.HARASSMENT: Constants.BLOCK_ONLY_HIGH,
            SafetySettingsConstants.SEXUAL_CONTENT: Constants.BLOCK_ONLY_HIGH,
            SafetySettingsConstants.OTHER: Constants.BLOCK_ONLY_HIGH,
        }
        safety_settings_user_config: dict[str, str] = self.config.get(
            SafetySettingsConstants.SAFETY_SETTINGS,
            safety_settings_default_config,
        )

        vertex_safety_settings: dict[
            HarmCategory, HarmBlockThreshold
        ] = self._get_vertex_safety_settings(safety_settings_user_config)

        llm: LLM = Vertex(
            project=str(self.config.get(Constants.PROJECT)),
            model=str(self.config.get(Constants.MODEL)),
            credentials=credentials,
            temperature=0,
            max_retries=max_retries,
            max_tokens=max_tokens,
            safety_settings=vertex_safety_settings,
        )
        return llm

    def _get_vertex_safety_settings(
        self, safety_settings_user_config: dict[str, str]
    ) -> dict[HarmCategory, HarmBlockThreshold]:
        vertex_safety_settings: dict[HarmCategory, HarmBlockThreshold] = dict()
        vertex_safety_settings[
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT
        ] = UNSTRACT_VERTEX_SAFETY_THRESHOLD_MAPPING[
            (
                safety_settings_user_config.get(
                    SafetySettingsConstants.DANGEROUS_CONTENT,
                    Constants.BLOCK_ONLY_HIGH,
                )
            )
        ]
        vertex_safety_settings[
            HarmCategory.HARM_CATEGORY_HATE_SPEECH
        ] = UNSTRACT_VERTEX_SAFETY_THRESHOLD_MAPPING[
            (
                safety_settings_user_config.get(
                    SafetySettingsConstants.HATE_SPEECH,
                    Constants.BLOCK_ONLY_HIGH,
                )
            )
        ]
        vertex_safety_settings[
            HarmCategory.HARM_CATEGORY_HARASSMENT
        ] = UNSTRACT_VERTEX_SAFETY_THRESHOLD_MAPPING[
            (
                safety_settings_user_config.get(
                    SafetySettingsConstants.HARASSMENT,
                    Constants.BLOCK_ONLY_HIGH,
                )
            )
        ]
        vertex_safety_settings[
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT
        ] = UNSTRACT_VERTEX_SAFETY_THRESHOLD_MAPPING[
            (
                safety_settings_user_config.get(
                    SafetySettingsConstants.SEXUAL_CONTENT,
                    Constants.BLOCK_ONLY_HIGH,
                )
            )
        ]
        vertex_safety_settings[
            HarmCategory.HARM_CATEGORY_UNSPECIFIED
        ] = UNSTRACT_VERTEX_SAFETY_THRESHOLD_MAPPING[
            (
                safety_settings_user_config.get(
                    SafetySettingsConstants.OTHER, Constants.BLOCK_ONLY_HIGH
                )
            )
        ]
        return vertex_safety_settings

    @staticmethod
    def parse_llm_err(e: ResponseValidationError) -> LLMError:
        """Parse the error from Vertex AI.

        Helps parse and raise errors from Vertex AI.
        https://ai.google.dev/api/generate-content#generatecontentresponse

        Args:
            e (ResponseValidationError): Exception from Vertex AI

        Returns:
            LLMError: Error to be sent to the user
        """
        assert len(e.responses) == 1, (
            "Expected e.responses to contain a single element "
            "since its a completion call and not chat."
        )
        resp = e.responses[0]
        candidates: list["Candidate"] = resp.candidates
        if not candidates:
            msg = str(resp.prompt_feedback)
        reason_messages = {
            FinishReason.MAX_TOKENS: (
                "The maximum number of tokens for the LLM has been reached. Please "
                "either tweak your prompts or try using another LLM."
            ),
            FinishReason.STOP: (
                "The LLM stopped generating a response due to the natural stop "
                "point of the model or a provided stop sequence."
            ),
            FinishReason.SAFETY: "The LLM response was flagged for safety reasons.",
            FinishReason.RECITATION: (
                "The LLM response was flagged for recitation reasons."
            ),
            FinishReason.BLOCKLIST: (
                "The LLM response generation was stopped because it "
                "contains forbidden terms."
            ),
            FinishReason.PROHIBITED_CONTENT: (
                "The LLM response generation was stopped because it "
                "potentially contains prohibited content."
            ),
            FinishReason.SPII: (
                "The LLM response generation was stopped because it potentially "
                "contains Sensitive Personally Identifiable Information."
            ),
        }

        reason_status_code = {
            FinishReason.MAX_TOKENS: 429,
            FinishReason.STOP: 200,
            FinishReason.SAFETY: 403,
            FinishReason.RECITATION: 403,
            FinishReason.BLOCKLIST: 403,
            FinishReason.PROHIBITED_CONTENT: 403,
            FinishReason.SPII: 403,
        }

        err_list = []
        status_code: Optional[int] = None
        for candidate in candidates:
            reason: FinishReason = candidate.finish_reason

            if candidate.finish_message:
                err_msg = candidate.finish_message
            else:
                err_msg = reason_messages.get(reason, str(candidate))

            status_code = reason_status_code.get(reason)
            err_list.append(err_msg)
        msg = "\n\nAnother error: \n".join(err_list)
        return LLMError(msg, actual_err=e, status_code=status_code)


================================================
File: src/unstract/sdk/adapters/llm/vertex_ai/src/static/json_schema.json
================================================
{
  "title": "Vertex AI LLM",
  "type": "object",
  "required": [
    "adapter_name",
    "project",
    "json_credentials",
    "model"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: vertexai-group-1"
    },
    "json_credentials": {
      "type": "string",
      "title": "Credentials",
      "default": "",
      "description": "Refer [GCloud docs](https://developers.google.com/workspace/guides/create-credentials#create_credentials_for_a_service_account) on adding keys for service account."
    },
    "model": {
      "type": "string",
      "title": "Model",
      "default": "",
      "description": "Provide the name of the model you defined for Vertex AI"
    },
    "project": {
      "type": "string",
      "title": "Project",
      "default": "",
      "description": "Provide the name of the deployment or project you defined for Vertex AI"
    },
    "max_retries": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Max Retries",
      "default": 5,
      "description": "Maximum number of retries to attempt when a request fails."
    },
    "max_tokens": {
      "type": "number",
      "minimum": 0,
      "multipleOf": 1,
      "title": "Max output tokens",
      "default": 2048,
      "description": "Maximum number of output tokens to generate. This is limited by the maximum supported by the model and will vary from model to model"
    },
    "safety_settings": {
      "type": "object",
      "title": "Safety Settings",
      "description": "Vertex AI's configurable safety filters",
      "properties": {
        "type": "string",
        "dangerous_content": {
          "type": "string",
          "title": "Dangerous Content",
          "enum": [
            "HARM_BLOCK_THRESHOLD_UNSPECIFIED",
            "BLOCK_LOW_AND_ABOVE",
            "BLOCK_MEDIUM_AND_ABOVE",
            "BLOCK_ONLY_HIGH",
            "BLOCK_NONE",
            "OFF"
          ],
          "default": "BLOCK_ONLY_HIGH",
          "description": "Settings for HARM_CATEGORY_DANGEROUS_CONTENT"
        },
        "hate_speech": {
          "type": "string",
          "title": "Hate Speech",
          "enum": [
            "HARM_BLOCK_THRESHOLD_UNSPECIFIED",
            "BLOCK_LOW_AND_ABOVE",
            "BLOCK_MEDIUM_AND_ABOVE",
            "BLOCK_ONLY_HIGH",
            "BLOCK_NONE",
            "OFF"
          ],
          "default": "BLOCK_ONLY_HIGH",
          "description": "Settings for HARM_CATEGORY_HATE_SPEECH"
        },
        "harassment": {
          "type": "string",
          "title": "Harassment",
          "enum": [
            "HARM_BLOCK_THRESHOLD_UNSPECIFIED",
            "BLOCK_LOW_AND_ABOVE",
            "BLOCK_MEDIUM_AND_ABOVE",
            "BLOCK_ONLY_HIGH",
            "BLOCK_NONE",
            "OFF"
          ],
          "default": "BLOCK_ONLY_HIGH",
          "description": "Settings for HARM_CATEGORY_HARASSMENT"
        },
        "sexual_content": {
          "type": "string",
          "title": "Sexual Content",
          "enum": [
            "HARM_BLOCK_THRESHOLD_UNSPECIFIED",
            "BLOCK_LOW_AND_ABOVE",
            "BLOCK_MEDIUM_AND_ABOVE",
            "BLOCK_ONLY_HIGH",
            "BLOCK_NONE",
            "OFF"
          ],
          "default": "BLOCK_ONLY_HIGH",
          "description": "Settings for HARM_CATEGORY_SEXUAL_CONTENT"
        },
        "other": {
          "type": "string",
          "title": "Other",
          "enum": [
            "HARM_BLOCK_THRESHOLD_UNSPECIFIED",
            "BLOCK_LOW_AND_ABOVE",
            "BLOCK_MEDIUM_AND_ABOVE",
            "BLOCK_ONLY_HIGH",
            "BLOCK_NONE",
            "OFF"
          ],
          "default": "BLOCK_ONLY_HIGH",
          "description": "Settings for HARM_CATEGORY_OTHER"
        }
      }
    }
  }
}


================================================
File: src/unstract/sdk/adapters/ocr/__init__.py
================================================
from unstract.sdk.adapters import AdapterDict
from unstract.sdk.adapters.ocr.register import OCRRegistry

adapters: AdapterDict = {}
OCRRegistry.register_adapters(adapters)


================================================
File: src/unstract/sdk/adapters/ocr/constants.py
================================================
class FileType:
    TEXT_PLAIN = "text/plain"
    IMAGE_JPEG = "image/jpeg"
    IMAGE_PNG = "image/png"
    IMAGE_TIFF = "image/tiff"
    IMAGE_BMP = "image/bmp"
    IMAGE_GIF = "image/gif"
    IMAGE_WEBP = "image/webp"
    APPLICATION_PDF = "application/pdf"
    ALLOWED_TYPES = [
        IMAGE_JPEG,
        IMAGE_PNG,
        IMAGE_TIFF,
        IMAGE_BMP,
        IMAGE_GIF,
        IMAGE_WEBP,
        APPLICATION_PDF,
    ]


================================================
File: src/unstract/sdk/adapters/ocr/ocr_adapter.py
================================================
from abc import ABC
from typing import Any, Optional

from unstract.sdk.adapters.base import Adapter
from unstract.sdk.adapters.enums import AdapterTypes


class OCRAdapter(Adapter, ABC):
    def __init__(self, name: str):
        super().__init__(name)
        self.name = name

    @staticmethod
    def get_id() -> str:
        return ""

    @staticmethod
    def get_name() -> str:
        return ""

    @staticmethod
    def get_description() -> str:
        return ""

    @staticmethod
    def get_icon() -> str:
        return ""

    @staticmethod
    def get_adapter_type() -> AdapterTypes:
        return AdapterTypes.OCR

    def process(
        self, input_file_path: str, output_file_path: Optional[str] = None
    ) -> str:
        # Overriding methods will contain actual implementation
        return ""

    def test_connection(self, llm_metadata: dict[str, Any]) -> bool:
        return False


================================================
File: src/unstract/sdk/adapters/ocr/register.py
================================================
import logging
import os
from importlib import import_module
from typing import Any

from unstract.sdk.adapters.constants import Common
from unstract.sdk.adapters.ocr.ocr_adapter import OCRAdapter
from unstract.sdk.adapters.registry import AdapterRegistry

logger = logging.getLogger(__name__)


class OCRRegistry(AdapterRegistry):
    @staticmethod
    def register_adapters(adapters: dict[str, Any]) -> None:
        current_directory = os.path.dirname(os.path.abspath(__file__))
        package = "unstract.sdk.adapters.ocr"

        for adapter in os.listdir(current_directory):
            adapter_path = os.path.join(current_directory, adapter, Common.SRC_FOLDER)
            # Check if the item is a directory and not a
            # special directory like __pycache__
            if os.path.isdir(adapter_path) and not adapter.startswith("__"):
                OCRRegistry._build_adapter_list(adapter, package, adapters)
        if len(adapters) == 0:
            logger.warning("No ocr adapter found.")

    @staticmethod
    def _build_adapter_list(
        adapter: str, package: str, adapters: dict[str, Any]
    ) -> None:
        try:
            full_module_path = f"{package}.{adapter}.{Common.SRC_FOLDER}"
            module = import_module(full_module_path)
            metadata = getattr(module, Common.METADATA, {})
            if metadata.get("is_active", False):
                adapter_class: OCRAdapter = metadata[Common.ADAPTER]
                adapter_id = adapter_class.get_id()
                if not adapter_id or (adapter_id in adapters):
                    logger.warning(f"Duplicate Id : {adapter_id}")
                else:
                    adapters[adapter_id] = {
                        Common.MODULE: module,
                        Common.METADATA: metadata,
                    }
        except ModuleNotFoundError as exception:
            logger.warning(f"Unable to import ocr adapters : {exception}")


================================================
File: src/unstract/sdk/adapters/ocr/google_document_ai/README.md
================================================
# Unstract Google Document AI OCR Adapter


================================================
File: src/unstract/sdk/adapters/ocr/google_document_ai/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-googledocumentai-ocr"
version = "0.0.1"
description = "Google Document AI OCR"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [

]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/ocr/google_document_ai/src/README.md
================================================
# Unstract Google Document AI OCR Adapter


================================================
File: src/unstract/sdk/adapters/ocr/google_document_ai/src/__init__.py
================================================
from .google_document_ai import GoogleDocumentAI

metadata = {
    "name": GoogleDocumentAI.__name__,
    "version": "1.0.0",
    "adapter": GoogleDocumentAI,
    "description": "Google Document AI OCR adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/ocr/google_document_ai/src/google_document_ai.py
================================================
import base64
import json
import logging
import os
from typing import Any, Optional

import requests
from filetype import filetype
from google.auth.transport import requests as google_requests
from google.oauth2.service_account import Credentials

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.ocr.constants import FileType
from unstract.sdk.adapters.ocr.ocr_adapter import OCRAdapter
from unstract.sdk.file_storage import FileStorage, FileStorageProvider

logger = logging.getLogger(__name__)


class GoogleDocumentAIKey:
    RAW_DOCUMENT = "rawDocument"
    MIME_TYPE = "mimeType"
    CONTENT = "content"
    SKIP_HUMAN_REVIEW = "skipHumanReview"
    FIELD_MASK = "fieldMask"


class Constants:
    URL = "url"
    CREDENTIALS = "credentials"
    CREDENTIAL_SCOPES = ["https://www.googleapis.com/auth/cloud-platform"]


class GoogleDocumentAI(OCRAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("GoogleDocumentAI")
        self.config = settings
        google_service_account = self.config.get(Constants.CREDENTIALS)
        if not google_service_account:
            logger.error("Google service account not found")
        else:
            self.google_service_account = json.loads(google_service_account)

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "googledocumentai|1013f64b-ecc9-4e35-b986-aebd60fb55d7"

    @staticmethod
    def get_name() -> str:
        return "GoogleDocumentAI"

    @staticmethod
    def get_description() -> str:
        return "Google Document AI OCR"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/GoogleDocumentAI.png"

     

    """ Construct the request body to be sent to Google AI Document server """

    def _get_request_body(
        self, file_type_mime: str, file_content_in_bytes: bytes
    ) -> dict[str, Any]:
        return {
            GoogleDocumentAIKey.RAW_DOCUMENT: {
                GoogleDocumentAIKey.MIME_TYPE: file_type_mime,
                GoogleDocumentAIKey.CONTENT: base64.b64encode(
                    file_content_in_bytes
                ).decode("utf-8"),
            },
            GoogleDocumentAIKey.SKIP_HUMAN_REVIEW: True,
            GoogleDocumentAIKey.FIELD_MASK: "text",
        }

    """ Construct the request headers to be sent
    to Google AI Document server """

    def _get_request_headers(self) -> dict[str, Any]:
        credentials = Credentials.from_service_account_info(
            self.google_service_account, scopes=Constants.CREDENTIAL_SCOPES
        )  # type: ignore
        credentials.refresh(google_requests.Request())  # type: ignore

        return {
            "Content-Type": "application/json; charset=utf-8",
            "Authorization": f"Bearer {credentials.token}",
        }

    """ Detect the mime type from the file content """

    def _get_input_file_type_mime(
        self,
        input_file_path: str,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> str:
        sample_contents = fs.read(path=input_file_path, mode="rb", length=100)
        file_type = filetype.guess(sample_contents)

        file_type_mime: str = file_type.MIME if file_type else FileType.TEXT_PLAIN

        if file_type_mime not in FileType.ALLOWED_TYPES:
            logger.error("Input file type not supported: " f"{file_type_mime}")

        logger.info(f"file: `{input_file_path} [{file_type_mime}]`\n\n")

        return file_type_mime

    def process(
        self,
        input_file_path: str,
        output_file_path: Optional[str] = None,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> str:
        try:
            file_type_mime = self._get_input_file_type_mime(input_file_path)
            if fs.exists(input_file_path):
                file_content_in_bytes = fs.read(path=input_file_path, mode="rb")
            else:
                raise AdapterError(f"File not found {input_file_path}")
            processor_url = self.config.get(Constants.URL, "") + ":process"
            headers = self._get_request_headers()
            data = self._get_request_body(
                file_type_mime=file_type_mime,
                file_content_in_bytes=file_content_in_bytes,
            )
            response = requests.post(processor_url, headers=headers, json=data)
            if response.status_code != 200:
                logger.error(f"Error while calling Google Document AI: {response.text}")
            response_json: dict[str, Any] = response.json()
            result_text: str = response_json["document"]["text"]
            if output_file_path is not None:
                fs.write(path=output_file_path, mode="w", encoding="utf-8")
            return result_text
        except Exception as e:
            logger.error(f"Error while processing document {e}")
            if not isinstance(e, AdapterError):
                raise AdapterError(str(e))
            else:
                raise e

    def test_connection(self) -> bool:
        try:
            url = self.config.get(Constants.URL, "")
            headers = self._get_request_headers()
            response = requests.get(url, headers=headers)
            if response.status_code != 200:
                logger.error(f"Error while testing Google Document AI: {response.text}")
                raise AdapterError(f"{response.status_code} - {response.reason}")
            else:
                return True
        except Exception as e:
            logger.error(f"Error occured while testing adapter {e}")
            if not isinstance(e, AdapterError):
                raise AdapterError(str(e))
            else:
                raise e


================================================
File: src/unstract/sdk/adapters/ocr/google_document_ai/src/static/json_schema.json
================================================
{
  "title": "Google Document AI OCR",
  "type": "object",
  "required": [
    "adapter_name",
    "url",
    "credentials"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: google-document-ai-1"
    },
    "url": {
      "type": "string",
      "title": "URL",
      "default": "",
      "format": "uri",
      "description": "The URL of the Google Document AI endpoint for the processor Example: https://{endpoint}/v1/projects/{project}/locations/{location}/processors/{processor}"
    },
    "credentials": {
      "type": "string",
      "title": "Google Service Account",
      "deafult": "",
      "description": "Service Account in JSON format"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/vectordb/__init__.py
================================================
from unstract.sdk.adapters import AdapterDict
from unstract.sdk.adapters.vectordb.register import VectorDBRegistry


adapters: AdapterDict = {}
VectorDBRegistry.register_adapters(adapters)


================================================
File: src/unstract/sdk/adapters/vectordb/constants.py
================================================
class VectorDbConstants:
    VECTOR_DB_NAME = "collection_name"
    EMBEDDING_DIMENSION = "embedding_dimension"
    DEFAULT_VECTOR_DB_NAME = "unstract"
    DEFAULT_EMBEDDING_SIZE = 2
    WAIT_TIME = "wait_time"


================================================
File: src/unstract/sdk/adapters/vectordb/exceptions.py
================================================
from qdrant_client.http.exceptions import ApiException as QdrantAPIException

from unstract.sdk.adapters.vectordb.qdrant.src import Qdrant
from unstract.sdk.adapters.vectordb.vectordb_adapter import VectorDBAdapter
from unstract.sdk.exceptions import VectorDBError


def parse_vector_db_err(e: Exception, vector_db: VectorDBAdapter) -> VectorDBError:
    """Parses the exception from LLM provider.

    Helps parse the LLM error and wraps it with our
    custom exception object to contain a user friendly message.

    Args:
        e (Exception): Error from LLM provider

    Returns:
        LLMError: Unstract's LLMError object
    """
    # Avoid wrapping VectorDBError objects again
    if isinstance(e, VectorDBError):
        return e

    if isinstance(e, QdrantAPIException):
        err = Qdrant.parse_vector_db_err(e)
    else:
        err = VectorDBError(str(e), actual_err=e)

    msg = f"Error from vector DB '{vector_db.get_name()}'."

    # Add a code block only for errors from clients
    if err.actual_err:
        msg += f"\n```\n{str(err)}\n```"
    else:
        msg += str(err)
    err.message = msg
    return err


================================================
File: src/unstract/sdk/adapters/vectordb/helper.py
================================================
import logging
import os
from typing import Optional

from llama_index.core import (
    MockEmbedding,
    SimpleDirectoryReader,
    StorageContext,
    VectorStoreIndex,
)
from llama_index.core.llms import MockLLM
from llama_index.core.vector_stores.types import BasePydanticVectorStore

from unstract.sdk.adapters.vectordb.constants import VectorDbConstants
from unstract.sdk.exceptions import VectorDBError

logger = logging.getLogger(__name__)


class VectorDBHelper:
    @staticmethod
    def test_vector_db_instance(
        vector_store: Optional[BasePydanticVectorStore],
    ) -> bool:
        try:
            if vector_store is None:
                return False

            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            local_path = f"{os.path.dirname(__file__)}/samples/"
            # Using mock llm and embedding here.
            # For custom embedding args will be:
            #     embed_model - InstructorEmbeddings(embed_batch_size=2)
            #     chunk_size - 512
            #     llm=None
            llm = MockLLM()
            embed_model = MockEmbedding(
                embed_dim=VectorDbConstants.DEFAULT_EMBEDDING_SIZE
            )
            index = VectorStoreIndex.from_documents(
                # By default, SimpleDirectoryReader discards paths which
                # contain one or more parts that are hidden.
                # In local, packages could be installed in a venv. This
                # means a path can contain a ".venv" in it which will
                # then be treated as hidden and subsequently discarded.
                documents=SimpleDirectoryReader(
                    local_path, exclude_hidden=False
                ).load_data(),
                storage_context=storage_context,
                llm=llm,
                embed_model=embed_model,
            )
            query_engine = index.as_query_engine(llm=llm)

            query_engine.query("What did the author learn?")
            return True

        except Exception as e:
            logger.error(f"Error occured while testing adapter {e}")
            raise VectorDBError(message=str(e), actual_err=e)

    @staticmethod
    def get_collection_name(
        collection_name_prefix: str,
        embedding_dimension: int,
    ) -> str:
        """
        Notes:
            This function constructs the collection / table name to store the
            documents in the vector db.
            If user supplies this field in the config metadata then system
            would pick that and append it as prefix to embedding type.
            If this does not come as user setting, then system looks for it
            in the get_vector_db() argument and append it to embedding type
            If it is not there in both places then system appends
            "unstract_vector_db" as prefix to embedding type.
            If embedding type is not passed in get_vector_db() as arg,
            then system ignores appending that
        Args:
            collection_name_prefix (str): the prefix to be added. If this is
                    not passed in, then the default DEFAULT_VECTOR_DB_NAME
                    will be picked up for prefixing
            embedding_dimension (str): this will be suffixed.
                    If this value is not passed in,
                    then only collection_name_prefix will be returned
                Eg. collection_name_prefix -> mock_org
                    embedding_dimension -> 1536
                    return value -> mock_org_unstract_1536

                    collection_name_prefix -> No value
                    embedding_type -> No value
                    return value -> unstract_vector_db

        """
        vector_db_collection_name: str = VectorDbConstants.DEFAULT_VECTOR_DB_NAME
        if embedding_dimension:
            vector_db_collection_name = (
                vector_db_collection_name + "_" + str(embedding_dimension)
            )
        if collection_name_prefix:
            vector_db_collection_name = (
                collection_name_prefix + "_" + vector_db_collection_name
            )
        logger.debug(f"Resolved vectorDB name: {vector_db_collection_name}")
        return vector_db_collection_name


================================================
File: src/unstract/sdk/adapters/vectordb/register.py
================================================
import logging
import os
from importlib import import_module
from typing import Any

from unstract.sdk.adapters.constants import Common
from unstract.sdk.adapters.registry import AdapterRegistry
from unstract.sdk.adapters.vectordb.vectordb_adapter import VectorDBAdapter

logger = logging.getLogger(__name__)


class VectorDBRegistry(AdapterRegistry):
    @staticmethod
    def register_adapters(adapters: dict[str, Any]) -> None:
        current_directory = os.path.dirname(os.path.abspath(__file__))
        package = "unstract.sdk.adapters.vectordb"

        for adapter in os.listdir(current_directory):
            adapter_path = os.path.join(current_directory, adapter, Common.SRC_FOLDER)
            # Check if the item is a directory and not a
            # special directory like __pycache__
            if os.path.isdir(adapter_path) and not adapter.startswith("__"):
                VectorDBRegistry._build_adapter_list(adapter, package, adapters)
        if len(adapters) == 0:
            logger.warning("No vectorDB adapter found.")

    @staticmethod
    def _build_adapter_list(
        adapter: str, package: str, adapters: dict[str, Any]
    ) -> None:
        try:
            full_module_path = f"{package}.{adapter}.{Common.SRC_FOLDER}"
            module = import_module(full_module_path)
            metadata = getattr(module, Common.METADATA, {})
            if metadata.get("is_active", False):
                adapter_class: VectorDBAdapter = metadata[Common.ADAPTER]
                adapter_id = adapter_class.get_id()
                if not adapter_id or (adapter_id in adapters):
                    logger.warning(f"Duplicate Id : {adapter_id}")
                else:
                    adapters[adapter_id] = {
                        Common.MODULE: module,
                        Common.METADATA: metadata,
                    }
        except ModuleNotFoundError as exception:
            logger.warning(f"Unable to import vectorDB adapters : {exception}")


================================================
File: src/unstract/sdk/adapters/vectordb/vectordb_adapter.py
================================================
from abc import ABC
from typing import Any, Union

from llama_index.core.schema import BaseNode
from llama_index.core.vector_stores import SimpleVectorStore
from llama_index.core.vector_stores.types import BasePydanticVectorStore, VectorStore

from unstract.sdk.adapters.base import Adapter
from unstract.sdk.adapters.enums import AdapterTypes
from unstract.sdk.exceptions import VectorDBError


class VectorDBAdapter(Adapter, ABC):
    def __init__(
        self,
        name: str,
        vector_db_instance: Union[VectorStore, BasePydanticVectorStore],
    ):
        super().__init__(name)
        self.name = name
        self._vector_db_instance: Union[
            VectorStore, BasePydanticVectorStore
        ] = vector_db_instance

    @staticmethod
    def get_id() -> str:
        return ""

    @staticmethod
    def get_name() -> str:
        return ""

    @staticmethod
    def get_description() -> str:
        return ""

    @staticmethod
    def get_icon() -> str:
        return ""

    @staticmethod
    def get_adapter_type() -> AdapterTypes:
        return AdapterTypes.VECTOR_DB

    @staticmethod
    def parse_vector_db_err(e: Exception) -> VectorDBError:
        """Parse the error from a vector DB.

        Helps parse errors from a vector DB and wraps with custom exception.

        Args:
            e (Exception): Exception from vector DB

        Returns:
            VectorDBError: Error to be sent to the user
        """
        return VectorDBError(str(e), actual_err=e)

    def get_vector_db_instance(
        self, vector_db_config: dict[str, Any]
    ) -> Union[BasePydanticVectorStore, VectorStore]:
        """Instantiate the llama index VectorStore / BasePydanticVectorStore
        class.

        Returns:
            BasePydanticVectorStore / VectorStore:
                            llama index implementation of the vector store
            Raises exceptions for any error
        """
        return SimpleVectorStore()

    def close(self, **kwargs: Any) -> None:
        """Closes the client connection.

        Returns:
            None
        """
        # Overriding implementations will have the corresponding
        # library methods invoked
        pass

    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
        """Delete the specified docs.

        Returns:
            None
        """
        # Overriding implementations will have the corresponding
        # library methods invoked
        self._vector_db_instance.delete(
            ref_doc_id=ref_doc_id, delete_kwargs=delete_kwargs
        )

    def add(self, ref_doc_id: str, nodes: list[BaseNode]) -> list[str]:
        return self._vector_db_instance.add(nodes=nodes)


================================================
File: src/unstract/sdk/adapters/vectordb/milvus/README.md
================================================
# Unstract Milvus Vector DB

================================================
File: src/unstract/sdk/adapters/vectordb/milvus/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-milvus-vectordb"
version = "0.0.1"
description = "Milvus Vector Database"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/vectordb/milvus/src/__init__.py
================================================
from .milvus import Milvus

metadata = {
    "name": Milvus.__name__,
    "version": "1.0.0",
    "adapter": Milvus,
    "description": "Milvus VectorDB adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/vectordb/milvus/src/milvus.py
================================================
import os
from typing import Any, Optional

from llama_index.core.vector_stores.types import VectorStore
from llama_index.vector_stores.milvus import MilvusVectorStore
from pymilvus import MilvusClient

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.vectordb.constants import VectorDbConstants
from unstract.sdk.adapters.vectordb.helper import VectorDBHelper
from unstract.sdk.adapters.vectordb.vectordb_adapter import VectorDBAdapter


class Constants:
    URI = "uri"
    TOKEN = "token"
    DIM_VALUE = 1536


class Milvus(VectorDBAdapter):
    def __init__(self, settings: dict[str, Any]):
        self._config = settings
        self._client: Optional[MilvusClient] = None
        self._collection_name: str = VectorDbConstants.DEFAULT_VECTOR_DB_NAME
        self._vector_db_instance = self._get_vector_db_instance()
        super().__init__("Milvus", self._vector_db_instance)

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "milvus|3f42f6f9-4b8e-4546-95f3-22ecc9aca442"

    @staticmethod
    def get_name() -> str:
        return "Milvus"

    @staticmethod
    def get_description() -> str:
        return "Milvus VectorDB"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/Milvus.png"

     

    def get_vector_db_instance(self) -> VectorStore:
        return self._vector_db_instance

    def _get_vector_db_instance(self) -> VectorStore:
        try:
            dimension = self._config.get(
                VectorDbConstants.EMBEDDING_DIMENSION,
                VectorDbConstants.DEFAULT_EMBEDDING_SIZE,
            )
            self._collection_name = VectorDBHelper.get_collection_name(
                self._config.get(VectorDbConstants.VECTOR_DB_NAME),
                dimension,
            )
            vector_db: VectorStore = MilvusVectorStore(
                uri=self._config.get(Constants.URI, ""),
                collection_name=self._collection_name,
                token=self._config.get(Constants.TOKEN, ""),
                dim=dimension,
            )
            if vector_db is not None:
                self._client = vector_db.client
            return vector_db
        except Exception as e:
            raise AdapterError(str(e))

    def test_connection(self) -> bool:
        vector_db = self.get_vector_db_instance()
        test_result: bool = VectorDBHelper.test_vector_db_instance(
            vector_store=vector_db
        )
        # Delete the collection that was created for testing
        if self._client is not None:
            self._client.drop_collection(self._collection_name)
        return test_result

    def close(self, **kwargs: Any) -> None:
        if self._client:
            self._client.close()


================================================
File: src/unstract/sdk/adapters/vectordb/milvus/src/static/json_schema.json
================================================
{
  "title": "Milvus Vector DB",
  "type": "object",
  "required": [
    "adapter_name",
    "uri"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: milvus-vdb-1"
    },
    "uri": {
      "type": "string",
      "title": "URI",
      "format": "uri",
      "default": "localhost:19530",
      "description": "Provide the URI of the Milvus server. Example: `https://<instance-id>.api.gcp-us-west1.zillizcloud.com`"
    },
    "token": {
      "type": "string",
      "title": "Token",
      "default": "",
      "format": "password",
      "description": "Provide the token (API Key) of the Milvus server. Leave empty for local server"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/vectordb/no_op/README.md
================================================
# Unstract NoOp vectordb adapter for load testing

A vectodb adapter that does not perform any operation. Waits for the configured time before returning a response. This can be useful to perform tests on the system


================================================
File: src/unstract/sdk/adapters/vectordb/no_op/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-no-op-vectordb"
version = "0.0.1"
description = "NoOp Vector Database"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/vectordb/no_op/src/__init__.py
================================================
from unstract.sdk.adapters.vectordb.no_op.src.no_op_vectordb import NoOpVectorDB

metadata = {
    "name": NoOpVectorDB.__name__,
    "version": "1.0.0",
    "adapter": NoOpVectorDB,
    "description": "NoOpVectorDB",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/vectordb/no_op/src/no_op_custom_vectordb.py
================================================
import time
from typing import Any

from llama_index.core.schema import TextNode
from llama_index.core.vector_stores.types import VectorStore, VectorStoreQueryResult


class NoOpCustomVectorDB(VectorStore):
    stores_text: bool = True
    stores_node: bool = True
    is_embedding_query: bool = True
    wait_time: float = 0

    def __init__(
        self,
        wait_time: float,
        dim: int,
    ) -> None:
        """Initialize params."""
        wait_time = wait_time
        dim = dim

    def query(self, query, **kwargs: Any) -> VectorStoreQueryResult:
        """Query vector store."""
        node1 = TextNode(text="This is a dummy document.", id_="1")
        similarity_scores = [0.9]
        embeddings = ["test"]  # Dummy embeddings for each node

        query_result = VectorStoreQueryResult(
            nodes=[node1], similarities=similarity_scores, ids=embeddings
        )
        time.sleep(self.wait_time)
        return query_result


================================================
File: src/unstract/sdk/adapters/vectordb/no_op/src/no_op_vectordb.py
================================================
import os
import time
from typing import Any

from llama_index.core.schema import BaseNode
from llama_index.core.vector_stores.types import VectorStore

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.vectordb.constants import VectorDbConstants
from unstract.sdk.adapters.vectordb.helper import VectorDBHelper
from unstract.sdk.adapters.vectordb.no_op.src.no_op_custom_vectordb import (
    NoOpCustomVectorDB,
)
from unstract.sdk.adapters.vectordb.vectordb_adapter import VectorDBAdapter


class NoOpVectorDB(VectorDBAdapter):
    def __init__(self, settings: dict[str, Any]):
        self._config = settings
        self._collection_name: str = VectorDbConstants.DEFAULT_VECTOR_DB_NAME
        self._vector_db_instance = self._get_vector_db_instance()
        super().__init__("NoOpVectorDB", self._vector_db_instance)

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "noOpVectorDb|ca4d6056-4971-4bc8-97e3-9e36290b5bc0"

    @staticmethod
    def get_name() -> str:
        return "No Op VectorDB"

    @staticmethod
    def get_description() -> str:
        return "No Op VectorDB"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/noOpVectorDb.png"

     

    def get_vector_db_instance(self) -> VectorStore:
        return self._vector_db_instance

    def _get_vector_db_instance(self) -> VectorStore:
        try:
            dimension = self._config.get(
                VectorDbConstants.EMBEDDING_DIMENSION,
                VectorDbConstants.DEFAULT_EMBEDDING_SIZE,
            )
            self._collection_name = VectorDBHelper.get_collection_name(
                self._config.get(VectorDbConstants.VECTOR_DB_NAME),
                dimension,
            )
            vector_db: VectorStore = NoOpCustomVectorDB(
                dim=dimension, wait_time=self._config.get(VectorDbConstants.WAIT_TIME)
            )
            self._client = vector_db.client
            return vector_db
        except Exception as e:
            raise AdapterError(str(e))

    def test_connection(self) -> bool:
        time.sleep(self._config.get("wait_time"))
        return True

    def close(self, **kwargs: Any) -> None:
        pass

    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
        pass

    def add(self, ref_doc_id: str, nodes: list[BaseNode]) -> list[str]:
        mock_result: list[str] = []
        time.sleep(self._config.get("wait_time"))
        return mock_result


================================================
File: src/unstract/sdk/adapters/vectordb/no_op/src/static/json_schema.json
================================================
{
  "title": "No Op Vector DB",
  "type": "object",
  "required": [
    "adapter_name",
    "wait_time"
  ],
  "description": "No Op Vector DB does not perform any operation, its used to test the performance of the system in the absence of 3rd party induced latencies",
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this No Op adapter instance. Example: no-op-instance-1"
    },
    "wait_time": {
      "type": "number",
      "title": "Wait time (in seconds)",
      "default": "0",
      "description": "Provide the time to wait (in seconds) before returning the response. This will be used for any query or addition of nodes to vectordb"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/vectordb/pinecone/README.md
================================================
# Unstract Pinecone Vector DB

================================================
File: src/unstract/sdk/adapters/vectordb/pinecone/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-pinecone-vectordb"
version = "0.0.1"
description = "Pinecone Vector Database"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/vectordb/pinecone/src/__init__.py
================================================
from .pinecone import Pinecone

metadata = {
    "name": Pinecone.__name__,
    "version": "1.0.0",
    "adapter": Pinecone,
    "description": "Pinecone VectorDB adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/vectordb/pinecone/src/pinecone.py
================================================
import logging
import os
from typing import Any, Optional

from llama_index.core.schema import BaseNode
from llama_index.core.vector_stores.types import BasePydanticVectorStore
from llama_index.vector_stores.pinecone import PineconeVectorStore
from pinecone import NotFoundException
from pinecone import Pinecone as LLamaIndexPinecone
from pinecone import PodSpec, ServerlessSpec

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.vectordb.constants import VectorDbConstants
from unstract.sdk.adapters.vectordb.helper import VectorDBHelper
from unstract.sdk.adapters.vectordb.vectordb_adapter import VectorDBAdapter

logger = logging.getLogger(__name__)


class Constants:
    API_KEY = "api_key"
    ENVIRONMENT = "environment"
    NAMESPACE = "namespace"
    DIMENSION = 1536
    METRIC = "euclidean"
    SPECIFICATION = "spec"
    SPEC_POD = "pod"
    SPEC_SERVERLESS = "serverless"
    CLOUD = "cloud"
    REGION = "region"
    DEFAULT_SPEC_COUNT_VALUE = 1
    DEFAULT_POD_TYPE = "p1.x1"


class Pinecone(VectorDBAdapter):
    def __init__(self, settings: dict[str, Any]):
        self._config = settings
        self._client: Optional[LLamaIndexPinecone] = None
        self._collection_name: str = VectorDbConstants.DEFAULT_VECTOR_DB_NAME
        self._vector_db_instance = self._get_vector_db_instance()
        super().__init__("Pinecone", self._vector_db_instance)

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "pinecone|83881133-485d-4ecc-b1f7-0009f96dc74a"

    @staticmethod
    def get_name() -> str:
        return "Pinecone"

    @staticmethod
    def get_description() -> str:
        return "Pinecone VectorDB"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/pinecone.png"

     

    def get_vector_db_instance(self) -> BasePydanticVectorStore:
        return self._vector_db_instance

    def _get_vector_db_instance(self) -> BasePydanticVectorStore:
        try:
            self._client = LLamaIndexPinecone(
                api_key=str(self._config.get(Constants.API_KEY))
            )
            dimension = self._config.get(
                VectorDbConstants.EMBEDDING_DIMENSION,
                VectorDbConstants.DEFAULT_EMBEDDING_SIZE,
            )
            collection_name = VectorDBHelper.get_collection_name(
                self._config.get(VectorDbConstants.VECTOR_DB_NAME),
                dimension,
            )
            self._collection_name = collection_name.replace("_", "-").lower()

            specification = self._config.get(Constants.SPECIFICATION)
            if specification == Constants.SPEC_POD:
                environment = self._config.get(Constants.ENVIRONMENT)
                spec = PodSpec(
                    environment=environment,
                    replicas=Constants.DEFAULT_SPEC_COUNT_VALUE,
                    shards=Constants.DEFAULT_SPEC_COUNT_VALUE,
                    pods=Constants.DEFAULT_SPEC_COUNT_VALUE,
                    pod_type=Constants.DEFAULT_POD_TYPE,
                )
            elif specification == Constants.SPEC_SERVERLESS:
                cloud = self._config.get(Constants.CLOUD)
                region = self._config.get(Constants.REGION)
                spec = ServerlessSpec(cloud=cloud, region=region)
            logger.info(f"Setting up Pinecone spec for {spec}")
            try:
                self._client.describe_index(name=self._collection_name)
            except NotFoundException:
                logger.info(
                    f"Index:{self._collection_name} does not exist. Creating it."
                )
                self._client.create_index(
                    name=self._collection_name,
                    dimension=dimension,
                    metric=Constants.METRIC,
                    spec=spec,
                )
            self.vector_db: BasePydanticVectorStore = PineconeVectorStore(
                index_name=self._collection_name,
                api_key=str(self._config.get(Constants.API_KEY)),
                environment=str(self._config.get(Constants.ENVIRONMENT)),
            )
            return self.vector_db
        except Exception as e:
            raise AdapterError(str(e))

    def test_connection(self) -> bool:
        vector_db = self.get_vector_db_instance()
        test_result: bool = VectorDBHelper.test_vector_db_instance(
            vector_store=vector_db
        )
        # Delete the collection that was created for testing
        if self._client:
            self._client.delete_index(self._collection_name)
        return test_result

    def close(self, **kwargs: Any) -> None:
        # Close connection is not defined for this client
        pass

    def delete(self, ref_doc_id: str, **delete_kwargs: dict[Any, Any]) -> None:
        specification = self._config.get(Constants.SPECIFICATION)
        if specification == Constants.SPEC_SERVERLESS:
            # To delete all records representing chunks of a single document,
            # first list the record IDs based on their common ID prefix,
            # and then delete the records by ID:
            try:
                index = self._client.Index(self._collection_name)  # type: ignore
                # Get all record having the ref_doc_id and delete them
                for ids in index.list(prefix=ref_doc_id):
                    logger.info(ids)
                    index.delete(ids=ids)
            except Exception as e:
                raise AdapterError(str(e))
        elif specification == Constants.SPEC_POD:
            if self.vector_db.environment == "gcp-starter":  # type: ignore
                raise AdapterError(
                    "Re-indexing is not supported on Starter indexes. "
                    "Use Serverless or paid plan for Pod spec"
                )
            else:
                super().delete(ref_doc_id=ref_doc_id, **delete_kwargs)

    def add(
        self,
        ref_doc_id: str,
        nodes: list[BaseNode],
    ) -> list[str]:
        for i, node in enumerate(nodes):
            node_id = ref_doc_id + "-" + node.node_id
            nodes[i].id_ = node_id
        return self.vector_db.add(nodes=nodes)


================================================
File: src/unstract/sdk/adapters/vectordb/pinecone/src/static/json_schema.json
================================================
{
  "title": "Pinecone Vector DB",
  "type": "object",
  "required": [
    "adapter_name",
    "api_key",
    "spec"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: pinecone-vdb-1"
    },
    "api_key": {
      "type": "string",
      "title": "API Key",
      "format": "password",
      "description": "Provide the API key for your Pinecone account"
    },
    "spec": {
      "type": "string",
      "title": "Specification",
      "enum": [
        "pod",
        "serverless"
      ],
      "default": "serverless",
      "description": "Configurations describing how the index should be deployed"
    }
  },
  "allOf": [
    {
      "if": {
        "properties": {
          "spec": {
            "const": "pod"
          }
        }
      },
      "then": {
        "properties": {
          "environment": {
            "type": "string",
            "title": "Environment",
            "default": "us-west1-gcp",
            "description": "Provide the environment for your Pinecone account. Example us-west1-gcp-free"
          },
          "replicas": {
            "type": "string",
            "title": "Number of replicas",
            "default": "1",
            "description": "The number of replicas to deploy for the pod index.",
            "readOnly": true
          },
          "shards": {
            "type": "string",
            "title": "Number of shards",
            "default": "1",
            "description": "The number of shards to use. Shards are used to expand the amount of vectors you can store beyond the capacity of a single pod.",
            "readOnly": true
          },
          "pods": {
            "type": "string",
            "title": "Number of pods",
            "default": "1",
            "description": "Number of pods to deploy.",
            "readOnly": true
          },
          "pod_type": {
            "type": "string",
            "title": "Pod type",
            "default": "p1.x1",
            "description": "This value combines pod type and pod size into a single string. This configuration is your main lever for vertical scaling.",
            "readOnly": true
          }
        },
        "required": [
          "environment"
        ]
      }
    },
    {
      "if": {
        "properties": {
          "spec": {
            "const": "serverless"
          }
        }
      },
      "then": {
        "properties": {
          "cloud": {
            "type": "string",
            "title": "Cloud",
            "default": "aws",
            "description": "Cloud provider"
          },
          "region": {
            "type": "string",
            "title": "Region",
            "default": "us-west1-gcp",
            "description": "Provide the AWS region for your Pinecone account. Example us-west1-gcp-free"
          }
        },
        "required": [
          "cloud",
          "region"
        ]
      }
    }
  ]
}


================================================
File: src/unstract/sdk/adapters/vectordb/postgres/README.md
================================================
# Unstract Postgres Vector DB

================================================
File: src/unstract/sdk/adapters/vectordb/postgres/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-postgres-vectordb"
version = "0.0.1"
description = "Postgres Vector Database"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/vectordb/postgres/src/__init__.py
================================================
from .postgres import Postgres

metadata = {
    "name": Postgres.__name__,
    "version": "1.0.0",
    "adapter": Postgres,
    "description": "Postgres VectorDB adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/vectordb/postgres/src/postgres.py
================================================
import os
from typing import Any, Optional
from urllib.parse import quote_plus

import psycopg2
from llama_index.core.vector_stores.types import BasePydanticVectorStore
from llama_index.vector_stores.postgres import PGVectorStore
from psycopg2._psycopg import connection

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.vectordb.constants import VectorDbConstants
from unstract.sdk.adapters.vectordb.helper import VectorDBHelper
from unstract.sdk.adapters.vectordb.vectordb_adapter import VectorDBAdapter


class Constants:
    DATABASE = "database"
    HOST = "host"
    PASSWORD = "password"
    PORT = "port"
    USER = "user"
    SCHEMA = "schema"
    ENABLE_SSL = "enable_ssl"


class Postgres(VectorDBAdapter):
    def __init__(self, settings: dict[str, Any]):
        self._config = settings
        self._client: Optional[connection] = None
        self._collection_name: str = VectorDbConstants.DEFAULT_VECTOR_DB_NAME
        self._schema_name: str = VectorDbConstants.DEFAULT_VECTOR_DB_NAME
        self._vector_db_instance = self._get_vector_db_instance()
        super().__init__("Postgres", self._vector_db_instance)

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "postgres|70ab6cc2-e86a-4e5a-896f-498a95022d34"

    @staticmethod
    def get_name() -> str:
        return "Postgres"

    @staticmethod
    def get_description() -> str:
        return "Postgres VectorDB"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/postgres.png"

     

    def get_vector_db_instance(self) -> BasePydanticVectorStore:
        return self._vector_db_instance

    def _get_vector_db_instance(self) -> BasePydanticVectorStore:
        try:
            encoded_password = quote_plus(str(self._config.get(Constants.PASSWORD)))
            dimension = self._config.get(
                VectorDbConstants.EMBEDDING_DIMENSION,
                VectorDbConstants.DEFAULT_EMBEDDING_SIZE,
            )
            self._collection_name = VectorDBHelper.get_collection_name(
                self._config.get(VectorDbConstants.VECTOR_DB_NAME),
                dimension,
            )
            self._schema_name = self._config.get(
                Constants.SCHEMA,
                VectorDbConstants.DEFAULT_VECTOR_DB_NAME,
            )
            vector_db: BasePydanticVectorStore = PGVectorStore.from_params(
                database=self._config.get(Constants.DATABASE),
                schema_name=self._schema_name,
                host=self._config.get(Constants.HOST),
                password=encoded_password,
                port=str(self._config.get(Constants.PORT)),
                user=self._config.get(Constants.USER),
                table_name=self._collection_name,
                embed_dim=dimension,
            )
            if self._config.get(Constants.ENABLE_SSL, True):
                ssl_mode = "require"
            else:
                ssl_mode = "disable"
            self._client = psycopg2.connect(
                database=self._config.get(Constants.DATABASE),
                host=self._config.get(Constants.HOST),
                user=self._config.get(Constants.USER),
                password=self._config.get(Constants.PASSWORD),
                port=str(self._config.get(Constants.PORT)),
                sslmode=ssl_mode,
            )

            return vector_db
        except Exception as e:
            raise AdapterError(str(e))

    def test_connection(self) -> bool:
        vector_db = self.get_vector_db_instance()
        test_result: bool = VectorDBHelper.test_vector_db_instance(
            vector_store=vector_db
        )

        # Delete the collection that was created for testing
        if self._client is not None:
            self._client.cursor().execute(
                f"DROP TABLE IF EXISTS "
                f"{self._schema_name}.data_{self._collection_name} CASCADE"
            )
            self._client.commit()

        return test_result

    def close(self, **kwargs: Any) -> None:
        if self._client:
            self._client.close()


================================================
File: src/unstract/sdk/adapters/vectordb/postgres/src/static/json_schema.json
================================================
{
  "title": "Postgres Vector DB",
  "type": "object",
  "required": [
    "adapter_name",
    "database",
    "host",
    "port",
    "user",
    "password"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: pg-vdb-1"
    },
    "database": {
      "type": "string",
      "title": "Database",
      "description": "Provide a name for the database"
    },
    "host": {
      "type": "string",
      "title": "Host",
      "description": "Hostname"
    },
    "port": {
      "type": "integer",
      "default": 5432,
      "title": "Port"
    },
    "user": {
      "type": "string",
      "title": "User",
      "description": "Username"
    },
    "password": {
      "type": "string",
      "title": "Password",
      "format": "password",
      "default": ""
    },
    "enable_ssl": {
      "type": "boolean",
      "title": "Enable SSL",
      "description": "On selecting the checkbox, data encryption using SSL is enabled",
      "default": true
    }
  }
}


================================================
File: src/unstract/sdk/adapters/vectordb/qdrant/README.md
================================================
# Unstract Qdrant Vector DB

================================================
File: src/unstract/sdk/adapters/vectordb/qdrant/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-qdrant-vectordb"
version = "0.0.1"
description = "Qdrant Vector Database"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/vectordb/qdrant/src/__init__.py
================================================
from .qdrant import Qdrant

metadata = {
    "name": Qdrant.__name__,
    "version": "1.0.0",
    "adapter": Qdrant,
    "description": "Qdrant VectorDB adapter",
    "is_active": True,
}

__all__ = ["Qdrant"]


================================================
File: src/unstract/sdk/adapters/vectordb/qdrant/src/qdrant.py
================================================
import logging
import os
from typing import Any, Optional

from llama_index.core.vector_stores.types import BasePydanticVectorStore
from llama_index.vector_stores.qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.http.exceptions import UnexpectedResponse

from unstract.sdk.adapters.vectordb.constants import VectorDbConstants
from unstract.sdk.adapters.vectordb.helper import VectorDBHelper
from unstract.sdk.adapters.vectordb.vectordb_adapter import VectorDBAdapter
from unstract.sdk.exceptions import VectorDBError

logger = logging.getLogger(__name__)


class Constants:
    URL = "url"
    API_KEY = "api_key"


class Qdrant(VectorDBAdapter):
    def __init__(self, settings: dict[str, Any]):
        self._config = settings
        self._client: Optional[QdrantClient] = None
        self._collection_name: str = VectorDbConstants.DEFAULT_VECTOR_DB_NAME
        self._vector_db_instance = self._get_vector_db_instance()
        super().__init__("Qdrant", self._vector_db_instance)

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "qdrant|41f64fda-2e4c-4365-89fd-9ce91bee74d0"

    @staticmethod
    def get_name() -> str:
        return "Qdrant"

    @staticmethod
    def get_description() -> str:
        return "Qdrant LLM"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/qdrant.png"

     

    def get_vector_db_instance(self) -> BasePydanticVectorStore:
        return self._vector_db_instance

    def _get_vector_db_instance(self) -> BasePydanticVectorStore:
        try:
            self._collection_name = VectorDBHelper.get_collection_name(
                self._config.get(VectorDbConstants.VECTOR_DB_NAME),
                self._config.get(VectorDbConstants.EMBEDDING_DIMENSION),
            )
            url = self._config.get(Constants.URL)
            api_key: Optional[str] = self._config.get(Constants.API_KEY, None)
            if api_key:
                self._client = QdrantClient(url=url, api_key=api_key)
            else:
                self._client = QdrantClient(url=url)
            vector_db: BasePydanticVectorStore = QdrantVectorStore(
                collection_name=self._collection_name,
                client=self._client,
                url=url,
                api_key=api_key,
            )
            return vector_db
        except Exception as e:
            raise self.parse_vector_db_err(e) from e

    def test_connection(self) -> bool:
        try:
            vector_db = self.get_vector_db_instance()
            test_result: bool = VectorDBHelper.test_vector_db_instance(
                vector_store=vector_db
            )
            # Delete the collection that was created for testing
            if self._client is not None:
                self._client.delete_collection(self._collection_name)
            return test_result
        except Exception as e:
            raise self.parse_vector_db_err(e) from e

    def close(self, **kwargs: Any) -> None:
        if self._client:
            self._client.close(**kwargs)

    @staticmethod
    def parse_vector_db_err(e: Exception) -> VectorDBError:
        # Avoid wrapping VectorDBError objects again
        if isinstance(e, VectorDBError):
            return e

        if isinstance(e, UnexpectedResponse):
            msg = str(e)
            if e.reason_phrase == "Not Found":
                msg = "Unable to connect to Qdrant, please check vector DB settings."
            elif e.reason_phrase == "Forbidden":
                msg = "Unable to access Qdrant, please check the API key provided."
            return VectorDBError(message=msg, actual_err=e)
        else:
            status_code = None
            if "client has been closed" in str(e):
                status_code = 503
            elif "timeout" in str(e):
                status_code = 504
            return VectorDBError(message=str(e), actual_err=e, status_code=status_code)


================================================
File: src/unstract/sdk/adapters/vectordb/qdrant/src/static/json_schema.json
================================================
{
  "title": "Qdrant Vector DB",
  "type": "object",
  "required": [
    "adapter_name",
    "url"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: qdrant-vdb-1"
    },
    "url": {
      "type": "string",
      "title": "URL",
      "pattern": "(http|https)?[\\w\\d\\-\\.:]+"
    },
    "api_key": {
      "type": "string",
      "title": "API Key",
      "default": "",
      "format": "password"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/vectordb/supabase/README.md
================================================
# Unstract Supabase Vector DB


================================================
File: src/unstract/sdk/adapters/vectordb/supabase/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-supabase-vectordb"
version = "0.0.1"
description = "Supabase Vector Database"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/vectordb/supabase/src/__init__.py
================================================
from .supabase import Supabase

metadata = {
    "name": Supabase.__name__,
    "version": "1.0.0",
    "adapter": Supabase,
    "description": "Supabase VectorDB adapter",
    "is_active": False,
}


================================================
File: src/unstract/sdk/adapters/vectordb/supabase/src/supabase.py
================================================
import os
from typing import Any, Optional
from urllib.parse import quote_plus

from llama_index.core.vector_stores.types import VectorStore
from llama_index.vector_stores.supabase import SupabaseVectorStore
from vecs import Client

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.vectordb.constants import VectorDbConstants
from unstract.sdk.adapters.vectordb.helper import VectorDBHelper
from unstract.sdk.adapters.vectordb.vectordb_adapter import VectorDBAdapter


class Constants:
    DATABASE = "database"
    HOST = "host"
    PASSWORD = "password"
    PORT = "port"
    USER = "user"
    COLLECTION_NAME = "base_demo"


class Supabase(VectorDBAdapter):
    def __init__(self, settings: dict[str, Any]):
        self._config = settings
        self._client: Optional[Client] = None
        self._collection_name: str = VectorDbConstants.DEFAULT_VECTOR_DB_NAME
        self._vector_db_instance = self._get_vector_db_instance()
        super().__init__("Supabase", self._vector_db_instance)

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "supabase|e6998e3c-3595-48c0-a190-188dbd803858"

    @staticmethod
    def get_name() -> str:
        return "Supabase"

    @staticmethod
    def get_description() -> str:
        return "Supabase VectorDB"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/supabase.png"

     

    def get_vector_db_instance(self) -> VectorStore:
        return self._vector_db_instance

    def _get_vector_db_instance(self) -> VectorStore:
        try:
            dimension = self._config.get(
                VectorDbConstants.EMBEDDING_DIMENSION,
                VectorDbConstants.DEFAULT_EMBEDDING_SIZE,
            )
            self._collection_name = VectorDBHelper.get_collection_name(
                self._config.get(VectorDbConstants.VECTOR_DB_NAME),
                self._config.get(
                    VectorDbConstants.EMBEDDING_DIMENSION,
                    dimension,
                ),
            )
            user = str(self._config.get(Constants.USER))
            password = str(self._config.get(Constants.PASSWORD))
            encoded_password = quote_plus(str(password))
            host = str(self._config.get(Constants.HOST))
            port = str(self._config.get(Constants.PORT))
            db_name = str(self._config.get(Constants.DATABASE))

            postgres_connection_string = (
                f"postgresql://{user}:{encoded_password}@{host}:{port}/{db_name}"
            )
            vector_db: VectorStore = SupabaseVectorStore(
                postgres_connection_string=postgres_connection_string,
                collection_name=self._collection_name,
                dimension=dimension,
            )
            if vector_db is not None:
                self._client = vector_db.client
            return vector_db
        except Exception as e:
            raise AdapterError(str(e))

    def test_connection(self) -> bool:
        vector_db = self.get_vector_db_instance()
        test_result: bool = VectorDBHelper.test_vector_db_instance(
            vector_store=vector_db
        )
        # Delete the collection that was created for testing
        if self._client is not None:
            self._client.delete_collection(self._collection_name)
        return test_result

    def close(self, **kwargs: Any) -> None:
        if self._client:
            self._client.close()


================================================
File: src/unstract/sdk/adapters/vectordb/supabase/src/static/json_schema.json
================================================
{
  "title": "Supabase Vector DB",
  "type": "object",
  "required": [
    "adapter_name",
    "database",
    "host",
    "port",
    "user",
    "password"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: supabase-vdb-1"
    },
    "database": {
      "type": "string",
      "title": "Database",
      "description": "Provide a name for the database"
    },
    "host": {
      "type": "string",
      "title": "Host",
      "default": "",
      "description": "Example: db.<instance-id>.supabase.co"
    },
    "port": {
      "type": "integer",
      "title": "Port",
      "default": 5432
    },
    "user": {
      "type": "string",
      "title": "User"
    },
    "password": {
      "type": "string",
      "title": "Password",
      "format": "password"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/vectordb/weaviate/README.md
================================================
# Unstract Weaviate Vector DB

================================================
File: src/unstract/sdk/adapters/vectordb/weaviate/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-weaviate-vectordb"
version = "0.0.1"
description = "Weaviate Vector Database"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/vectordb/weaviate/src/__init__.py
================================================
from .weaviate import Weaviate

metadata = {
    "name": Weaviate.__name__,
    "version": "1.0.0",
    "adapter": Weaviate,
    "description": "Weaviate VectorDB adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/vectordb/weaviate/src/weaviate.py
================================================
import logging
import os
from typing import Any, Optional

import weaviate
from llama_index.core.vector_stores.types import BasePydanticVectorStore
from llama_index.vector_stores.weaviate import WeaviateVectorStore
from weaviate.classes.init import Auth
from weaviate.exceptions import UnexpectedStatusCodeException

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.vectordb.constants import VectorDbConstants
from unstract.sdk.adapters.vectordb.helper import VectorDBHelper
from unstract.sdk.adapters.vectordb.vectordb_adapter import VectorDBAdapter

logger = logging.getLogger(__name__)


class Constants:
    URL = "url"
    API_KEY = "api_key"


class Weaviate(VectorDBAdapter):
    def __init__(self, settings: dict[str, Any]):
        self._config = settings
        self._client: Optional[weaviate.Client] = None
        self._collection_name: str = VectorDbConstants.DEFAULT_VECTOR_DB_NAME
        self._vector_db_instance = self._get_vector_db_instance()
        super().__init__("Weaviate", self._vector_db_instance)

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "weaviate|294e08df-4e4a-40f2-8f0d-9e4940180ccc"

    @staticmethod
    def get_name() -> str:
        return "Weaviate"

    @staticmethod
    def get_description() -> str:
        return "Weaviate VectorDB"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/Weaviate.png"

     

    def get_vector_db_instance(self) -> BasePydanticVectorStore:
        return self._vector_db_instance

    def _get_vector_db_instance(self) -> BasePydanticVectorStore:
        try:
            collection_name = VectorDBHelper.get_collection_name(
                self._config.get(VectorDbConstants.VECTOR_DB_NAME),
                self._config.get(VectorDbConstants.EMBEDDING_DIMENSION),
            )
            # Capitalise the frst letter as Weaviate expects this
            # LLama-index throws the error if not capitalised while using
            # Weaviate
            self._collection_name = collection_name.capitalize()
            self._client = weaviate.connect_to_weaviate_cloud(
                cluster_url=str(self._config.get(Constants.URL)),
                auth_credentials=Auth.api_key(str(self._config.get(Constants.API_KEY))),
            )

            try:
                # Class definition object. Weaviate's autoschema
                # feature will infer properties when importing.
                class_obj = {
                    "class": self._collection_name,
                    "vectorizer": "none",
                }
                # Create the colletion
                self._client.collections.create_from_dict(class_obj)
            except Exception as e:
                if isinstance(e, UnexpectedStatusCodeException):
                    if "already exists" in e.message:
                        logger.warning(f"Collection already exists: {e}")
                else:
                    raise e
            vector_db: BasePydanticVectorStore = WeaviateVectorStore(
                weaviate_client=self._client,
                index_name=self._collection_name,
            )
            return vector_db
        except Exception as e:
            raise AdapterError(str(e))

    def test_connection(self) -> bool:
        vector_db = self.get_vector_db_instance()
        test_result: bool = VectorDBHelper.test_vector_db_instance(
            vector_store=vector_db
        )
        # Delete the collection that was created for testing
        if self._client is not None:
            self._client.collections.delete(self._collection_name)
        return test_result

    def close(self, **kwargs: Any) -> None:
        if self._client:
            self._client.close(**kwargs)


================================================
File: src/unstract/sdk/adapters/vectordb/weaviate/src/static/json_schema.json
================================================
{
  "title": "Weaviate Vector DB",
  "type": "object",
  "required": [
    "adapter_name",
    "url"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: weaviate-vdb-1"
    },
    "url": {
      "type": "string",
      "title": "URL",
      "default": "",
      "format": "uri",
      "description": "The URL of the Vector DB instance. Example: https://<your-instance>.weaviate.network"
    },
    "api_key": {
      "type": "string",
      "title": "Api Key",
      "deafult": "",
      "format": "password"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/x2text/__init__.py
================================================
from unstract.sdk.adapters import AdapterDict
from unstract.sdk.adapters.x2text.register import X2TextRegistry

adapters: AdapterDict = {}
X2TextRegistry.register_adapters(adapters)


================================================
File: src/unstract/sdk/adapters/x2text/constants.py
================================================
class X2TextConstants:
    PLATFORM_SERVICE_API_KEY = "PLATFORM_SERVICE_API_KEY"
    X2TEXT_HOST = "X2TEXT_HOST"
    X2TEXT_PORT = "X2TEXT_PORT"
    ENABLE_HIGHLIGHT = "enable_highlight"
    TAGS = "tags"
    EXTRACTED_TEXT = "extracted_text"
    WHISPER_HASH = "whisper-hash"
    WHISPER_HASH_V2 = "whisper_hash"


================================================
File: src/unstract/sdk/adapters/x2text/dto.py
================================================
from dataclasses import dataclass
from typing import Optional


@dataclass
class TextExtractionMetadata:
    whisper_hash: str


@dataclass
class TextExtractionResult:
    extracted_text: str
    extraction_metadata: Optional[TextExtractionMetadata] = None


================================================
File: src/unstract/sdk/adapters/x2text/helper.py
================================================
import logging
from typing import Any, Optional

import requests
from requests import Response
from requests.exceptions import ConnectionError, HTTPError, Timeout

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.utils import AdapterUtils
from unstract.sdk.adapters.x2text.constants import X2TextConstants
from unstract.sdk.constants import MimeType
from unstract.sdk.file_storage import FileStorage, FileStorageProvider

logger = logging.getLogger(__name__)


class X2TextHelper:
    """Helpers meant for x2text adapters."""

    @staticmethod
    def parse_response(
        response: Response,
        out_file_path: Optional[str] = None,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> tuple[str, bool]:
        """Parses the response from a request.

        Optionally it can write the output to a file

        Args:
            response (Response): Response to parse
            out_file_path (Optional[str], optional): Output file path to write
                 to, skipped if None or emtpy. Defaults to None.
            fs (FileStorage): file storage object to perfrom file operations

        Returns:
            tuple[str, bool]: Response's content and status of parsing
        """
        if not response.ok and not response.content:
            return "", False
        if isinstance(response.content, bytes):
            output = response.content.decode("utf-8")
        if out_file_path:
            fs.write(path=out_file_path, mode="w", encoding="utf-8", data=output)
        return output, True


class UnstructuredHelper:
    """Helpers meant for unstructured-community and unstructured-enterprise."""

    URL = "url"
    API_KEY = "api_key"
    TEST_CONNECTION = "test-connection"
    PROCESS = "process"

    @staticmethod
    def test_server_connection(unstructured_adapter_config: dict[str, Any]) -> bool:
        UnstructuredHelper.make_request(
            unstructured_adapter_config, UnstructuredHelper.TEST_CONNECTION
        )
        return True

    @staticmethod
    def process_document(
        unstructured_adapter_config: dict[str, Any],
        input_file_path: str,
        output_file_path: Optional[str] = None,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> str:
        try:
            response: Response
            local_storage = FileStorage(FileStorageProvider.LOCAL)
            if not local_storage.exists(input_file_path):
                fs.download(from_path=input_file_path, to_path=input_file_path)
            with open(input_file_path, "rb") as input_f:
                mime_type = local_storage.mime_type(path=input_file_path)
                files = {"file": (input_file_path, input_f, mime_type)}
                response = UnstructuredHelper.make_request(
                    unstructured_adapter_config=unstructured_adapter_config,
                    request_type=UnstructuredHelper.PROCESS,
                    files=files,
                )
            output, is_success = X2TextHelper.parse_response(
                response=response, out_file_path=output_file_path, fs=fs
            )
            if not is_success:
                raise AdapterError("Couldn't extract text from file")
            return output
        except OSError as e:
            msg = f"OS error while reading {input_file_path} "
            if output_file_path:
                msg += f"and writing {output_file_path}"
            msg += f": {str(e)}"
            logger.error(msg)
            raise AdapterError(str(e))

    @staticmethod
    def make_request(
        unstructured_adapter_config: dict[str, Any],
        request_type: str,
        **kwargs: dict[Any, Any],
    ) -> Response:
        unstructured_url = unstructured_adapter_config.get(UnstructuredHelper.URL)

        x2text_service_url = unstructured_adapter_config.get(
            X2TextConstants.X2TEXT_HOST
        )
        x2text_service_port = unstructured_adapter_config.get(
            X2TextConstants.X2TEXT_PORT
        )
        platform_service_api_key = unstructured_adapter_config.get(
            X2TextConstants.PLATFORM_SERVICE_API_KEY
        )
        headers = {
            "accept": MimeType.JSON,
            "Authorization": f"Bearer {platform_service_api_key}",
        }
        body = {
            "unstructured-url": unstructured_url,
        }
        # Add api key only if present
        api_key = unstructured_adapter_config.get(UnstructuredHelper.API_KEY)
        if api_key:
            body["unstructured-api-key"] = api_key

        x2text_url = (
            f"{x2text_service_url}:{x2text_service_port}"
            f"/api/v1/x2text/{request_type}"
        )
        # Add files only if the request is for process
        files = None
        if "files" in kwargs:
            files = kwargs["files"] if kwargs["files"] is not None else None
        try:
            response = requests.post(
                x2text_url, headers=headers, data=body, files=files
            )
            response.raise_for_status()
        except ConnectionError as e:
            logger.error(f"Adapter error: {e}")
            raise AdapterError(
                "Unable to connect to unstructured-io's service, "
                "please check the URL"
            )
        except Timeout as e:
            msg = "Request to unstructured-io's service has timed out"
            logger.error(f"{msg}: {e}")
            raise AdapterError(msg)
        except HTTPError as e:
            logger.error(f"Adapter error: {e}")
            default_err = "Error while calling the unstructured-io service"
            msg = AdapterUtils.get_msg_from_request_exc(
                err=e, message_key="detail", default_err=default_err
            )
            raise AdapterError("unstructured-io: " + msg)
        return response


================================================
File: src/unstract/sdk/adapters/x2text/register.py
================================================
import logging
import os
from importlib import import_module
from typing import Any

from unstract.sdk.adapters.constants import Common
from unstract.sdk.adapters.registry import AdapterRegistry
from unstract.sdk.adapters.x2text.x2text_adapter import X2TextAdapter

logger = logging.getLogger(__name__)


class X2TextRegistry(AdapterRegistry):
    @staticmethod
    def register_adapters(adapters: dict[str, Any]) -> None:
        current_directory = os.path.dirname(os.path.abspath(__file__))
        package = "unstract.sdk.adapters.x2text"

        for adapter in os.listdir(current_directory):
            adapter_path = os.path.join(current_directory, adapter, Common.SRC_FOLDER)
            # Check if the item is a directory and not a
            # special directory like __pycache__
            if os.path.isdir(adapter_path) and not adapter.startswith("__"):
                X2TextRegistry._build_adapter_list(adapter, package, adapters)
        if len(adapters) == 0:
            logger.warning("No X2Text adapter found.")

    @staticmethod
    def _build_adapter_list(
        adapter: str, package: str, adapters: dict[str, Any]
    ) -> None:
        try:
            full_module_path = f"{package}.{adapter}.{Common.SRC_FOLDER}"
            module = import_module(full_module_path)
            metadata = getattr(module, Common.METADATA, {})
            if metadata.get("is_active", False):
                adapter_class: X2TextAdapter = metadata[Common.ADAPTER]
                adapter_id = adapter_class.get_id()
                if not adapter_id or (adapter_id in adapters):
                    logger.warning(f"Duplicate Id : {adapter_id}")
                else:
                    adapters[adapter_id] = {
                        Common.MODULE: module,
                        Common.METADATA: metadata,
                    }
        except ModuleNotFoundError as exception:
            logger.warning(f"Unable to import X2Text adapters : {exception}")


================================================
File: src/unstract/sdk/adapters/x2text/x2text_adapter.py
================================================
from abc import ABC
from typing import Any, Optional

from unstract.sdk.adapters.base import Adapter
from unstract.sdk.adapters.enums import AdapterTypes
from unstract.sdk.adapters.x2text.dto import TextExtractionResult
from unstract.sdk.file_storage import FileStorage, FileStorageProvider


class X2TextAdapter(Adapter, ABC):
    def __init__(self, name: str):
        super().__init__(name)
        self.name = name

    @staticmethod
    def get_id() -> str:
        return ""

    @staticmethod
    def get_name() -> str:
        return ""

    @staticmethod
    def get_description() -> str:
        return ""

    @staticmethod
    def get_icon() -> str:
        return ""

    @staticmethod
    def get_adapter_type() -> AdapterTypes:
        return AdapterTypes.X2TEXT

    def test_connection(self) -> bool:
        return False

    def process(
        self,
        input_file_path: str,
        output_file_path: Optional[str] = None,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
        **kwargs: dict[Any, Any],
    ) -> TextExtractionResult:
        return TextExtractionResult(
            extracted_text="extracted text", extraction_metadata=None
        )


================================================
File: src/unstract/sdk/adapters/x2text/llama_parse/README.md
================================================
# Unstract Llama Parse X2Text Adapter

================================================
File: src/unstract/sdk/adapters/x2text/llama_parse/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-llama-parse-x2text"
version = "0.0.1"
description = "Llama Parse X2Text Adapter"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/x2text/llama_parse/src/__init__.py
================================================
from .llama_parse import LlamaParseAdapter

metadata = {
    "name": LlamaParseAdapter.__name__,
    "version": "1.0.0",
    "adapter": LlamaParseAdapter,
    "description": "LlamaParse X2Text adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/x2text/llama_parse/src/constants.py
================================================
class LlamaParseConfig:
    """Dictionary keys used to process LlamaParse."""

    API_KEY = "api_key"
    BASE_URL = "base_url"
    RESULT_TYPE = "result_type"
    NUM_WORKERS = "num_workers"
    VERBOSE = "verbose"
    LANGUAGE = "language"


================================================
File: src/unstract/sdk/adapters/x2text/llama_parse/src/llama_parse.py
================================================
import logging
import os
import pathlib
from typing import Any, Optional

from httpx import ConnectError
from llama_parse import LlamaParse

from unstract.sdk.adapters.exceptions import AdapterError
from unstract.sdk.adapters.x2text.dto import TextExtractionResult
from unstract.sdk.adapters.x2text.llama_parse.src.constants import LlamaParseConfig
from unstract.sdk.adapters.x2text.x2text_adapter import X2TextAdapter
from unstract.sdk.file_storage import FileStorage, FileStorageProvider

logger = logging.getLogger(__name__)


class LlamaParseAdapter(X2TextAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("LlamaParse")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "llamaparse|78860239-b3cc-4cc5-b3de-f84315f75d14"

    @staticmethod
    def get_name() -> str:
        return "LlamaParse"

    @staticmethod
    def get_description() -> str:
        return "LlamaParse X2Text"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/llama-parse.png"

     

    def _call_parser(
        self,
        input_file_path: str,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> str:
        parser = LlamaParse(
            api_key=self.config.get(LlamaParseConfig.API_KEY),
            base_url=self.config.get(LlamaParseConfig.BASE_URL),
            result_type=self.config.get(LlamaParseConfig.RESULT_TYPE),
            verbose=self.config.get(LlamaParseConfig.VERBOSE),
            language="en",
            ignore_errors=False,
        )

        try:
            file_extension = pathlib.Path(input_file_path).suffix
            if not file_extension:
                try:
                    input_file_extension = fs.guess_extension(input_file_path)
                    input_file_path_copy = input_file_path
                    input_file_path = ".".join(
                        (input_file_path_copy, input_file_extension)
                    )
                    text_content = fs.read(
                        path=input_file_path_copy, mode="rb", encoding="utf-8"
                    )
                    fs.write(
                        path=input_file_path,
                        data=text_content,
                        mode="wb",
                        encoding="utf-8",
                    )
                except OSError as os_err:
                    logger.error("Exception raised while handling input file.")
                    raise AdapterError(str(os_err))

            file_bytes = fs.read(path=input_file_path, mode="rb")
            documents = parser.load_data(
                file_bytes, extra_info={"file_name": input_file_path}
            )

        except ConnectError as connec_err:
            logger.error(f"Invalid Base URL given. : {connec_err}")
            raise AdapterError(
                "Unable to connect to llama-parse`s service, "
                "please check the Base URL"
            )
        except Exception as exe:
            logger.error(
                "Seems like an invalid API Key or possible internal errors: {exe}"
            )
            raise AdapterError(str(exe))

        response_text = documents[0].text
        return response_text  # type:ignore

    def process(
        self,
        input_file_path: str,
        output_file_path: Optional[str] = None,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
        **kwargs: dict[Any, Any],
    ) -> TextExtractionResult:
        response_text = self._call_parser(input_file_path=input_file_path, fs=fs)
        if output_file_path:
            fs.write(
                path=output_file_path,
                mode="w",
                encoding="utf-8",
                data=response_text,
            )

        return TextExtractionResult(extracted_text=response_text)

    def test_connection(self) -> bool:
        self._call_parser(
            input_file_path=f"{os.path.dirname(__file__)}/static/test_input.doc"
        )
        return True


================================================
File: src/unstract/sdk/adapters/x2text/llama_parse/src/static/json_schema.json
================================================
{
    "title": "Llama Parse Text Extractor",
    "type": "object",
    "required": [
      "api_key"
    ],
    "properties": {
      "adapter_name": {
        "type": "string",
        "title": "Name",
        "default": "",
        "description": "Provide a unique name for this adapter instance. Example: llama-parse-1"
      },
        "api_key": {
            "type": "string",
            "title": "API Key",
            "format": "password",
            "default": "",
            "description": "Provide the token (API Key) of the Llama Parse server"
        },
      "url": {
        "type": "string",
        "title": "URL",
        "format": "url",
        "default": "https://api.cloud.llamaindex.ai",
        "description": "Provide the Base URL of llama Parse server."
      },
      "result_type": {
        "type": "string",
        "title": "Result Type",
        "enum": [
          "text",
          "markdown"
        ],
        "default": "text",
        "description": "Choose the type of result - `markdown` or `text`."
      },
      "verbose": {
        "type": "boolean",
        "title": "Verbose",
        "default": true,
        "description": "If set, verbose result is included."
      }
    }
  }


================================================
File: src/unstract/sdk/adapters/x2text/llama_parse/src/static/test_input.doc
================================================
"Hello Unstract!"

================================================
File: src/unstract/sdk/adapters/x2text/llm_whisperer/README.md
================================================
# Unstract LLMWhisperer X2Text Adapter

## Env variables

The below env variables are resolved by LLMWhisperer adapter

| Variable                     | Description                                                                                  |
| ---------------------------- | -------------------------------------------------------------------------------------------- |
| `ADAPTER_LLMW_POLL_INTERVAL` | Time in seconds to wait before polling LLMWhisperer's status API. Defaults to 30s            |
| `ADAPTER_LLMW_MAX_POLLS`     | Total number of times to poll the status API. Defaults to 30                                 |


================================================
File: src/unstract/sdk/adapters/x2text/llm_whisperer/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-llm_whisperer-x2text"
version = "0.0.1"
description = "LLMWhisperer X2Text Adapter"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/x2text/llm_whisperer/src/__init__.py
================================================
from .llm_whisperer import LLMWhisperer

metadata = {
    "name": LLMWhisperer.__name__,
    "version": "1.0.0",
    "adapter": LLMWhisperer,
    "description": "LLMWhisperer X2Text adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/x2text/llm_whisperer/src/constants.py
================================================
import os
from enum import Enum


class ProcessingModes(Enum):
    OCR = "ocr"
    TEXT = "text"


class Modes(Enum):
    NATIVE_TEXT = "native_text"
    LOW_COST = "low_cost"
    HIGH_QUALITY = "high_quality"
    FORM = "form"


class OutputModes(Enum):
    LINE_PRINTER = "line-printer"
    DUMP_TEXT = "dump-text"
    TEXT = "text"


class HTTPMethod(Enum):
    GET = "GET"
    POST = "POST"


class WhispererHeader:
    UNSTRACT_KEY = "unstract-key"


class WhispererEndpoint:
    """Endpoints available at LLMWhisperer service."""

    TEST_CONNECTION = "test-connection"
    WHISPER = "whisper"
    STATUS = "whisper-status"
    RETRIEVE = "whisper-retrieve"


class WhispererEnv:
    """Env variables for LLMWhisperer.

    Can be used to alter behaviour at runtime.

    Attributes:
        POLL_INTERVAL: Time in seconds to wait before polling
            LLMWhisperer's status API. Defaults to 30s
        MAX_POLLS: Total number of times to poll the status API.
            Set to -1 to poll indefinitely. Defaults to -1
    """

    POLL_INTERVAL = "ADAPTER_LLMW_POLL_INTERVAL"
    MAX_POLLS = "ADAPTER_LLMW_MAX_POLLS"


class WhispererConfig:
    """Dictionary keys used to configure LLMWhisperer service."""

    URL = "url"
    PROCESSING_MODE = "processing_mode"
    MODE = "mode"
    OUTPUT_MODE = "output_mode"
    UNSTRACT_KEY = "unstract_key"
    MEDIAN_FILTER_SIZE = "median_filter_size"
    GAUSSIAN_BLUR_RADIUS = "gaussian_blur_radius"
    FORCE_TEXT_PROCESSING = "force_text_processing"
    LINE_SPLITTER_TOLERANCE = "line_splitter_tolerance"
    HORIZONTAL_STRETCH_FACTOR = "horizontal_stretch_factor"
    PAGES_TO_EXTRACT = "pages_to_extract"
    STORE_METADATA_FOR_HIGHLIGHTING = "store_metadata_for_highlighting"
    ADD_LINE_NOS = "add_line_nos"
    OUTPUT_JSON = "output_json"
    PAGE_SEPARATOR = "page_seperator"
    MARK_VERTICAL_LINES = "mark_vertical_lines"
    MARK_HORIZONTAL_LINES = "mark_horizontal_lines"


class WhisperStatus:
    """Values returned / used by /whisper-status endpoint."""

    PROCESSING = "processing"
    PROCESSED = "processed"
    DELIVERED = "delivered"
    UNKNOWN = "unknown"
    # Used for async processing
    WHISPER_HASH = "whisper-hash"
    STATUS = "status"


class WhispererDefaults:
    """Defaults meant for LLMWhisperer."""

    MEDIAN_FILTER_SIZE = 0
    GAUSSIAN_BLUR_RADIUS = 0.0
    FORCE_TEXT_PROCESSING = False
    LINE_SPLITTER_TOLERANCE = 0.75
    HORIZONTAL_STRETCH_FACTOR = 1.0
    POLL_INTERVAL = int(os.getenv(WhispererEnv.POLL_INTERVAL, 30))
    MAX_POLLS = int(os.getenv(WhispererEnv.MAX_POLLS, 30))
    PAGES_TO_EXTRACT = ""
    ADD_LINE_NOS = True
    OUTPUT_JSON = True
    PAGE_SEPARATOR = "<<< >>>"
    MARK_VERTICAL_LINES = False
    MARK_HORIZONTAL_LINES = False


================================================
File: src/unstract/sdk/adapters/x2text/llm_whisperer/src/llm_whisperer.py
================================================
import json
import logging
import os
import time
from pathlib import Path
from typing import Any, Optional

import requests
from requests import Response
from requests.exceptions import ConnectionError, HTTPError, Timeout

from unstract.sdk.adapters.exceptions import ExtractorError
from unstract.sdk.adapters.utils import AdapterUtils
from unstract.sdk.adapters.x2text.constants import X2TextConstants
from unstract.sdk.adapters.x2text.dto import (
    TextExtractionMetadata,
    TextExtractionResult,
)
from unstract.sdk.adapters.x2text.llm_whisperer.src.constants import (
    HTTPMethod,
    OutputModes,
    ProcessingModes,
    WhispererConfig,
    WhispererDefaults,
    WhispererEndpoint,
    WhispererHeader,
    WhisperStatus,
)
from unstract.sdk.adapters.x2text.x2text_adapter import X2TextAdapter
from unstract.sdk.constants import MimeType
from unstract.sdk.file_storage import FileStorage, FileStorageProvider

logger = logging.getLogger(__name__)


class LLMWhisperer(X2TextAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("LLMWhisperer")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "llmwhisperer|0a1647f0-f65f-410d-843b-3d979c78350e"

    @staticmethod
    def get_name() -> str:
        return "LLMWhisperer"

    @staticmethod
    def get_description() -> str:
        return "LLMWhisperer X2Text"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/LLMWhisperer.png"

     

    def _get_request_headers(self) -> dict[str, Any]:
        """Obtains the request headers to authenticate with LLMWhisperer.

        Returns:
            str: Request headers
        """
        return {
            "accept": MimeType.JSON,
            WhispererHeader.UNSTRACT_KEY: self.config.get(WhispererConfig.UNSTRACT_KEY),
        }

    def _make_request(
        self,
        request_method: HTTPMethod,
        request_endpoint: str,
        headers: Optional[dict[str, Any]] = None,
        params: Optional[dict[str, Any]] = None,
        data: Optional[Any] = None,
    ) -> Response:
        """Makes a request to LLMWhisperer service.

        Args:
            request_method (HTTPMethod): HTTPMethod to call. Can be GET or POST
            request_endpoint (str): LLMWhisperer endpoint to hit
            headers (Optional[dict[str, Any]], optional): Headers to pass.
                Defaults to None.
            params (Optional[dict[str, Any]], optional): Query params to pass.
                Defaults to None.
            data (Optional[Any], optional): Data to pass in case of POST.
                Defaults to None.

        Returns:
            Response: Response from the request
        """
        llm_whisperer_svc_url = (
            f"{self.config.get(WhispererConfig.URL)}" f"/v1/{request_endpoint}"
        )
        if not headers:
            headers = self._get_request_headers()

        try:
            response: Response
            if request_method == HTTPMethod.GET:
                response = requests.get(
                    url=llm_whisperer_svc_url, headers=headers, params=params
                )
            elif request_method == HTTPMethod.POST:
                response = requests.post(
                    url=llm_whisperer_svc_url,
                    headers=headers,
                    params=params,
                    data=data,
                )
            else:
                raise ExtractorError(f"Unsupported request method: {request_method}")
            response.raise_for_status()
        except ConnectionError as e:
            logger.error(f"Adapter error: {e}")
            raise ExtractorError(
                "Unable to connect to LLMWhisperer service, please check the URL"
            )
        except Timeout as e:
            msg = "Request to LLMWhisperer has timed out"
            logger.error(f"{msg}: {e}")
            raise ExtractorError(msg)
        except HTTPError as e:
            logger.error(f"Adapter error: {e}")
            default_err = "Error while calling the LLMWhisperer service"
            msg = AdapterUtils.get_msg_from_request_exc(
                err=e, message_key="message", default_err=default_err
            )
            raise ExtractorError(msg)
        return response

    def _get_whisper_params(self, enable_highlight: bool = False) -> dict[str, Any]:
        """Gets query params meant for /whisper endpoint.

        The params is filled based on the configuration passed.

        Returns:
            dict[str, Any]: Query params
        """
        params = {
            WhispererConfig.PROCESSING_MODE: self.config.get(
                WhispererConfig.PROCESSING_MODE, ProcessingModes.TEXT.value
            ),
            # Not providing default value to maintain legacy compatablity
            # Providing default value will overide the params
            # processing_mode, force_text_processing
            WhispererConfig.MODE: self.config.get(WhispererConfig.MODE),
            WhispererConfig.OUTPUT_MODE: self.config.get(
                WhispererConfig.OUTPUT_MODE, OutputModes.LINE_PRINTER.value
            ),
            WhispererConfig.FORCE_TEXT_PROCESSING: self.config.get(
                WhispererConfig.FORCE_TEXT_PROCESSING,
                WhispererDefaults.FORCE_TEXT_PROCESSING,
            ),
            WhispererConfig.LINE_SPLITTER_TOLERANCE: self.config.get(
                WhispererConfig.LINE_SPLITTER_TOLERANCE,
                WhispererDefaults.LINE_SPLITTER_TOLERANCE,
            ),
            WhispererConfig.HORIZONTAL_STRETCH_FACTOR: self.config.get(
                WhispererConfig.HORIZONTAL_STRETCH_FACTOR,
                WhispererDefaults.HORIZONTAL_STRETCH_FACTOR,
            ),
            WhispererConfig.PAGES_TO_EXTRACT: self.config.get(
                WhispererConfig.PAGES_TO_EXTRACT,
                WhispererDefaults.PAGES_TO_EXTRACT,
            ),
            WhispererConfig.ADD_LINE_NOS: WhispererDefaults.ADD_LINE_NOS,
            WhispererConfig.OUTPUT_JSON: WhispererDefaults.OUTPUT_JSON,
            WhispererConfig.PAGE_SEPARATOR: self.config.get(
                WhispererConfig.PAGE_SEPARATOR,
                WhispererDefaults.PAGE_SEPARATOR,
            ),
            WhispererConfig.MARK_VERTICAL_LINES: self.config.get(
                WhispererConfig.MARK_VERTICAL_LINES,
                WhispererDefaults.MARK_VERTICAL_LINES,
            ),
            WhispererConfig.MARK_HORIZONTAL_LINES: self.config.get(
                WhispererConfig.MARK_HORIZONTAL_LINES,
                WhispererDefaults.MARK_HORIZONTAL_LINES,
            ),
        }
        if not params[WhispererConfig.FORCE_TEXT_PROCESSING]:
            params.update(
                {
                    WhispererConfig.MEDIAN_FILTER_SIZE: self.config.get(
                        WhispererConfig.MEDIAN_FILTER_SIZE,
                        WhispererDefaults.MEDIAN_FILTER_SIZE,
                    ),
                    WhispererConfig.GAUSSIAN_BLUR_RADIUS: self.config.get(
                        WhispererConfig.GAUSSIAN_BLUR_RADIUS,
                        WhispererDefaults.GAUSSIAN_BLUR_RADIUS,
                    ),
                }
            )

        if enable_highlight:
            params.update(
                {WhispererConfig.STORE_METADATA_FOR_HIGHLIGHTING: enable_highlight}
            )
        return params

    def test_connection(self) -> bool:
        self._make_request(
            request_method=HTTPMethod.GET,
            request_endpoint=WhispererEndpoint.TEST_CONNECTION,
        )
        return True

    def _check_status_until_ready(
        self, whisper_hash: str, headers: dict[str, Any], params: dict[str, Any]
    ) -> WhisperStatus:
        """Checks the extraction status by polling.

        Polls the /whisper-status endpoint in fixed intervals of
        env: ADAPTER_LLMW_POLL_INTERVAL for a certain number of times
        controlled by env: ADAPTER_LLMW_MAX_POLLS.

        Args:
            whisper_hash (str): Identifier for the extraction,
                returned by LLMWhisperer
            headers (dict[str, Any]): Headers to pass for the status check
            params (dict[str, Any]): Params to pass for the status check

        Returns:
            WhisperStatus: Status of the extraction
        """
        POLL_INTERVAL = WhispererDefaults.POLL_INTERVAL
        MAX_POLLS = WhispererDefaults.MAX_POLLS
        request_count = 0

        # Check status in fixed intervals upto max poll count.
        while True:
            request_count += 1
            logger.info(
                f"Checking status with interval: {POLL_INTERVAL}s"
                f", request count: {request_count} [max: {MAX_POLLS}]"
            )
            status_response = self._make_request(
                request_method=HTTPMethod.GET,
                request_endpoint=WhispererEndpoint.STATUS,
                headers=headers,
                params=params,
            )
            if status_response.status_code == 200:
                status_data = status_response.json()
                status = status_data.get(WhisperStatus.STATUS, WhisperStatus.UNKNOWN)
                logger.info(f"Whisper status for {whisper_hash}: {status}")
                if status in [WhisperStatus.PROCESSED, WhisperStatus.DELIVERED]:
                    break
            else:
                raise ExtractorError(
                    "Error checking LLMWhisperer status: "
                    f"{status_response.status_code} - {status_response.text}"
                )

            # Exit with error if max poll count is reached
            if request_count >= MAX_POLLS:
                raise ExtractorError(
                    "Unable to extract text after attempting" f" {request_count} times"
                )
            time.sleep(POLL_INTERVAL)

        return status

    def _extract_async(self, whisper_hash: str) -> str:
        """Makes an async extraction with LLMWhisperer.

        Polls and checks the status first before proceeding to retrieve once.

        Args:
            whisper_hash (str): Identifier of the extraction

        Returns:
            str: Extracted contents from the file
        """
        logger.info(f"Extracting async for whisper hash: {whisper_hash}")

        headers: dict[str, Any] = self._get_request_headers()
        params = {
            WhisperStatus.WHISPER_HASH: whisper_hash,
            WhispererConfig.OUTPUT_JSON: WhispererDefaults.OUTPUT_JSON,
        }

        # Polls in fixed intervals and checks status
        self._check_status_until_ready(
            whisper_hash=whisper_hash, headers=headers, params=params
        )

        retrieve_response = self._make_request(
            request_method=HTTPMethod.GET,
            request_endpoint=WhispererEndpoint.RETRIEVE,
            headers=headers,
            params=params,
        )
        if retrieve_response.status_code == 200:
            return retrieve_response.json()
        else:
            raise ExtractorError(
                "Error retrieving from LLMWhisperer: "
                f"{retrieve_response.status_code} - {retrieve_response.text}"
            )

    def _send_whisper_request(
        self,
        input_file_path: str,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
        enable_highlight: bool = False,
    ) -> requests.Response:
        headers = self._get_request_headers()
        headers["Content-Type"] = "application/octet-stream"
        params = self._get_whisper_params(enable_highlight)

        response: requests.Response
        try:
            response = self._make_request(
                request_method=HTTPMethod.POST,
                request_endpoint=WhispererEndpoint.WHISPER,
                headers=headers,
                params=params,
                data=fs.read(path=input_file_path, mode="rb"),
            )
        except OSError as e:
            logger.error(f"OS error while reading {input_file_path}: {e}")
            raise ExtractorError(str(e))
        return response

    def _extract_text_from_response(
        self,
        output_file_path: Optional[str],
        response: requests.Response,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> str:
        output_json = {}
        if response.status_code == 200:
            output_json = response.json()
        elif response.status_code == 202:
            whisper_hash = response.json().get(WhisperStatus.WHISPER_HASH)
            output_json = self._extract_async(whisper_hash=whisper_hash)
        else:
            raise ExtractorError("Couldn't extract text from file")
        if output_file_path:
            self._write_output_to_file(
                output_json=output_json, output_file_path=Path(output_file_path), fs=fs
            )
        return output_json.get("text", "")

    def _write_output_to_file(
        self,
        output_json: dict,
        output_file_path: Path,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> None:
        """Writes the extracted text and metadata to the specified output file
        and metadata file.

        Args:
            output_json (dict): The dictionary containing the extracted data,
                with "text" as the key for the main content.
            output_file_path (Path): The file path where the extracted text
                should be written.

        Raises:
            ExtractorError: If there is an error while writing the output file.
        """
        try:
            text_output = output_json.get("text", "")
            logger.info(f"Writing output to {output_file_path}")
            fs.write(
                path=output_file_path,
                mode="w",
                encoding="utf-8",
                data=text_output,
            )
            try:
                # Define the directory of the output file and metadata paths
                output_dir = output_file_path.parent
                metadata_dir = output_dir / "metadata"
                metadata_file_name = output_file_path.with_suffix(".json").name
                metadata_file_path = metadata_dir / metadata_file_name
                # Ensure the metadata directory exists
                fs.mkdir(str(metadata_dir), create_parents=True)
                # Remove the "text" key from the metadata
                metadata = {
                    key: value for key, value in output_json.items() if key != "text"
                }
                metadata_json = json.dumps(metadata, ensure_ascii=False, indent=4)
                logger.info(f"Writing metadata to {metadata_file_path}")

                fs.write(
                    path=metadata_file_path,
                    mode="w",
                    encoding="utf-8",
                    data=metadata_json,
                )
            except Exception as e:
                logger.error(
                    f"Error while writing metadata to {metadata_file_path}: {e}"
                )

        except Exception as e:
            logger.error(f"Error while writing {output_file_path}: {e}")
            raise ExtractorError(str(e))

    def process(
        self,
        input_file_path: str,
        output_file_path: Optional[str] = None,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
        **kwargs: dict[Any, Any],
    ) -> TextExtractionResult:
        """Used to extract text from documents.

        Args:
            input_file_path (str): Path to file that needs to be extracted
            output_file_path (Optional[str], optional): File path to write
                extracted text into, if None doesn't write to a file.
                Defaults to None.

        Returns:
            str: Extracted text
        """

        response: requests.Response = self._send_whisper_request(
            input_file_path,
            fs,
            bool(kwargs.get(X2TextConstants.ENABLE_HIGHLIGHT, False)),
        )

        metadata = TextExtractionMetadata(
            whisper_hash=response.headers.get(X2TextConstants.WHISPER_HASH, "")
        )

        return TextExtractionResult(
            extracted_text=self._extract_text_from_response(
                output_file_path, response, fs
            ),
            extraction_metadata=metadata,
        )


================================================
File: src/unstract/sdk/adapters/x2text/llm_whisperer/src/static/json_schema.json
================================================
{
  "title": "LLMWhisperer v1 Text Extractor",
  "type": "object",
  "required": [
    "adapter_name",
    "unstract_key",
    "url"
  ],
  "description": "LLMWhisperer v1 is deprecated, use the cheaper and faster [LLMWhisperer v2](https://docs.unstract.com/llmwhisperer/llm_whisperer/faqs/v1_to_v2/) instead.",
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: LLMWhisperer 1"
    },
    "url": {
      "type": "string",
      "title": "URL",
      "format": "uri",
      "default": "https://llmwhisperer-api.unstract.com",
      "description": "Provide the URL of the LLMWhisperer service. Please note that this version of LLMWhisperer is deprecated."
    },
    "unstract_key": {
      "type": "string",
      "title": "Unstract Key",
      "format": "password",
      "description": "API key obtained from the [Unstract developer portal](https://unstract-api-resource.developer.azure-api.net)"
    },
    "mode": {
      "type": "string",
      "title": "Mode",
      "enum": [
        "native_text",
        "low_cost",
        "high_quality",
        "form"
      ],
      "default": "form",
      "description": "Processing mode to use, described in the [LLMWhisperer v1 documentation](https://docs.unstract.com/llmwhisperer/1.0.0/llm_whisperer/apis/llm_whisperer_text_extraction_api/#processing-modes)"
    },
    "output_mode": {
      "type": "string",
      "title": "Output Mode",
      "enum": [
        "line-printer",
        "dump-text",
        "text"
      ],
      "default": "line-printer",
      "description": "Output mode to use, described in the [LLMWhisperer v1 documentation](https://docs.unstract.com/llmwhisperer/1.0.0/llm_whisperer/apis/llm_whisperer_text_extraction_api/#output-modes)"
    },

    "line_splitter_tolerance": {
      "type": "number",
      "title": "Line Splitter Tolerance",
      "default": 0.4,
      "description": "Reduce this value to split lines less often, increase to split lines more often. Useful when PDFs have multi column layout with text in each column that is not aligned."
    },
    "horizontal_stretch_factor": {
      "type": "number",
      "title": "Horizontal Stretch Factor",
      "default": 1.0,
      "description": "Increase this value to stretch text horizontally, decrease to compress text horizontally. Useful when multi column text merge with each other."
    },
    "pages_to_extract": {
      "type": "string",
      "title": "Page number(s) or range to extract",
      "default": "",
      "pattern": "^(\\s*\\d+-\\d+|\\s*\\d+-|\\s*\\d+|^$)(,\\d+-\\d+|,\\d+-|,\\d+)*$",
      "description": "Specify the range of pages to extract (e.g., 1-5, 7, 10-12, 50-). Leave it empty to extract all pages."
    },
    "page_seperator": {
      "type": "string",
      "title": "Page separator",
      "default": "<<< >>>",
      "description": "Specify a pattern to separate the pages in the document (e.g., <<< {{page_no}} >>>, <<< >>>). This pattern will be inserted at the end of every page. Omit {{page_no}} if you don't want to include the page number in the separator."
    }
  },
  "if": {
    "anyOf": [
      {
        "properties": {
          "mode": {
            "const": "low_cost"
          }
        }
      },
      {
        "properties": {
          "mode": {
            "const": "high_quality"
          }
        }
      },
      {
        "properties": {
          "mode": {
            "const": "form"
          }
        }
      }
    ]
  },
  "then": {
    "properties": {
      "median_filter_size": {
        "type": "integer",
        "title": "Median Filter Size",
        "default": 0,
        "description": "The size of the median filter to use for pre-processing the image during OCR based extraction. Useful to eliminate scanning artifacts and low quality JPEG artifacts. Default is 0 if the value is not explicitly set. Available only in the Enterprise version."
      },
      "gaussian_blur_radius": {
        "type": "number",
        "title": "Gaussian Blur Radius",
        "default": 0.0,
        "description": "The radius of the gaussian blur to use for pre-processing the image during OCR based extraction. Useful to eliminate noise from the image. Default is 0.0 if the value is not explicitly set. Available only in the Enterprise version."
      },
      "mark_vertical_lines": {
        "type": "boolean",
        "title": "Mark Vertical Lines",
        "default": false,
        "description": "Detect vertical lines in the document and replicate the same using text (using \"|\" symbol). Use this for displaying tables with borders."
      },
      "mark_horizontal_lines": {
        "type": "boolean",
        "title": "Mark Horizontal Lines",
        "default": false,
        "description": "Detect horizontal lines in the document and replicate the same using text (using \"-\" symbol). Use this for displaying tables with borders and other horizontal serperators found in the document."
      }
    },
    "required": [
      "median_filter_size",
      "gaussian_blur_radius"
    ]
  }
}


================================================
File: src/unstract/sdk/adapters/x2text/llm_whisperer_v2/README.md
================================================
# Unstract LLMWWhisperer v2 X2Text Adapter

## Env variables

The below env variables are resolved by LLMWhisperer adapter

| Variable                     | Description                                                                                  |
| ---------------------------- | -------------------------------------------------------------------------------------------- |
| `ADAPTER_LLMW_POLL_INTERVAL` | Time in seconds to wait before polling LLMWhisperer's status API. Defaults to 30s            |
| `ADAPTER_LLMW_MAX_POLLS`     | Total number of times to poll the status API. Defaults to 30                                 |


---
id: llm_whisperer_apis_changelog
---

# Changelog

## Version 2.0.0

:::warning
This version of the API is not backward compatible with the previous version.
:::

### API endpoint

- The base URL for the **V2** APIs is `https://llmwhisperer-api.unstract.com/api/v2`

### Global change in parameter naming

- All use of `whisper-hash` as a parameter has been replaced with `whisper_hash` for consistency. 

### Whisper parameters

#### Added
- `mode` (str, optional): The processing mode. 
- `mark_vertical_lines` (bool, optional): Whether to reproduce vertical lines in the document.
- `mark_horizontal_lines` (bool, optional): Whether to reproduce horizontal lines in the document. 
- `line_splitter_strategy` (str, optional): The line splitter strategy to use. An advanced option for customizing the line splitting process. 
- `lang` (str, optional): The language of the document. 
- `tag` (str, optional): A tag to associate with the document. Used for auditing and tracking purposes.
- `file_name` (str, optional): The name of the file being processed. Used for auditing and tracking purposes.
- `use_webhook` (str, optional): The name of the webhook to call after the document is processed.
- `webhook_metadata` (str, optional): Metadata to send to the webhook after the document is processed.

#### Removed
- `timeout` (int, optional): The timeout for API requests. *There is no sync mode now. All requests are async.*
- `force_text_processing` (bool, optional): Whether to force text processing. *This is feature is removed*
- `ocr_provider` (str, optional): The OCR provider to use. *This is superseded by `mode`*
- `processing_mode` (str, optional): The processing mode. *This is superseded by `mode`*
- `store_metadata_for_highlighting` (bool, optional): Whether to store metadata for highlighting. *Feature is removed. Data still available and set back when retrieve is called*


### New features

#### Webhooks

- Added support for webhooks. You can now register a webhook and use it to receive the processed document.


================================================
File: src/unstract/sdk/adapters/x2text/llm_whisperer_v2/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-llm_whisperer-x2text-v2"
version = "0.0.1"
description = "V2 of LLMWhisperer X2Text Adapter"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/x2text/llm_whisperer_v2/src/__init__.py
================================================
from .llm_whisperer_v2 import LLMWhispererV2

metadata = {
    "name": LLMWhispererV2.__name__,
    "version": "1.0.0",
    "adapter": LLMWhispererV2,
    "description": "LLMWhispererV2 X2Text adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/x2text/llm_whisperer_v2/src/constants.py
================================================
import os
from enum import Enum


class Modes(Enum):
    NATIVE_TEXT = "native_text"
    LOW_COST = "low_cost"
    HIGH_QUALITY = "high_quality"
    FORM = "form"


class OutputModes(Enum):
    LAYOUT_PRESERVING = "layout_preserving"
    TEXT = "text"


class HTTPMethod(Enum):
    GET = "GET"
    POST = "POST"


class WhispererHeader:
    UNSTRACT_KEY = "unstract-key"


class WhispererEndpoint:
    """Endpoints available at LLMWhisperer service."""

    TEST_CONNECTION = "test-connection"
    WHISPER = "whisper"
    STATUS = "whisper-status"
    RETRIEVE = "whisper-retrieve"
    HIGHLIGHTS = "highlights"


class WhispererEnv:
    """Env variables for LLMWhisperer.

    Can be used to alter behaviour at runtime.

    Attributes:
        POLL_INTERVAL: Time in seconds to wait before polling
            LLMWhisperer's status API. Defaults to 30s
        MAX_POLLS: Total number of times to poll the status API.
            Set to -1 to poll indefinitely. Defaults to -1
        STATUS_RETRIES: Number of times to retry calling LLLMWhisperer's
        status API on failure during polling. Defaults to 5.
    """

    WAIT_TIMEOUT = "ADAPTER_LLMW_WAIT_TIMEOUT"
    LOG_LEVEL = "LOG_LEVEL"


class WhispererConfig:
    """Dictionary keys used to configure LLMWhisperer service."""

    URL = "url"
    MODE = "mode"
    OUTPUT_MODE = "output_mode"
    UNSTRACT_KEY = "unstract_key"
    MEDIAN_FILTER_SIZE = "median_filter_size"
    GAUSSIAN_BLUR_RADIUS = "gaussian_blur_radius"
    LINE_SPLITTER_TOLERANCE = "line_splitter_tolerance"
    LINE_SPLITTER_STRATEGY = "line_spitter_strategy"
    HORIZONTAL_STRETCH_FACTOR = "horizontal_stretch_factor"
    PAGES_TO_EXTRACT = "pages_to_extract"
    MARK_VERTICAL_LINES = "mark_vertical_lines"
    MARK_HORIZONTAL_LINES = "mark_horizontal_lines"
    PAGE_SEPARATOR = "page_seperator"
    URL_IN_POST = "url_in_post"
    TAG = "tag"
    USE_WEBHOOK = "use_webhook"
    WEBHOOK_METADATA = "webhook_metadata"
    TEXT_ONLY = "text_only"
    WAIT_TIMEOUT = "wait_timeout"
    WAIT_FOR_COMPLETION = "wait_for_completion"
    LOGGING_LEVEL = "logging_level"
    ADD_LINE_NOS = "add_line_nos"
    EXTRACT_ALL_LINES = "extract_all_lines"
    LINES = "lines"


class WhisperStatus:
    """Values returned / used by /whisper-status endpoint."""

    PROCESSING = "processing"
    PROCESSED = "processed"
    DELIVERED = "delivered"
    UNKNOWN = "unknown"
    # Used for async processing
    WHISPER_HASH = "whisper_hash"
    STATUS = "status"


class WhispererDefaults:
    """Defaults meant for LLMWhisperer."""

    MEDIAN_FILTER_SIZE = 0
    GAUSSIAN_BLUR_RADIUS = 0.0
    FORCE_TEXT_PROCESSING = False
    LINE_SPLITTER_TOLERANCE = 0.75
    LINE_SPLITTER_STRATEGY = "left-priority"
    HORIZONTAL_STRETCH_FACTOR = 1.0
    PAGES_TO_EXTRACT = ""
    PAGE_SEPARATOR = "<<<"
    MARK_VERTICAL_LINES = False
    MARK_HORIZONTAL_LINES = False
    URL_IN_POST = False
    TAG = "default"
    TEXT_ONLY = False
    WAIT_TIMEOUT = int(os.getenv(WhispererEnv.WAIT_TIMEOUT, 300))
    WAIT_FOR_COMPLETION = True
    LOGGING_LEVEL = os.getenv(WhispererEnv.LOG_LEVEL, "INFO")


================================================
File: src/unstract/sdk/adapters/x2text/llm_whisperer_v2/src/dto.py
================================================
from dataclasses import dataclass
from typing import Optional, Union, List


@dataclass
class WhispererRequestParams:
    """DTO for LLM Whisperer API request parameters.

    Args:
        tag (Optional[Union[str, List[str]]]): Tag value. Can be initialized with List[str] or str.
             Will be converted to str or None after initialization.
        enable_highlight (bool): Whether to enable highlighting. Defaults to False.
    """

    # TODO: Extend this DTO to include all Whisperer API parameters
    tag: Optional[Union[str, List[str]]] = None
    enable_highlight: bool = False

    def __post_init__(self) -> None:
        # TODO: Allow list of tags once it's supported in LLMW v2
        if isinstance(self.tag, list):
            self.tag = self.tag[0] if self.tag else None


================================================
File: src/unstract/sdk/adapters/x2text/llm_whisperer_v2/src/helper.py
================================================
import json
import logging
from io import BytesIO
from pathlib import Path
from typing import Any, Optional

import requests
from requests import Response
from requests.exceptions import ConnectionError, HTTPError, Timeout
from unstract.llmwhisperer.client_v2 import (
    LLMWhispererClientException,
    LLMWhispererClientV2,
)

from unstract.sdk.adapters.exceptions import ExtractorError
from unstract.sdk.adapters.utils import AdapterUtils
from unstract.sdk.adapters.x2text.llm_whisperer_v2.src.constants import (
    Modes,
    OutputModes,
    WhispererConfig,
    WhispererDefaults,
    WhispererHeader,
    WhisperStatus,
)
from unstract.sdk.adapters.x2text.constants import X2TextConstants
from unstract.sdk.adapters.x2text.llm_whisperer_v2.src.dto import WhispererRequestParams
from unstract.sdk.constants import MimeType
from unstract.sdk.file_storage import FileStorage, FileStorageProvider

logger = logging.getLogger(__name__)


class LLMWhispererHelper:
    @staticmethod
    def get_request_headers(config: dict[str, Any]) -> dict[str, Any]:
        """Obtains the request headers to authenticate with LLMWhisperer.

        Returns:
            str: Request headers
        """
        return {
            "accept": MimeType.JSON,
            WhispererHeader.UNSTRACT_KEY: config.get(WhispererConfig.UNSTRACT_KEY),
        }

    @staticmethod
    def test_connection_request(
        config: dict[str, Any], request_endpoint: str
    ) -> Response:
        llm_whisperer_svc_url = f"{config.get(WhispererConfig.URL)}" f"/api/v2"
        headers = LLMWhispererHelper.get_request_headers(config=config)

        try:
            response: Response
            url = f"{llm_whisperer_svc_url}/{request_endpoint}"
            response = requests.get(url=url, headers=headers)
            response.raise_for_status()
        except ConnectionError as e:
            logger.error(f"Adapter error: {e}")
            raise ExtractorError(
                "Unable to connect to LLMWhisperer service, please check the URL",
                actual_err=e,
                status_code=503,
            )
        except Timeout as e:
            msg = "Request to LLMWhisperer has timed out"
            logger.error(f"{msg}: {e}")
            raise ExtractorError(msg, actual_err=e, status_code=504)
        except HTTPError as e:
            logger.error(f"Adapter error: {e}")
            default_err = "Error while calling the LLMWhisperer service"
            msg = AdapterUtils.get_msg_from_request_exc(
                err=e, message_key="message", default_err=default_err
            )
            raise ExtractorError(msg, status_code=e.response.status_code, actual_err=e)

    @staticmethod
    def make_request(
        config: dict[str, Any],
        headers: Optional[dict[str, Any]] = None,
        params: Optional[dict[str, Any]] = None,
        data: Optional[Any] = None,
        type: str = "whisper"
    ) -> Response:
        """Makes a request to LLMWhisperer service.

        Args:
            request_method (HTTPMethod): HTTPMethod to call. Can be GET or POST
            request_endpoint (str): LLMWhisperer endpoint to hit
            headers (Optional[dict[str, Any]], optional): Headers to pass.
                Defaults to None.
            params (Optional[dict[str, Any]], optional): Query params to pass.
                Defaults to None.
            data (Optional[Any], optional): Data to pass in case of POST.
                Defaults to None.

        Returns:
            Response: Response from the request
        """
        llm_whisperer_svc_url = f"{config.get(WhispererConfig.URL)}" f"/api/v2"
        if not headers:
            headers = LLMWhispererHelper.get_request_headers(config=config)

        try:
            response: dict[str, Any]
            client = LLMWhispererClientV2(
                base_url=llm_whisperer_svc_url,
                api_key=config.get(WhispererConfig.UNSTRACT_KEY),
                logging_level=WhispererDefaults.LOGGING_LEVEL,
            )
            if type == "whisper":
                response = client.whisper(**params, stream=data)
                if response["status_code"] == 200:
                    response["extraction"][X2TextConstants.WHISPER_HASH_V2] = response.get(
                        X2TextConstants.WHISPER_HASH_V2, ""
                    )
                    return response["extraction"]
                else:
                    raise ExtractorError(
                        response["message"],
                        response["status_code"],
                        actual_err=response,
                    )
            elif type == "highlight":
                response = client.get_highlight_data(**params)
                return response

        except ConnectionError as e:
            logger.error(f"Adapter error: {e}")
            raise ExtractorError(
                "Unable to connect to LLMWhisperer service, please check the URL",
                actual_err=e,
                status_code=503,
            )
        except Timeout as e:
            msg = "Request to LLMWhisperer has timed out"
            logger.error(f"{msg}: {e}")
            raise ExtractorError(msg, actual_err=e, status_code=504)
        except LLMWhispererClientException as e:
            logger.error(f"LLM Whisperer error: {e}")
            raise ExtractorError(
                message=f"LLM Whisperer error: {e}",
                actual_err=e,
                status_code=500,
            )

        return response

    @staticmethod
    def get_whisperer_params(
        config: dict[str, Any], extra_params: WhispererRequestParams
    ) -> dict[str, Any]:
        """Gets query params meant for /whisper endpoint.

        The params is filled based on the configuration passed.

        Returns:
            dict[str, Any]: Query params
        """
        params = {
            WhispererConfig.MODE: config.get(WhispererConfig.MODE, Modes.FORM.value),
            WhispererConfig.OUTPUT_MODE: config.get(
                WhispererConfig.OUTPUT_MODE, OutputModes.LAYOUT_PRESERVING.value
            ),
            WhispererConfig.LINE_SPLITTER_TOLERANCE: config.get(
                WhispererConfig.LINE_SPLITTER_TOLERANCE,
                WhispererDefaults.LINE_SPLITTER_TOLERANCE,
            ),
            WhispererConfig.LINE_SPLITTER_STRATEGY: config.get(
                WhispererConfig.LINE_SPLITTER_STRATEGY,
                WhispererDefaults.LINE_SPLITTER_STRATEGY,
            ),
            WhispererConfig.HORIZONTAL_STRETCH_FACTOR: config.get(
                WhispererConfig.HORIZONTAL_STRETCH_FACTOR,
                WhispererDefaults.HORIZONTAL_STRETCH_FACTOR,
            ),
            WhispererConfig.PAGES_TO_EXTRACT: config.get(
                WhispererConfig.PAGES_TO_EXTRACT,
                WhispererDefaults.PAGES_TO_EXTRACT,
            ),
            WhispererConfig.MARK_VERTICAL_LINES: config.get(
                WhispererConfig.MARK_VERTICAL_LINES,
                WhispererDefaults.MARK_VERTICAL_LINES,
            ),
            WhispererConfig.MARK_HORIZONTAL_LINES: config.get(
                WhispererConfig.MARK_HORIZONTAL_LINES,
                WhispererDefaults.MARK_HORIZONTAL_LINES,
            ),
            WhispererConfig.PAGE_SEPARATOR: config.get(
                WhispererConfig.PAGE_SEPARATOR,
                WhispererDefaults.PAGE_SEPARATOR,
            ),
            WhispererConfig.ADD_LINE_NOS: extra_params.enable_highlight,
            # Not providing default value to maintain legacy compatablity
            # these are optional params and identifiers for audit
            WhispererConfig.TAG: extra_params.tag
            or config.get(
                WhispererConfig.TAG,
                WhispererDefaults.TAG,
            ),
            WhispererConfig.USE_WEBHOOK: config.get(WhispererConfig.USE_WEBHOOK, ""),
            WhispererConfig.WEBHOOK_METADATA: config.get(
                WhispererConfig.WEBHOOK_METADATA
            ),
            WhispererConfig.WAIT_TIMEOUT: config.get(
                WhispererConfig.WAIT_TIMEOUT,
                WhispererDefaults.WAIT_TIMEOUT,
            ),
            WhispererConfig.WAIT_FOR_COMPLETION: WhispererDefaults.WAIT_FOR_COMPLETION,
        }
        if params[WhispererConfig.MODE] == Modes.LOW_COST.value:
            params.update(
                {
                    WhispererConfig.MEDIAN_FILTER_SIZE: config.get(
                        WhispererConfig.MEDIAN_FILTER_SIZE,
                        WhispererDefaults.MEDIAN_FILTER_SIZE,
                    ),
                    WhispererConfig.GAUSSIAN_BLUR_RADIUS: config.get(
                        WhispererConfig.GAUSSIAN_BLUR_RADIUS,
                        WhispererDefaults.GAUSSIAN_BLUR_RADIUS,
                    ),
                }
            )
        return params

    @staticmethod
    def send_whisper_request(
        input_file_path: str,
        config: dict[str, Any],
        extra_params: WhispererRequestParams,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> requests.Response:
        params = LLMWhispererHelper.get_whisperer_params(
            config=config, extra_params=extra_params
        )
        response: requests.Response
        try:
            input_file_data = BytesIO(fs.read(path=input_file_path, mode="rb"))
            enable_highlight = extra_params.enable_highlight
            response = LLMWhispererHelper.make_request(
                config=config,
                params=params,
                data=input_file_data,
            )
            if enable_highlight:
                whisper_hash = response.get(X2TextConstants.WHISPER_HASH_V2, "")
                highlight_data = LLMWhispererHelper.make_highlight_data_request(
                    config,
                    whisper_hash,
                    enable_highlight,
                )
                response["line_metadata"] = highlight_data
        except OSError as e:
            logger.error(f"OS error while reading {input_file_path}: {e}")
            raise ExtractorError(str(e))
        return response

    @staticmethod
    def make_highlight_data_request(
        config: dict[str, Any],
        whisper_hash: str,
        enable_highlight: bool
    ) -> dict[Any, Any]:
        """Makes a call to get highlight data from LLMWhisperer.

        Args:
            whisper_hash (str): Identifier of the extraction

        Returns:
            str: Extracted contents from the file
        """
        logger.info(f"Extracting async for whisper hash: {whisper_hash}")

        headers: dict[str, Any] = LLMWhispererHelper.get_request_headers(config)
        params = {
            WhisperStatus.WHISPER_HASH: whisper_hash,
            WhispererConfig.EXTRACT_ALL_LINES: enable_highlight,
            WhispererConfig.LINES: "",
        }

        retrieve_response = LLMWhispererHelper.make_request(
            config=config,
            headers=headers,
            params=params,
            type="highlight",
        )
        return retrieve_response
        
    @staticmethod
    def extract_text_from_response(
        output_file_path: Optional[str],
        response: dict[str, Any],
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> str:
        if not response:
            raise ExtractorError("Couldn't extract text from file", status_code=500)
        output_json = {}
        output_json = response
        if output_file_path:
            LLMWhispererHelper.write_output_to_file(
                output_json=output_json,
                output_file_path=Path(output_file_path),
                fs=fs,
            )
        return output_json.get("result_text", "")

    @staticmethod
    def write_output_to_file(
        output_json: dict,
        output_file_path: Path,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> None:
        """Writes the extracted text and metadata to the specified output file
        and metadata file.

        Args:
            output_json (dict): The dictionary containing the extracted data,
                with "text" as the key for the main content.
            output_file_path (Path): The file path where the extracted text
                should be written.

        Raises:
            ExtractorError: If there is an error while writing the output file.
        """
        try:
            text_output = output_json.get("result_text", "")
            logger.info(f"Writing output to {output_file_path}")
            fs.write(
                path=str(output_file_path),
                mode="w",
                data=text_output,
                encoding="utf-8",
            )
        except Exception as e:
            logger.error(f"Error while writing {output_file_path}: {e}")
            raise ExtractorError(str(e))
        try:
            # Define the directory of the output file and metadata paths
            output_dir = output_file_path.parent
            metadata_dir = output_dir / "metadata"
            metadata_file_name = output_file_path.with_suffix(".json").name
            metadata_file_path = metadata_dir / metadata_file_name
            # Ensure the metadata directory exists
            fs.mkdir(create_parents=True, path=str(metadata_dir))
            # Remove the "result_text" key from the metadata
            metadata = {
                key: value for key, value in output_json.items() if key != "result_text"
            }
            metadata_json = json.dumps(metadata, ensure_ascii=False, indent=4)
            logger.info(f"Writing metadata to {metadata_file_path}")
            fs.write(
                path=str(metadata_file_path),
                mode="w",
                data=metadata_json,
                encoding="utf-8",
            )
        except Exception as e:
            logger.warn(f"Error while writing metadata to {metadata_file_path}: {e}")

================================================
File: src/unstract/sdk/adapters/x2text/llm_whisperer_v2/src/llm_whisperer_v2.py
================================================
import json
import logging
import os
from typing import Any, Optional

import requests

from unstract.sdk.adapters.x2text.constants import X2TextConstants
from unstract.sdk.adapters.x2text.dto import (
    TextExtractionMetadata,
    TextExtractionResult,
)
from unstract.sdk.adapters.x2text.llm_whisperer_v2.src.constants import (
    HTTPMethod,
    WhispererEndpoint,
)
from unstract.sdk.adapters.x2text.llm_whisperer_v2.src.dto import WhispererRequestParams
from unstract.sdk.adapters.x2text.llm_whisperer_v2.src.helper import LLMWhispererHelper
from unstract.sdk.adapters.x2text.x2text_adapter import X2TextAdapter
from unstract.sdk.file_storage import FileStorage, FileStorageProvider

logger = logging.getLogger(__name__)


class LLMWhispererV2(X2TextAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("LLMWhispererV2")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "llmwhisperer|a5e6b8af-3e1f-4a80-b006-d017e8e67f93"

    @staticmethod
    def get_name() -> str:
        return "LLMWhisperer V2"

    @staticmethod
    def get_description() -> str:
        return "LLMWhisperer V2 X2Text"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/LLMWhispererV2.png"

    def test_connection(self) -> bool:
        LLMWhispererHelper.test_connection_request(
            config=self.config,
            request_endpoint=WhispererEndpoint.TEST_CONNECTION,
        )
        return True

    def process(
        self,
        input_file_path: str,
        output_file_path: Optional[str] = None,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
        **kwargs: dict[Any, Any],
    ) -> TextExtractionResult:
        """Used to extract text from documents.

        Args:
            input_file_path (str): Path to file that needs to be extracted
            output_file_path (Optional[str], optional): File path to write
                extracted text into, if None doesn't write to a file.
                Defaults to None.

        Returns:
            str: Extracted text
        """

        enable_highlight = kwargs.get(X2TextConstants.ENABLE_HIGHLIGHT, False)
        extra_params = WhispererRequestParams(
            tag=kwargs.get(X2TextConstants.TAGS),
            enable_highlight=enable_highlight,
        )
        response: requests.Response = LLMWhispererHelper.send_whisper_request(
            input_file_path=input_file_path,
            config=self.config,
            fs=fs,
            extra_params=extra_params,
        )
        metadata = TextExtractionMetadata(
            whisper_hash=response.get(X2TextConstants.WHISPER_HASH_V2, "")
        )

        return TextExtractionResult(
            extracted_text=LLMWhispererHelper.extract_text_from_response(
                output_file_path, response, fs=fs,
            ),
            extraction_metadata=metadata,
        )


================================================
File: src/unstract/sdk/adapters/x2text/llm_whisperer_v2/src/static/json_schema.json
================================================
{
  "title": "LLMWhisperer v2 Text Extractor",
  "type": "object",
  "required": [
    "adapter_name",
    "unstract_key",
    "url"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "llm-whisperer-v2",
      "description": "Provide a unique name for this adapter instance. Example: LLMWhisperer 1"
    },
    "url": {
      "type": "string",
      "title": "URL",
      "format": "uri",
      "default": "https://llmwhisperer-api.us-central.unstract.com",
      "description": "Provide the base URL of the LLMWhisperer service based on your region, can be obtained from the [Unstract developer portal](https://us-central.unstract.com/landing?selectedProduct=llm-whisperer)."
    },
    "unstract_key": {
      "type": "string",
      "title": "Unstract Key",
      "format": "password",
      "description": "API key obtained from the [Unstract developer portal](https://us-central.unstract.com/landing?selectedProduct=llm-whisperer)"
    },
    "mode": {
      "type": "string",
      "title": "Mode",
      "enum": [
        "native_text",
        "low_cost",
        "high_quality",
        "form"
      ],
      "default": "form",
      "description": "Processing mode to use, described in the [LLMWhisperer documentation](https://docs.unstract.com/llmwhisperer/llm_whisperer/apis/llm_whisperer_text_extraction_api/#modes)."
    },
    "output_mode": {
      "type": "string",
      "title": "Output Mode",
      "enum": [
        "layout_preserving",
        "text"
      ],
      "default": "layout_preserving",
      "description": "Output format, described in the [LLMWhisperer documentation](https://docs.unstract.com/llmwhisperer/llm_whisperer/apis/llm_whisperer_text_extraction_api/#output-modes)"
    },
    "line_splitter_tolerance": {
      "type": "number",
      "title": "Line Splitter Tolerance",
      "default": 0.4,
      "description": "Factor to decide when to move text to the next line when it is above or below the baseline. The default value of 0.4 signifies 40% of the average character height"
    },
    "line_splitter_strategy": {
      "type": "string",
      "title": "Line Splitter Strategy",
      "default":"left-priority",
      "description": "An advanced option for customizing the line splitting process."
    },
    "horizontal_stretch_factor": {
      "type": "number",
      "title": "Horizontal Stretch Factor",
      "default": 1.0,
      "description": "Increase this value to stretch text horizontally, decrease to compress text horizontally. Useful when multi column text merge with each other."
    },
    "pages_to_extract": {
      "type": "string",
      "title": "Page number(s) or range to extract",
      "default": "",
      "pattern": "^(\\s*\\d+-\\d+|\\s*\\d+-|\\s*\\d+|^$)(,\\d+-\\d+|,\\d+-|,\\d+)*$",
      "description": "Specify the range of pages to extract (e.g., 1-5, 7, 10-12, 50-). Leave it empty to extract all pages."
    },
    "page_seperator": {
      "type": "string",
      "title": "Page separator",
      "default": "<<<",
      "description": "Specify a pattern to separate the pages in the document. This pattern will be inserted at the end of every page (e.g., `<<< {{page_no}} >>>`, `<<< >>>`). Omit `{{page_no}}` if you don't want to include the page number in the separator."
    },
    "mark_vertical_lines": {
      "type": "boolean",
      "title": "Mark vertical lines",
      "default": false,
      "description": "States whether to reproduce vertical lines in the document. Note: This parameter is not applicable if `mode` chosen is `native_text`."
    },
    "mark_horizontal_lines": {
      "type": "boolean",
      "title": "Mark horizontal lines",
      "default": false,
      "description": "States whether to reproduce horizontal lines in the document. Note: This parameter is not applicable if `mode` chosen is `native_text` and will not work if `mark_vertical_lines` is set to `false`."
    },
    "tag": {
      "type": "string",
      "title": "Tag",
      "default": "default",
      "description": "Auditing feature. Set a value which will be associated with the invocation of the adapter. This can be used for cross referencing in usage reports."
    },
    "use_webhook": {
      "type": "string",
      "title": "Webhook",
      "default": "",
      "description": "The webhook's name which will should be called after the conversion is complete. The name should have been registered earlier using the webhooks management endpoint"
    },
    "webhook_metadata": {
      "type": "string",
      "title": "Webhook Metadata",
      "default": "",
      "description": "Any metadata which should be sent to the webhook. This data is sent verbatim to the callback endpoint."
    }
  },
  "if": {
    "anyOf": [
      {
        "properties": {
          "mode": {
            "const": "low_cost"
          }
        }
      }
    ]
  },
  "then": {
    "properties": {
      "median_filter_size": {
        "type": "integer",
        "title": "Median Filter Size",
        "default": 0,
        "description": "The size of the median filter to use for pre-processing the image during OCR based extraction. Useful to eliminate scanning artifacts and low quality JPEG artifacts. Default is 0 if the value is not explicitly set. Available only in the Enterprise version."
      },
      "gaussian_blur_radius": {
        "type": "number",
        "title": "Gaussian Blur Radius",
        "default": 0.0,
        "description": "The radius of the gaussian blur to use for pre-processing the image during OCR based extraction. Useful to eliminate noise from the image. Default is 0.0 if the value is not explicitly set. Available only in the Enterprise version."
      }
    },
    "required": [
      "median_filter_size",
      "gaussian_blur_radius"
    ]
  }
}


================================================
File: src/unstract/sdk/adapters/x2text/no_op/README.md
================================================
# Unstract NoOp x2text adapter for load testing

A x2text adapter that does not perform any operation. Waits for the configured time before returning a response. This can be useful to perform tests on the system


================================================
File: src/unstract/sdk/adapters/x2text/no_op/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-unstructured-io-x2text"
version = "0.0.1"
description = "UnstructuredIO X2Text Adapter"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/x2text/no_op/src/__init__.py
================================================
from unstract.sdk.adapters.x2text.no_op.src.no_op_x2text import NoOpX2Text

metadata = {
    "name": NoOpX2Text.__name__,
    "version": "1.0.0",
    "adapter": NoOpX2Text,
    "description": "NoOpX2Text",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/x2text/no_op/src/no_op_x2text.py
================================================
import logging
import os
import time
from typing import Any, Optional

from unstract.sdk.adapters.x2text.dto import TextExtractionResult
from unstract.sdk.adapters.x2text.x2text_adapter import X2TextAdapter
from unstract.sdk.file_storage import FileStorage, FileStorageProvider

logger = logging.getLogger(__name__)


class NoOpX2Text(X2TextAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("NoOpX2Text")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "noOpX2text|mp66d1op-7100-d340-9101-846fc7115676"

    @staticmethod
    def get_name() -> str:
        return "No Op X2Text"

    @staticmethod
    def get_description() -> str:
        return "No Op X2Text Adapter"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/noOpx2Text.png"

     

    def process(
        self,
        input_file_path: str,
        output_file_path: Optional[str] = None,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
        **kwargs: dict[Any, Any],
    ) -> TextExtractionResult:
        extracted_text: str = (
            "This is a No Op x2text adapter response."
            " This is a sample response and intended for testing \f"
        )
        time.sleep(self.config.get("wait_time"))
        if output_file_path:
            fs.write(
                path=output_file_path, mode="w", data=extracted_text, encoding="utf-8"
            )
        return TextExtractionResult(extracted_text=extracted_text)

    def test_connection(self) -> bool:
        time.sleep(self.config.get("wait_time"))
        return True


================================================
File: src/unstract/sdk/adapters/x2text/no_op/src/static/json_schema.json
================================================
{
  "title": "No Op Text Extractor",
  "type": "object",
  "required": [
    "adapter_name",
    "wait_time"
  ],
  "description": "No Op Text Extractor does not perform any operation, its used to test the performance of the system in the absence of 3rd party induced latencies",
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this No Op adapter instance. Example: no-op-instance-1"
    },
    "wait_time": {
      "type": "number",
      "title": "Wait time  (in seconds)",
      "default": "0",
      "description": "Provide the time to wait (in seconds) before returning the response"
    }
  }
}


================================================
File: src/unstract/sdk/adapters/x2text/unstructured_community/README.md
================================================
# Unstract UnstructuredIO Community X2Text Adapter


================================================
File: src/unstract/sdk/adapters/x2text/unstructured_community/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-unstructured-io-x2text"
version = "0.0.1"
description = "UnstructuredIO X2Text Adapter"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/x2text/unstructured_community/src/__init__.py
================================================
from .unstructured_community import UnstructuredCommunity

metadata = {
    "name": UnstructuredCommunity.__name__,
    "version": "1.0.0",
    "adapter": UnstructuredCommunity,
    "description": "UnstructuredIO X2Text adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/x2text/unstructured_community/src/unstructured_community.py
================================================
import logging
import os
from typing import Any, Optional

from unstract.sdk.adapters.x2text.dto import TextExtractionResult
from unstract.sdk.adapters.x2text.helper import UnstructuredHelper
from unstract.sdk.adapters.x2text.x2text_adapter import X2TextAdapter
from unstract.sdk.file_storage import FileStorage, FileStorageProvider

logger = logging.getLogger(__name__)


class UnstructuredCommunity(X2TextAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("UnstructuredIOCommunity")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "unstructuredcommunity|eeed506f-1875-457f-9101-846fc7115676"

    @staticmethod
    def get_name() -> str:
        return "Unstructured IO Community"

    @staticmethod
    def get_description() -> str:
        return "Unstructured IO Community X2Text"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/UnstructuredIO.png"

     

    def process(
        self,
        input_file_path: str,
        output_file_path: Optional[str] = None,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
        **kwargs: dict[Any, Any],
    ) -> TextExtractionResult:
        extracted_text: str = UnstructuredHelper.process_document(
            self.config, input_file_path, output_file_path, fs
        )

        return TextExtractionResult(extracted_text=extracted_text)

    def test_connection(self) -> bool:
        result: bool = UnstructuredHelper.test_server_connection(self.config)
        return result


================================================
File: src/unstract/sdk/adapters/x2text/unstructured_community/src/static/json_schema.json
================================================
{
  "title": "Unstructured IO Community Text Extractor",
  "type": "object",
  "required": [
    "adapter_name",
    "url"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: Unstructured IO 1"
    },
    "url": {
      "type": "string",
      "title": "URL",
      "format": "uri",
      "default": "http://unstract-unstructured-io:8000/general/v0/general",
      "description": "Provide the URL of Unstructured IO server."
    }
  }
}


================================================
File: src/unstract/sdk/adapters/x2text/unstructured_enterprise/README.md
================================================
# Unstract UnstructuredIO Enterprise X2Text Adapter


================================================
File: src/unstract/sdk/adapters/x2text/unstructured_enterprise/pyproject.toml
================================================
[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"


[project]
name = "unstract-unstructured-io-x2text"
version = "0.0.1"
description = "UnstructuredIO X2Text Adapter"
authors = [
    {name = "Zipstack Inc.", email = "devsupport@zipstack.com"},
]
dependencies = [
]
requires-python = ">=3.9"
readme = "README.md"
classifiers = [
  "Programming Language :: Python"
]
license = {text = "MIT"}

[tool.pdm.build]
includes = ["src"]
package-dir = "src"
# source-includes = ["tests"]


================================================
File: src/unstract/sdk/adapters/x2text/unstructured_enterprise/src/__init__.py
================================================
from .unstructured_enterprise import UnstructuredEnterprise

metadata = {
    "name": UnstructuredEnterprise.__name__,
    "version": "1.0.0",
    "adapter": UnstructuredEnterprise,
    "description": "UnstructuredIO X2Text adapter",
    "is_active": True,
}


================================================
File: src/unstract/sdk/adapters/x2text/unstructured_enterprise/src/unstructured_enterprise.py
================================================
import logging
import os
from typing import Any, Optional

from unstract.sdk.adapters.x2text.dto import TextExtractionResult
from unstract.sdk.adapters.x2text.helper import UnstructuredHelper
from unstract.sdk.adapters.x2text.x2text_adapter import X2TextAdapter
from unstract.sdk.file_storage import FileStorage, FileStorageProvider

logger = logging.getLogger(__name__)


class UnstructuredEnterprise(X2TextAdapter):
    def __init__(self, settings: dict[str, Any]):
        super().__init__("UnstructuredIOEnterprise")
        self.config = settings

    SCHEMA_PATH = f"{os.path.dirname(__file__)}/static/json_schema.json"

    @staticmethod
    def get_id() -> str:
        return "unstructuredenterprise|eb1b6c58-221f-4db0-a4a5-e5f9cdca44e1"

    @staticmethod
    def get_name() -> str:
        return "Unstructured IO Enterprise"

    @staticmethod
    def get_description() -> str:
        return "Unstructured IO Enterprise X2Text"

    @staticmethod
    def get_icon() -> str:
        return "/icons/adapter-icons/UnstructuredIO.png"

     

    def process(
        self,
        input_file_path: str,
        output_file_path: Optional[str] = None,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
        **kwargs: dict[str, Any],
    ) -> TextExtractionResult:
        extracted_text: str = UnstructuredHelper.process_document(
            self.config, input_file_path, output_file_path, fs
        )

        return TextExtractionResult(extracted_text=extracted_text)

    def test_connection(self) -> bool:
        result: bool = UnstructuredHelper.test_server_connection(self.config)
        return result


================================================
File: src/unstract/sdk/adapters/x2text/unstructured_enterprise/src/static/json_schema.json
================================================
{
  "title": "Unstructured IO Enterprise Text Extractor",
  "type": "object",
  "required": [
    "adapter_name",
    "url",
    "api_key"
  ],
  "properties": {
    "adapter_name": {
      "type": "string",
      "title": "Name",
      "default": "",
      "description": "Provide a unique name for this adapter instance. Example: Unstructured IO 1"
    },
    "url": {
      "type": "string",
      "title": "URL",
      "format": "url",
      "default": "https://api.unstructured.io/general/v0/general",
      "description": "Provide the URL of Unstructured IO server."
    },
    "api_key": {
      "type": "string",
      "title": "API Key",
      "format": "password",
      "default": "",
      "description": "Provide the token (API Key) of the Unstructured Enterprise IO server"
    }
  }
}


================================================
File: src/unstract/sdk/file_storage/__init__.py
================================================
# isort:skip_file
__all__ = [
    "FileStorage",
    "FileStorageProvider",
    "FileStorageHelper",
    "PermanentFileStorage",
    "SharedTemporaryFileStorage",
    "EnvHelper",
    "StorageType",
]

# Do not change the order of the imports below to avoid circular dependency issues

from unstract.sdk.file_storage.constants import StorageType
from unstract.sdk.file_storage.helper import FileStorageHelper
from unstract.sdk.file_storage.impl import FileStorage
from unstract.sdk.file_storage.permanent import PermanentFileStorage
from unstract.sdk.file_storage.provider import FileStorageProvider
from unstract.sdk.file_storage.shared_temporary import (
    SharedTemporaryFileStorage,
)
from unstract.sdk.file_storage.env_helper import EnvHelper


================================================
File: src/unstract/sdk/file_storage/constants.py
================================================
from enum import Enum


class FileOperationParams:
    READ_ENTIRE_LENGTH = -1
    EXTENSION_DEFAULT_READ_LENGTH = 100
    DEFAULT_ENCODING = "utf-8"


class FileSeekPosition:
    START = 0
    CURRENT = 1
    END = 2


class StorageType(Enum):
    PERMANENT = "permanent"
    SHARED_TEMPORARY = "shared_temporary"


class CredentialKeyword:
    PROVIDER = "provider"
    CREDENTIALS = "credentials"


================================================
File: src/unstract/sdk/file_storage/env_helper.py
================================================
import json
import logging
import os

from unstract.sdk.exceptions import FileStorageError
from unstract.sdk.file_storage.constants import CredentialKeyword, StorageType
from unstract.sdk.file_storage.impl import FileStorage
from unstract.sdk.file_storage.permanent import PermanentFileStorage
from unstract.sdk.file_storage.provider import FileStorageProvider
from unstract.sdk.file_storage.shared_temporary import SharedTemporaryFileStorage

logger = logging.getLogger(__name__)


class EnvHelper:
    ENV_CONFIG_FORMAT = (
        '{"provider": "gcs", ' '"credentials": {"token": "/path/to/google/creds.json"}}'
    )

    @staticmethod
    def get_storage(storage_type: StorageType, env_name: str) -> FileStorage:
        """Helper function for clients to pick up remote storage configuration
        from env, initialise the file storage for the same and return the
        instance.

        Args:
            storage_type: Permanent / Temporary file storage
            env_name: Name of the env which has the file storage config

        Returns:
            FileStorage: FIleStorage instance initialised using the provider
            and credentials configured in the env
        """
        try:
            file_storage_creds = json.loads(os.environ.get(env_name, ""))
            provider = FileStorageProvider(
                file_storage_creds[CredentialKeyword.PROVIDER]
            )
            credentials = file_storage_creds.get(CredentialKeyword.CREDENTIALS, {})
            if storage_type == StorageType.PERMANENT:
                file_storage = PermanentFileStorage(provider=provider, **credentials)
            elif storage_type == StorageType.SHARED_TEMPORARY:
                file_storage = SharedTemporaryFileStorage(
                    provider=provider, **credentials
                )
            else:
                raise NotImplementedError()
            return file_storage
        except KeyError as e:
            logger.error(f"Required credentials are missing in the env: {str(e)}")
            logger.error(f"The configuration format is {EnvHelper.ENV_CONFIG_FORMAT}")
            raise e
        except FileStorageError as e:
            raise e


================================================
File: src/unstract/sdk/file_storage/helper.py
================================================
import logging
from typing import Any

import fsspec
from fsspec import AbstractFileSystem

from unstract.sdk.exceptions import FileOperationError, FileStorageError
from unstract.sdk.file_storage.provider import FileStorageProvider

logger = logging.getLogger(__name__)


class FileStorageHelper:
    @staticmethod
    def file_storage_init(
        provider: FileStorageProvider, **storage_config: dict[str, Any]
    ) -> AbstractFileSystem:
        """Initialises file storage based on provider.

        Args:
            provider (FileStorageProvider): Provider
            storage_config : Storage config params based on the provider.
            Sent as-is to the provider implementation.

        Returns:
            NA
        """

        try:
            protocol = provider.value
            if provider == FileStorageProvider.LOCAL:
                # Hard set auto_mkdir to True as default
                storage_config.update({"auto_mkdir": True})  # type: ignore
            elif provider in [FileStorageProvider.MINIO]:
                # Initialise using s3 for Minio
                protocol = FileStorageProvider.S3.value

            fs = fsspec.filesystem(
                protocol=protocol,
                **storage_config,
            )
            logger.debug(f"Connected to {provider.value} file system")
        except KeyError as e:
            logger.error(
                f"Error in initialising {provider.value} "
                f"file system because of missing config {e}"
            )
            raise FileStorageError(str(e)) from e
        except Exception as e:
            logger.error(f"Error in initialising {provider.value} " f"file system {e}")
            raise FileStorageError(str(e)) from e
        return fs

    @staticmethod
    def local_file_system_init() -> AbstractFileSystem:
        """Initialises FileStorage backed up by Local file system.

        Returns:
            NA
        """
        try:
            fs = fsspec.filesystem(protocol=FileStorageProvider.LOCAL.value)
            logger.debug(f"Connected to {FileStorageProvider.LOCAL.value} file system")
            return fs
        except Exception as e:
            logger.error(
                f"Error in initialising {FileStorageProvider.GCS.value}"
                f" file system {e}"
            )
            raise FileStorageError(str(e)) from e


def skip_local_cache(func):
    """Helper function/decorator for handling FileNotFound exception and making
    sure that the error is not because of stale cache.

    Args:
        func: The original function that is called in the context

    Returns:
        NA
    """

    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except FileNotFoundError:
            _handle_file_not_found(func, *args, **kwargs)
        except Exception as e:
            raise FileOperationError(str(e)) from e

    return wrapper


def _handle_file_not_found(func, *args, **kwargs):
    """Helper function for handling FileNotFound exception and making sure that
    the error is not because of stale cache.

    Args:
        func: The original function that is called in the context
        args: The context of the function call as an array
        kwargs: args to the function being called in this context

    Returns:
        NA
    """
    try:
        # FileNotFound could have been caused by stale cache.
        # Hence invalidate cache and retry again
        args[0].fs.invalidate_cache()
        return func(*args, **kwargs)
    except Exception as e:
        if isinstance(e, FileNotFoundError):
            raise e
        else:
            raise FileOperationError(str(e)) from e


================================================
File: src/unstract/sdk/file_storage/impl.py
================================================
import json
import logging
from datetime import datetime
from hashlib import sha256
from typing import Any, Union

import filetype
import fsspec
import magic
import yaml

from unstract.sdk.exceptions import FileOperationError
from unstract.sdk.file_storage.constants import FileOperationParams, FileSeekPosition
from unstract.sdk.file_storage.helper import FileStorageHelper, skip_local_cache
from unstract.sdk.file_storage.interface import FileStorageInterface
from unstract.sdk.file_storage.provider import FileStorageProvider

logger = logging.getLogger(__name__)


class FileStorage(FileStorageInterface):
    # This class integrates fsspec library for file operations

    fs: fsspec  # fsspec file system handle
    provider: FileStorageProvider

    def __init__(self, provider: FileStorageProvider, **storage_config: dict[str, Any]):
        self.fs = FileStorageHelper.file_storage_init(provider, **storage_config)
        self.provider = provider

    @skip_local_cache
    def read(
        self,
        path: str,
        mode: str,
        encoding: str = FileOperationParams.DEFAULT_ENCODING,
        seek_position: int = 0,
        length: int = FileOperationParams.READ_ENTIRE_LENGTH,
    ) -> Union[bytes, str]:
        """Read the file pointed to by the file_handle.

        Args:
            path (str): Path to the file to be opened
            mode (str): Mode in which the file is to be opened. Usual options
                        include r, rb, w and wb
            encoding (str): Encoding type like utf-8 or utf-16
            seek_position (int): Position to start reading from
            length (int): Number of bytes to be read. Default is full
            file content.

        Returns:
            Union[bytes, str] - File contents in bytes/string based on the opened mode
        """
        with self.fs.open(path=path, mode=mode, encoding=encoding) as file_handle:
            if seek_position > 0:
                file_handle.seek(seek_position)
            return file_handle.read(length)

    def write(
        self,
        path: str,
        mode: str,
        encoding: str = FileOperationParams.DEFAULT_ENCODING,
        seek_position: int = 0,
        data: Union[bytes, str] = "",
    ) -> int:
        """Write data in the file pointed to by the file-handle.

        Args:
            path (str): Path to the file to be opened
            mode (str): Mode in whicg the file is to be opened. Usual options
                        include r, rb, w and wb
            encoding (str): Encoding type like utf-8 or utf-16
            seek_position (int): Position to start writing from
            data (Union[bytes, str]): Contents to be written

        Returns:
            int: Number of bytes that were successfully written to the file
        """
        try:
            with self.fs.open(path=path, mode=mode, encoding=encoding) as file_handle:
                return file_handle.write(data)
        except Exception as e:
            raise FileOperationError(str(e)) from e

    @skip_local_cache
    def seek(
        self,
        path: str,
        location: int = 0,
        position: FileSeekPosition = FileSeekPosition.START,
    ) -> int:
        """Place the file pointer to the mentioned location in the file
        relative to the position.

        Args:
            path (str): path of the file
            location (int): Nth byte position. To be understood in relation to
            the arg "position"
            position (FileSeekPosition): from start of file, current location
            or end of file

        Returns:
            int: file pointer location after seeking to the mentioned position
        """
        with self.fs.open(path=path, mode="rb") as file_handle:
            return file_handle.seek(location, position)

    def mkdir(self, path: str, create_parents: bool = True):
        """Create a directory.

        Args:
            path (str): Path of the directory to be created
            create_parents (bool): Specify if parent directories to be created
            if any of the nested directory does not exist
        """
        try:
            self.fs.mkdir(path=path, create_parents=create_parents)
        except FileExistsError:
            logger.debug(f"Path {path} already exists.")
        except Exception as e:
            raise FileOperationError(str(e)) from e

    @skip_local_cache
    def exists(self, path: str) -> bool:
        """Checks if a file/directory path exists.

        Args:
            path (str): File/directory path

        Returns:
            bool: If the file/directory  exists or not
        """
        try:
            return self.fs.exists(path)
        except Exception as e:
            raise FileOperationError(str(e)) from e

    @skip_local_cache
    def ls(self, path: str) -> list[str]:
        """List the directory path.

        Args:
            path (str): Directory path

        Returns:
            List[str]: List of files / directories under the path
        """
        return self.fs.ls(path)

    @skip_local_cache
    def rm(self, path: str, recursive: bool = True):
        """Removes a file or directory mentioned in path.

        Args:
            path (str): Path to the file / directory
            recursive (bool): Whether the files and folders nested
            under path are to be removed or not

        Returns:
            NA
        """
        return self.fs.rm(path=path, recursive=recursive)

    @skip_local_cache
    def cp(
        self,
        src: str,
        dest: str,
        recursive: bool = False,
        overwrite: bool = True,
    ):
        """Copies files from source(lpath) path to the destination(rpath) path.

        Args:
            src (str): Path to the source
            dest (str): Path to the destination
            recursive (bool): Copy recursively when set to True
            overwrite (bool): Overwrite existing path with same name

        Returns:
            NA
        """
        return self.fs.cp(src, dest, recursive=recursive, overwrite=overwrite)

    @skip_local_cache
    def size(self, path: str) -> int:
        """Get the size of the file specified in path.

        Args:
            path (str): Path to the file

        Returns:
            int: Size of the file in bytes
        """
        file_info = self.fs.info(path)
        return file_info["size"]

    @skip_local_cache
    def modification_time(self, path: str) -> datetime:
        """Get the last modification time of the file specified in path.

        Args:
            path (str): Path to the file

        Returns:
            datetime: Last modified time in datetime
        """
        file_info = self.fs.info(path)
        file_mtime = file_info["mtime"]
        if not isinstance(file_mtime, datetime):
            file_mtime = datetime.fromtimestamp(file_mtime)
        return file_mtime

    def mime_type(
        self,
        path: str,
        read_length: int = FileOperationParams.READ_ENTIRE_LENGTH,
    ) -> str:
        """Gets the file MIME type for an input file. Uses libmagic to perform
        the same.

        Args:
            path (str): Path of the input file
            read_length (int): Length(bytes) to be read from the file for in
            order to identify the mime type. Defaults to read the entire length.

        Returns:
            str: MIME type of the file
        """
        sample_contents = self.read(path=path, mode="rb", length=read_length)
        mime_type = magic.from_buffer(sample_contents, mime=True)
        return mime_type

    @skip_local_cache
    def download(self, from_path: str, to_path: str):
        """Downloads the file mentioned in from_path to to_path on the local
        system. The instance calling the method needs to be the FileStorage
        initialised with the remote file system.

        Args:
            from_path (str): Path of the file to be downloaded (remote)
            to_path (str): Path where the file is to be downloaded
            on local system

        Returns:
            NA
        """
        self.fs.get(rpath=from_path, lpath=to_path)

    @skip_local_cache
    def upload(self, from_path: str, to_path: str):
        """Uploads the file mentioned in from_path (local system) to to_path
        (remote system). The instance calling the method needs to be the
        FileStorage initialised with the remote file system where the file
        needs to be uploaded.

        Args:
            from_path (str): Path of the file to be uploaded (local)
            to_path (str): Path where the file is to be uploaded (usually remote)

        Returns:
            NA
        """
        self.fs.put(from_path, to_path)

    def glob(self, path: str) -> list[str]:
        """Lists files under path matching the pattern sepcified as part of
        path in the argument.

        Args:
            path (str): path to the directory where files matching the
            specified pattern is to be found
            Eg. a/b/c/*.txt will list all txt files under a/b/c/

        Returns:
            list[str]: List of file names matching any pattern specified
        """
        try:
            return self.fs.glob(path)
        except Exception as e:
            raise FileOperationError(str(e)) from e

    @skip_local_cache
    def get_hash_from_file(self, path: str) -> str:
        """Computes the hash for a file.

        Uses sha256 to compute the file hash through a buffered read.

        Args:
            file_path (str): Path to file that needs to be hashed

        Returns:
            str: SHA256 hash of the file
        """

        h = sha256()
        b = bytearray(128 * 1024)
        mv = memoryview(b)
        with self.fs.open(path) as f:
            while n := f.readinto(mv):
                h.update(mv[:n])
        return str(h.hexdigest())

    def json_dump(
        self,
        path: str,
        data: dict[str, Any],
        **kwargs: dict[Any, Any],  # type: ignore
    ):
        """Dumps data into the given file specified by path.

        Args:
            path (str): Path to file where JSON is to be dumped
            data (dict): Object to be written to the file
            **kwargs (dict): Any other additional arguments
        """
        try:
            with self.fs.open(path=path, mode="w", encoding="utf-8") as f:
                json.dump(obj=data, fp=f, **kwargs)  # type: ignore
        except Exception as e:
            raise FileOperationError(str(e)) from e

    def yaml_dump(
        self,
        path: str,
        data: dict[str, Any],
        **kwargs: dict[Any, Any],  # type: ignore
    ):
        """Dumps data into the given file specified by path.

        Args:
            path (str): Path to file where yml is to be dumped
            data (dict): Object to be written to the file
            **kwargs (dict): Any other additional arguments
        """
        try:
            with self.fs.open(path=path, mode="w", encoding="utf-8") as f:
                yaml.dump(data=data, stream=f, **kwargs)  # type: ignore
        except Exception as e:
            raise FileOperationError(str(e)) from e

    @skip_local_cache
    def json_load(self, path: str) -> dict[Any, Any]:
        with self.fs.open(path=path) as json_file:
            data: dict[str, Any] = json.load(json_file)
            return data

    @skip_local_cache
    def yaml_load(
        self,
        path: str,
    ) -> dict[Any, Any]:
        """Loads data from a file as yaml.

        Args:
            path (str): Path to file where yml is to be loaded

        Returns:
            dict[Any, Any]: Data loaded as yaml
        """
        with self.fs.open(path=path) as f:
            data: dict[str, Any] = yaml.safe_load(f)
            return data

    def guess_extension(self, path: str) -> str:
        """Returns the extension of the file passed.

        Args:
            path (str): String holding the path

        Returns:
            str: File extension
        """
        file_extension = ""
        sample_contents = self.read(
            path=path,
            mode="rb",
            length=FileOperationParams.EXTENSION_DEFAULT_READ_LENGTH,
        )
        if sample_contents:
            file_type = filetype.guess(sample_contents)
            file_extension = file_type.EXTENSION
        return file_extension

    def walk(self, path: str, max_depth=None, topdown=True):
        """Walks the dir in the path and returns the list of files/dirs.

        Args:
            path (str): Root to recurse into
            maxdepth (int): Maximum recursion depth. None means limitless,
            but not recommended
                on link-based file-systems.
            topdown (bool): Whether to walk the directory tree from the top
            downwards or from
                the bottom upwards.

        Returns:
            Iterator containing the list of files and folders
        """
        # Invalidating cache explicitly to avoid any stale listing
        self.fs.invalidate_cache(path=path)
        return self.fs.walk(path, maxdepth=max_depth, topdown=topdown)


================================================
File: src/unstract/sdk/file_storage/interface.py
================================================
from abc import ABC, abstractmethod
from datetime import datetime
from typing import Any, Union

from fsspec import AbstractFileSystem

from unstract.sdk.file_storage.constants import FileOperationParams, FileSeekPosition


class FileStorageInterface(ABC):
    @abstractmethod
    def read(
        self,
        path: str,
        mode: str,
        encoding: str = FileOperationParams.DEFAULT_ENCODING,
        seek_position: int = 0,
        length: int = FileOperationParams.READ_ENTIRE_LENGTH,
    ) -> Union[bytes, str]:
        pass

    @abstractmethod
    def write(
        self,
        path: str,
        mode: str,
        encoding: str = FileOperationParams.DEFAULT_ENCODING,
        seek_position: int = 0,
        data: Union[bytes, str] = "",
    ) -> int:
        pass

    @abstractmethod
    def seek(
        self,
        file_handle: Union[AbstractFileSystem],
        location: int = 0,
        position: FileSeekPosition = FileSeekPosition.START,
    ) -> int:
        pass

    @abstractmethod
    def mkdir(self, path: str, create_parents: bool):
        pass

    @abstractmethod
    def exists(self, path: str) -> bool:
        pass

    @abstractmethod
    def ls(self, path: str) -> list[str]:
        pass

    @abstractmethod
    def rm(self, path: str, recursive: bool = True):
        pass

    @abstractmethod
    def cp(
        self,
        lpath: str,
        rpath: str,
        recursive: bool = False,
        overwrite: bool = True,
    ):
        pass

    @abstractmethod
    def size(self, path: str) -> int:
        pass

    @abstractmethod
    def modification_time(self, path: str) -> datetime:
        pass

    @abstractmethod
    def mime_type(
        self,
        path: str,
        read_length: int = FileOperationParams.READ_ENTIRE_LENGTH,
    ) -> str:
        pass

    @abstractmethod
    def download(self, from_path: str, to_path: str):
        pass

    @abstractmethod
    def glob(self, path: str) -> list[str]:
        pass

    @abstractmethod
    def get_hash_from_file(self, path: str) -> str:
        pass

    @abstractmethod
    def json_dump(
        self,
        path: str,
        data: dict[str, Any],
        **kwargs: dict[Any, Any],
    ):
        pass

    @abstractmethod
    def yaml_dump(
        self,
        path: str,
        data: dict[str, Any],
        **kwargs: dict[Any, Any],
    ):
        pass

    @abstractmethod
    def json_load(self, path: str) -> dict[Any, Any]:
        pass

    @abstractmethod
    def yaml_load(
        self,
        path: str,
    ) -> dict[Any, Any]:
        pass

    @abstractmethod
    def guess_extension(self, path: str) -> str:
        pass

    @abstractmethod
    def walk(self, path: str):
        pass


================================================
File: src/unstract/sdk/file_storage/permanent.py
================================================
import logging
from typing import Any, Optional, Union

import filetype
import magic

from unstract.sdk.exceptions import FileOperationError, FileStorageError
from unstract.sdk.file_storage.constants import FileOperationParams
from unstract.sdk.file_storage.impl import FileStorage
from unstract.sdk.file_storage.provider import FileStorageProvider

logger = logging.getLogger(__name__)


class PermanentFileStorage(FileStorage):
    SUPPORTED_FILE_STORAGE_TYPES = [
        FileStorageProvider.GCS.value,
        FileStorageProvider.S3.value,
        FileStorageProvider.MINIO.value,
        FileStorageProvider.LOCAL.value,
        FileStorageProvider.AZURE.value,
    ]

    def __init__(
        self,
        provider: FileStorageProvider,
        **storage_config: dict[str, Any],
    ):
        if provider.value not in self.SUPPORTED_FILE_STORAGE_TYPES:
            raise FileStorageError(
                f"File storage provider `{provider.value}` is not "
                f"supported in Permanent mode. "
                f"Supported providers: {self.SUPPORTED_FILE_STORAGE_TYPES}"
            )
        if (
            provider == FileStorageProvider.GCS
            or provider == FileStorageProvider.LOCAL
            or provider == FileStorageProvider.MINIO
            or provider == FileStorageProvider.S3
            or provider == FileStorageProvider.AZURE
        ):
            super().__init__(provider, **storage_config)
        else:
            raise NotImplementedError(f"Provider {provider.value} is not implemented")

    def _copy_on_read(self, path: str, legacy_storage_path: str):
        """Copies the file to the remote storage lazily if not present already.
        Checks if the file is present in the Local File system. If yes, copies
        the file to the mentioned path using the remote file system. This is a
        silent copy done on need basis.

        Args:
            path (str): Path to the file
            legacy_storage_path (str): Legacy path to the same file

        Returns:
            NA
        """
        # If path does not exist on remote storage
        if not self.exists(path):
            local_file_storage = FileStorage(provider=FileStorageProvider.LOCAL)
            local_file_path = legacy_storage_path
            # If file exists on local storage, then migrate the file
            # to remote storage
            if local_file_storage.exists(local_file_path):
                self.upload(local_file_path, path)
                logger.info(
                    f"Uploading {local_file_path} from "
                    f"{local_file_storage.provider} to remote "
                    f"storage {self.provider} in the path {path}"
                )

    def read(
        self,
        path: str,
        mode: str,
        encoding: str = FileOperationParams.DEFAULT_ENCODING,
        seek_position: int = 0,
        length: int = FileOperationParams.READ_ENTIRE_LENGTH,
        legacy_storage_path: Optional[str] = None,
    ) -> Union[bytes, str]:
        """Read the file pointed to by the file_handle.

        Args:
            path (str): Path to the file to be opened
            mode (str): Mode in whicg the file is to be opened. Usual options
                        include r, rb, w and wb
            encoding (str): Encoding type like utf-8 or utf-16
            seek_position (int): Position to start reading from
            length (int): Number of bytes to be read. Default is full
            file content.
            legacy_storage_path (str):  Legacy path to the same file

        Returns:
            Union[bytes, str] - File contents in bytes/string based on the opened mode
        """
        try:
            # Lazy copy to the destination/remote file system
            if legacy_storage_path:
                self._copy_on_read(path, legacy_storage_path)
            return super().read(path, mode, encoding, seek_position, length)
        except Exception as e:
            if isinstance(e, FileNotFoundError) or isinstance(e, FileOperationError):
                raise e
            raise FileOperationError(str(e)) from e

    def mime_type(
        self,
        path: str,
        read_length: int = FileOperationParams.READ_ENTIRE_LENGTH,
        legacy_storage_path: Optional[str] = None,
    ) -> str:
        """Gets the file MIME type for an input file. Uses libmagic to perform
        the same.

        Args:
            path (str): Path of the input file
            read_length (int): Length(bytes) to be read from the file for in
            order to identify the mime type. Defaults to read the entire length.
            legacy_storage_path (str):  Legacy path to the same file

        Returns:
            str: MIME type of the file
        """
        sample_contents = self.read(
            path=path,
            mode="rb",
            length=read_length,
            legacy_storage_path=legacy_storage_path,
        )
        mime_type = magic.from_buffer(sample_contents, mime=True)
        return mime_type

    def guess_extension(
        self, path: str, legacy_storage_path: Optional[str] = None
    ) -> str:
        """Returns the extension of the file passed.

        Args:
            path (str): String holding the path
            legacy_storage_path (str):  Legacy path to the same file

        Returns:
            str: File extension
        """
        file_extension = ""
        sample_contents = self.read(
            path=path,
            mode="rb",
            length=FileOperationParams.EXTENSION_DEFAULT_READ_LENGTH,
            legacy_storage_path=legacy_storage_path,
        )
        if sample_contents:
            file_type = filetype.guess(sample_contents)
            file_extension = file_type.EXTENSION
        return file_extension


================================================
File: src/unstract/sdk/file_storage/provider.py
================================================
import enum


class FileStorageProvider(enum.Enum):
    AZURE = "abfs"
    GCS = "gcs"
    S3 = "s3"
    MINIO = "minio"
    REDIS = "redis"
    LOCAL = "local"


================================================
File: src/unstract/sdk/file_storage/shared_temporary.py
================================================
from typing import Any

from unstract.sdk.exceptions import FileStorageError
from unstract.sdk.file_storage import FileStorage, FileStorageProvider


class SharedTemporaryFileStorage(FileStorage):
    SUPPORTED_FILE_STORAGE_TYPES = [
        FileStorageProvider.MINIO.value,
        FileStorageProvider.REDIS.value,
    ]

    def __init__(
        self,
        provider: FileStorageProvider,
        **storage_config: dict[str, Any],
    ):
        if provider.value not in self.SUPPORTED_FILE_STORAGE_TYPES:
            raise FileStorageError(
                f"File storage provider is not supported in Shared Temporary mode. "
                f"Supported providers: {self.SUPPORTED_FILE_STORAGE_TYPES}"
            )
        if provider == FileStorageProvider.MINIO:
            super().__init__(provider, **storage_config)
        elif provider == FileStorageProvider.REDIS:
            super().__init__(provider)
        else:
            raise NotImplementedError


================================================
File: src/unstract/sdk/scripts/tool_gen.py
================================================
#!/usr/bin/env python
import argparse
import shutil
from importlib.resources import files
from pathlib import Path


def new_tool(args):
    tool_name = args.tool_name
    if tool_name is None:
        print("Tool name is required")
        exit(1)
    location = args.location
    if location is None:
        print("Location is required")
        exit(1)
    overwrite = args.overwrite
    print(f"Creating new tool {tool_name} at {location}")
    # Check if folder exists
    folder = Path(location).joinpath(tool_name)
    if folder.exists():
        if overwrite:
            print("Folder exists, overwriting")
        else:
            print("Folder exists, exiting")
            exit(1)
    else:
        folder.mkdir(parents=True, exist_ok=True)

    source = Path(files("unstract.sdk").joinpath("static/tool_template/v1/"))
    print(f"Copying files from {source} to {folder}")
    # Copy all files in source to folder, recursively
    shutil.copytree(source, folder, dirs_exist_ok=True)


def main() -> None:
    parser = argparse.ArgumentParser(
        prog="Unstract tool generator",
        description="Script to generate a new Unstract tool",
        epilog="Unstract SDK",
    )
    parser.add_argument(
        "--command", type=str, help="Command to execute", required=True
    )
    parser.add_argument(
        "--tool-name", type=str, help="Tool name", required=False
    )
    parser.add_argument(
        "--location",
        type=str,
        help="Director to create the new tool in",
        required=False,
    )
    parser.add_argument(
        "--overwrite",
        help="Overwrite existing tool",
        required=False,
        default=False,
        action="store_true",
    )
    args = parser.parse_args()
    command = str.upper(args.command)

    if command == "NEW":
        try:
            new_tool(args)
        except Exception as e:
            print(f"Error creating new tool: {e}")
            exit(1)
        print("New tool created successfully")
    else:
        print("Command not supported")
        exit(1)


if __name__ == "__main__":
    main()


================================================
File: src/unstract/sdk/static/tool_template/v1/README.md
================================================
# Your Tool Name

_TODO: Add the description for your tool_

## Required environment variables

| Variable                   | Description                                                           |
| -------------------------- | --------------------------------------------------------------------- |
| `PLATFORM_SERVICE_HOST`    | The host in which the platform service is running                     |
| `PLATFORM_SERVICE_PORT`    | The port in which the service is listening                            |
| `PLATFORM_SERVICE_API_KEY` | The API key for the platform                                          |
| `TOOL_DATA_DIR`            | The directory in the filesystem which has contents for tool execution |

_TODO: Add more variables here if required_

## Testing the tool locally

### Setting up a dev environment

Setup a virtual environment and activate it

```commandline
python -m venv .venv
source .venv/bin/activate
```

Install the dependencies for the tool

```commandline
pip install -r requirements.txt
```

To use the local development version of the [unstract-sdk](https://pypi.org/project/unstract-sdk/) install it from the local repository.
Replace the path with the path to your local repository

```commandline
pip install -e ~/path_to_repo/sdks/.
```

### Tool execution preparation

Load the environment variables for the tool.
Make a copy of the `sample.env` file and name it `.env`. Fill in the required values.
They get loaded with [python-dotenv](https://pypi.org/project/python-dotenv/) through the SDK.

Update the tool's `data_dir` marked by the `TOOL_DATA_DIR` env. This has to be done before each tool execution since the tool updates the `INFILE` and `METADATA.json`.

### Run SPEC command

Represents the JSON schema for the runtime configurable `settings` of a tool

```commandline
python main.py --command SPEC
```

### Run PROPERTIES command

Describes some metadata for the tool such as its `version`, `description`, `inputs` and `outputs`

```commandline
python main.py --command PROPERTIES
```

### Run ICON command

Returns the SVG icon for the tool, used by Unstract's frontend

```commandline
python main.py --command ICON
```

### Run VARIABLES command

Represents the runtime variables or envs that will be used by the tool

```commandline
python main.py --command VARIABLES
```

### Run RUN command

_TODO: Update the example below with the correct parameters_

The schema of the JSON required for settings can be found by running the [SPEC](#run-spec-command) command. Alternatively if you have access to the code base, it is located in the `config` folder as `spec.json`.

```commandline
python main.py \
    --command RUN \
    --settings '{
        }' \
    --log-level DEBUG

```

## Testing the tool from its docker image

Build the tool docker image from the folder containing the `Dockerfile` with

```commandline
docker build -t unstract/tool-example:0.0.1 .
```

Make sure the directory pointed by `TOOL_DATA_DIR` has the required information for the tool to run and
necessary services like the `unstract-platform-service` is up.
To test the tool from its docker image, run the following command

```commandline
docker run -it \
    --network unstract-network \
    --env-file .env \
    -v "$(pwd)"/data_dir:/app/data_dir \
    unstract/tool-example:0.0.1 \
    --command RUN \
    --settings '{
        }' \
    --log-level DEBUG

```


================================================
File: src/unstract/sdk/static/tool_template/v1/Dockerfile
================================================
FROM python:3.9-slim

LABEL maintainer="Zipstack Inc."
ENV UNSTRACT_ENTRYPOINT "python /app/src/main.py"

# Set the working directory in the container
WORKDIR /app
COPY requirements.txt /app/
RUN pip install --no-cache-dir -r requirements.txt
# Copy the contents of your project directory into the container at /app
COPY src /app/src/
WORKDIR /app/src


ENTRYPOINT ["python", "main.py"]


================================================
File: src/unstract/sdk/static/tool_template/v1/requirements.txt
================================================
# Add your dependencies here

# Required for all unstract tools
unstract-sdk~=0.0 # Select a version based on your need


================================================
File: src/unstract/sdk/static/tool_template/v1/sample.env
================================================
PLATFORM_SERVICE_HOST=http://unstract-platform-service
PLATFORM_SERVICE_PORT=3001
PLATFORM_SERVICE_API_KEY=<add_platform_key_from_Unstract_frontend>
TOOL_DATA_DIR=../data_dir


================================================
File: src/unstract/sdk/static/tool_template/v1/.dockerignore
================================================
venv/
.venv/
.env


================================================
File: src/unstract/sdk/static/tool_template/v1/data_dir/INFILE
================================================
This is an example INFILE that can be of any type. This file serves as an input to the tool that can be used to carry out its functionality.
At the end of tool execution, the tool result gets written back to the INFILE. In the context of workflow execution, this helps the
next tool in the workflow carry out its desired task.


================================================
File: src/unstract/sdk/static/tool_template/v1/data_dir/METADATA.json
================================================
{
    "source_name": "source_file.txt",
    "source_hash": "xxx",
    "workflow_id": "00000000-0000-0000-0000-000000000000",
    "execution_id": "00000000-0000-0000-0000-000000000000",
    "organization_id": "00000000-0000-0000-0000-000000000000"
}


================================================
File: src/unstract/sdk/static/tool_template/v1/data_dir/SOURCE
================================================
This is an example SOURCE file which can be of any file type. This file is practically the input to a workflow and should never be
modified by a tool, however it can be read by a tool to carry out its functionality.


================================================
File: src/unstract/sdk/static/tool_template/v1/src/main.py
================================================
import sys
from typing import Any

from unstract.sdk.tool.base import BaseTool
from unstract.sdk.tool.entrypoint import ToolEntrypoint


# TODO: Rename tool's class
class ConcreteTool(BaseTool):
    # TODO: Add any checks that need to be done before running the tool
    def validate(self, input_file: str, settings: dict[str, Any]) -> None:
        pass

    def run(
        self,
        settings: dict[str, Any],
        input_file: str,
        output_dir: str,
    ) -> None:
        # -------------- TODO: Add your code here ----------------
        # 1. Read the input_file
        # 2. Process on its contents
        # 3. Write files to the output_dir which need to be copied to the
        #    destination.
        # 4. Write the tool result TEXT or JSON
        # TODO: Write tool result of dict or str
        self.write_tool_result(data={})


if __name__ == "__main__":
    args = sys.argv[1:]
    # TODO: Rename tool's class
    tool = ConcreteTool.from_tool_args(args=args)
    ToolEntrypoint.launch(tool=tool, args=args)


================================================
File: src/unstract/sdk/static/tool_template/v1/src/config/properties.json
================================================
{
  "schemaVersion": "0.0.1",
  "displayName": "<TODO: Enter the display name of the tool>",
  "functionName": "<TODO: Enter the unique function name of the tool>",
  "toolVersion": "0.0.1",
  "description": "<TODO: Enter the tool's description, which will be fed into an LLM for workflow generation>",
  "input": {
    "description": "<TODO: Description of the input file to a tool>"
  },
  "output": {
    "description": "<TODO: Description of what the tool outputs and requires to be copied to a destination>"
  },
  "result": {
    "type": "<TODO: The result type returned from a tool, can be one of JSON/TXT>",
    "description": "<TODO: Description of what the tool returns>",
    "schema": {}
  },
  "adapter": {
    "languageModels": [
      {
        "isEnabled": false,
        "adapterId": "<TODO: The expected key for adapter id in tool settings <Optional: Default: llmAdapterId>>",
        "title": "<TODO: The title to be expected in tool settings>",
        "isRequired": false,
        "description": "<TODO: The description for adapter>"
      }
    ],
    "embeddingServices": [
      {
        "adapterId": "<TODO: The expected key for adapter id in tool settings <Optional: Default: embeddingAdapterId>>",
        "isEnabled": false
      }
    ],
    "vectorStores": [
      {
        "adapterId": "<TODO: The expected key for adapter id in tool settings <Optional: Default: vectorDbAdapterId>>",
        "isEnabled": false
      }
    ]
  },
  "ioCompatibility": {
    "api": {
      "sourceSupport": true,
      "destinationSupport": true,
      "additionalArgs": {
        "sync": true
      }
    },
    "file": {
      "sourceSupport": true,
      "destinationSupport": true,
      "additionalArgs": {}
    },
    "db": {
      "destinationSupport": true,
      "additionalArgs": {}
    }
  },
  "restrictions": {
    "maxFileSize": "<TODO: Allowed input file size it can process in B, KB, MB or GB. For eg: 10MB>",
    "allowedFileTypes": [
      "<TODO: List of allowed input file types it can process, * represents all types. For eg: pdf, txt.etc.>"
    ]
  }
}


================================================
File: src/unstract/sdk/static/tool_template/v1/src/config/spec.json
================================================
{
  "title": "<TODO: Add title>",
  "description": "<TODO: Add description>",
  "type": "object",
  "required": [
  ],
  "properties": {
  }
}


================================================
File: src/unstract/sdk/tool/base.py
================================================
import datetime
import os
from abc import ABC, abstractmethod
from json import JSONDecodeError
from pathlib import Path
from typing import Any, Union

from unstract.sdk.constants import (
    Command,
    LogLevel,
    MetadataKey,
    PropKey,
    ToolEnv,
    ToolExecKey,
)
from unstract.sdk.exceptions import FileStorageError
from unstract.sdk.file_storage import EnvHelper, StorageType
from unstract.sdk.tool.mixin import ToolConfigHelper
from unstract.sdk.tool.parser import ToolArgsParser
from unstract.sdk.tool.stream import StreamMixin
from unstract.sdk.utils import ToolUtils


class BaseTool(ABC, StreamMixin):
    """Abstract class for Unstract tools."""

    def __init__(self, log_level: LogLevel = LogLevel.INFO) -> None:
        """Creates an UnstractTool.

        Args:
            log_level (str): Log level for the tool
                Can be one of INFO, DEBUG, WARN, ERROR, FATAL.
        """
        self.start_time = datetime.datetime.now()
        super().__init__(log_level=log_level)
        self.properties = ToolConfigHelper.properties()
        self.spec = ToolConfigHelper.spec()
        self.variables = ToolConfigHelper.variables()
        self.workflow_id = ""
        self.execution_id = ""
        self.file_execution_id = ""
        self.tags = []
        self.source_file_name = ""
        self.org_id = ""
        self._exec_metadata = {}
        self.filestorage_provider = None
        self.workflow_filestorage = None
        self.execution_dir = None
        filestorage_env = os.environ.get(
            ToolEnv.WORKFLOW_EXECUTION_FILE_STORAGE_CREDENTIALS
        )
        if filestorage_env:
            self.execution_dir = Path(self.get_env_or_die(ToolEnv.EXECUTION_DATA_DIR))

            try:
                self.workflow_filestorage = EnvHelper.get_storage(
                    StorageType.SHARED_TEMPORARY,
                    ToolEnv.WORKFLOW_EXECUTION_FILE_STORAGE_CREDENTIALS,
                )
            except KeyError as e:
                self.stream_error_and_exit(
                    f"Required credentials is missing in the env: {str(e)}"
                )
            except FileStorageError as e:
                self.stream_error_and_exit(
                    "Error while initialising storage: %s",
                    e,
                    stack_info=True,
                    exc_info=True,
                )

    @classmethod
    def from_tool_args(cls, args: list[str]) -> "BaseTool":
        """Builder method to create a tool from args passed to a tool.

        Refer the tool's README to know more about the possible args

        Args:
            args (List[str]): Arguments passed to a tool

        Returns:
            AbstractTool: Abstract base tool class
        """
        parsed_args = ToolArgsParser.parse_args(args)
        tool = cls(log_level=parsed_args.log_level)
        if parsed_args.command not in Command.static_commands():
            tool._exec_metadata = tool._get_exec_metadata()
            tool.workflow_id = tool._exec_metadata.get(MetadataKey.WORKFLOW_ID)
            tool.execution_id = tool._exec_metadata.get(MetadataKey.EXECUTION_ID, "")
            tool.file_execution_id = tool._exec_metadata.get(
                MetadataKey.FILE_EXECUTION_ID, ""
            )
            tool.tags = tool._exec_metadata.get(MetadataKey.TAGS, [])
            tool.source_file_name = tool._exec_metadata.get(MetadataKey.SOURCE_NAME, "")
            tool.org_id = tool._exec_metadata.get(MetadataKey.ORG_ID)
        return tool

    def elapsed_time(self) -> float:
        """Returns the elapsed time since the tool was created."""
        return (datetime.datetime.now() - self.start_time).total_seconds()

    def handle_static_command(self, command: str) -> None:
        """Handles a static command.

        Used to handle commands that do not require any processing. Currently,
        the only supported static commands are
        SPEC, PROPERTIES, VARIABLES and ICON.

        This is used by the Unstract SDK to handle static commands.
        It is not intended to be used by the tool. The tool
        stub will automatically handle static commands.

        Args:
            command (str): The static command.
        Returns:
            None
        """
        if command == Command.SPEC:
            self.stream_spec(ToolUtils.json_to_str(self.spec))
        elif command == Command.PROPERTIES:
            self.stream_properties(ToolUtils.json_to_str(self.properties))
        elif command == Command.ICON:
            self.stream_icon(ToolConfigHelper.icon())
        elif command == Command.VARIABLES:
            self.stream_variables(ToolUtils.json_to_str(self.variables))
        else:
            raise ValueError(f"Unknown command {command}")

    def _get_file_from_data_dir(self, file_to_get: str, raise_err: bool = False) -> str:
        base_path = self.execution_dir
        file_path = base_path / file_to_get
        if raise_err and not self.workflow_filestorage.exists(path=file_path):
            self.stream_error_and_exit(
                f"{file_to_get} is missing in EXECUTION_DATA_DIR"
            )

        return str(file_path)

    def get_source_file(self) -> str:
        """Gets the absolute path to the workflow execution's input file
        (SOURCE).

        Returns:
            str: Absolute path to the source file
        """
        return self._get_file_from_data_dir(ToolExecKey.SOURCE, raise_err=True)

    def get_input_file(self) -> str:
        """Gets the absolute path to the input file that's meant for the tool
        being run (INFILE).

        Returns:
            str: Absolute path to the input file
        """
        return self._get_file_from_data_dir(ToolExecKey.INFILE, raise_err=True)

    def get_output_dir(self) -> str:
        """Get the absolute path to the output folder where the tool needs to
        place its output file. This is where the tool writes its output files
        that need to be copied into the destination (COPY_TO_FOLDER path).

        Returns:
            str: Absolute path to the output directory.
        """
        base_path = self.execution_dir
        return str(base_path / ToolExecKey.OUTPUT_DIR)

    @property
    def get_exec_metadata(self) -> dict[str, Any]:
        """Getter for `exec_metadata` of the tool.

        Returns:
            dict[str, Any]: Contents of METADATA.json
        """
        return self._exec_metadata

    def _get_exec_metadata(self) -> dict[str, Any]:
        """Retrieve the contents from METADATA.json present in the data
        directory. This file contains metadata for the tool execution.

        Returns:
            dict[str, Any]: Contents of METADATA.json
        """
        base_path = self.execution_dir
        metadata_path = base_path / ToolExecKey.METADATA_FILE
        metadata_json = {}
        try:
            metadata_json = ToolUtils.load_json(
                file_to_load=metadata_path, fs=self.workflow_filestorage
            )
        except JSONDecodeError as e:
            self.stream_error_and_exit(f"JSON decode error for {metadata_path}: {e}")
        except FileNotFoundError:
            self.stream_error_and_exit(f"Metadata file not found at {metadata_path}")
        except OSError as e:
            self.stream_error_and_exit(f"OS Error while opening {metadata_path}: {e}")
        return metadata_json

    def _write_exec_metadata(self, metadata: dict[str, Any]) -> None:
        """Helps write the `METADATA.JSON` file.

        Args:
            metadata (dict[str, Any]): Metadata to write
        """
        base_path = self.execution_dir
        metadata_path = base_path / ToolExecKey.METADATA_FILE
        ToolUtils.dump_json(
            file_to_dump=metadata_path,
            json_to_dump=metadata,
            fs=self.workflow_filestorage,
        )

    def _update_exec_metadata(self) -> None:
        """Updates the execution metadata after a tool executes.

        Currently overrwrites most of the keys with the latest tool
        executed.
        """
        tool_metadata = {
            MetadataKey.TOOL_NAME: self.properties[PropKey.FUNCTION_NAME],
            MetadataKey.ELAPSED_TIME: self.elapsed_time(),
            MetadataKey.OUTPUT_TYPE: self.properties[PropKey.RESULT][PropKey.TYPE],
        }
        if MetadataKey.TOTAL_ELA_TIME not in self._exec_metadata:
            self._exec_metadata[MetadataKey.TOTAL_ELA_TIME] = self.elapsed_time()
        else:
            self._exec_metadata[MetadataKey.TOTAL_ELA_TIME] += self.elapsed_time()

        if MetadataKey.TOOL_META not in self._exec_metadata:
            self._exec_metadata[MetadataKey.TOOL_META] = [tool_metadata]
        else:
            self._exec_metadata[MetadataKey.TOOL_META].append(tool_metadata)

        self._write_exec_metadata(metadata=self._exec_metadata)

    def update_exec_metadata(self, metadata: dict[str, Any]) -> None:
        """Helps update the execution metadata with the provided metadata
        dictionary.

        This method iterates over the key-value pairs in the input metadata dictionary
        and updates the internal `_exec_metadata` dictionary of the tool instance
        accordingly. It then writes the updated metadata to the `METADATA.json`
        file in the tool's data directory.

        Args:
            metadata (dict[str, Any]): A dictionary containing the metadata
            key-value pairs to update in the execution metadata.

        Returns:
            None
        """
        for key, value in metadata.items():
            self._exec_metadata[key] = value

        self._write_exec_metadata(metadata=self._exec_metadata)

    def write_tool_result(self, data: Union[str, dict[str, Any]]) -> None:
        """Helps write contents of the tool result into TOOL_DATA_DIR.

        Args:
            data (Union[str, dict[str, Any]]): Data to be written
        """
        output_type = self.properties[PropKey.RESULT][PropKey.TYPE]
        if output_type is PropKey.OutputType.JSON and not isinstance(data, dict):
            # TODO: Validate JSON type output with output schema as well
            self.stream_error_and_exit(
                f"Expected result to have type {PropKey.OutputType.JSON} "
                f"but {type(data)} is passed"
            )

        # TODO: Review if below is necessary
        result = {
            "workflow_id": self.workflow_id,
            "elapsed_time": self.elapsed_time(),
            "output": data,
        }
        self.stream_result(result)

        self._update_exec_metadata()
        # INFILE is overwritten for next tool to run
        input_file_path: Path = Path(self.get_input_file())
        ToolUtils.dump_json(
            file_to_dump=input_file_path,
            json_to_dump=data,
            fs=self.workflow_filestorage,
        )

    def validate(self, input_file: str, settings: dict[str, Any]) -> None:
        """Override to implement custom validation for the tool.

        Args:
            input_file (str): Path to the file that will be used for execution.
            settings (dict[str, Any]): Settings configured for the tool.
                Defaults initialized through the tool's SPEC.
        """
        pass

    @abstractmethod
    def run(
        self,
        settings: dict[str, Any],
        input_file: str,
        output_dir: str,
    ) -> None:
        """Implements RUN command for the tool.

        Args:
            settings (dict[str, Any]): Settings for the tool
            input_file (str): Path to the input file
            output_dir (str): Path to write the tool output to which gets copied
                into the destination
        """
        pass


================================================
File: src/unstract/sdk/tool/entrypoint.py
================================================
from unstract.sdk.tool.base import BaseTool
from unstract.sdk.tool.executor import ToolExecutor
from unstract.sdk.tool.parser import ToolArgsParser


class ToolEntrypoint:
    """Class that contains methods for the entrypoint for a tool."""

    @staticmethod
    def launch(tool: BaseTool, args: list[str]) -> None:
        """Entrypoint function for a tool.

        It parses the arguments passed to a tool and executes
        the intended command.

        Args:
            tool (AbstractTool): Tool to execute
            args (List[str]): Arguments passed to a tool
        """
        parsed_args = ToolArgsParser.parse_args(args)
        executor = ToolExecutor(tool=tool)
        executor.execute(parsed_args)


================================================
File: src/unstract/sdk/tool/executor.py
================================================
import argparse
import logging
import shutil
from json import loads
from pathlib import Path
from typing import Any

from unstract.sdk import get_sdk_version
from unstract.sdk.constants import Command
from unstract.sdk.tool.base import BaseTool
from unstract.sdk.tool.validator import ToolValidator

logger = logging.getLogger(__name__)


class ToolExecutor:
    """Takes care of executing a tool's intended command."""

    def __init__(self, tool: BaseTool) -> None:
        self.tool = tool

    def execute(self, args: argparse.Namespace) -> None:
        """Executes the tool with the passed arguments.

        Args:
            args (argparse.Namespace): Parsed arguments to execute with
        """
        command = str.upper(args.command)

        if command in Command.static_commands():
            self.tool.handle_static_command(command)
        elif command == Command.RUN:
            self.execute_run(args=args)
        else:
            self.tool.stream_error_and_exit(f"Command {command} not supported")

    def _setup_for_run(self) -> None:
        """Helps initialize tool execution for RUN command."""
        shutil.rmtree(self.tool.get_output_dir(), ignore_errors=True)
        Path(self.tool.get_output_dir()).mkdir(parents=True, exist_ok=True)

    def execute_run(self, args: argparse.Namespace) -> None:
        """Executes the tool's RUN command.

        Args:
            args (argparse.Namespace): Parsed arguments to execute with
        """
        if args.settings is None:
            self.tool.stream_error_and_exit("--settings are required for RUN command")
        settings: dict[str, Any] = loads(args.settings)

        tool_name = self.tool.properties["displayName"]
        tool_version = self.tool.properties["toolVersion"]
        self.tool.stream_log(
            f"Running tool '{tool_name}:{tool_version}' with "
            f"Workflow ID: {self.tool.workflow_id}, "
            f"Execution ID: {self.tool.execution_id}, "
            f"SDK Version: {get_sdk_version()}"
        )
        validator = ToolValidator(self.tool)
        settings = validator.validate_pre_execution(settings=settings)

        self.tool.stream_log(
            f"Executing for file: {self.tool.get_exec_metadata['source_name']}, "
            f"with tool settings: {settings}"
        )

        try:
            self.tool.run(
                settings=settings,
                input_file=self.tool.get_input_file(),
                output_dir=self.tool.get_output_dir(),
            )
        except Exception as e:
            msg = f"Error while running tool '{tool_name}': {str(e)}"
            logger.error(msg, stack_info=True, exc_info=True)
            self.tool.stream_error_and_exit(msg)

        # TODO: Call tool method to validate if output was written


================================================
File: src/unstract/sdk/tool/mime_types.py
================================================
# flake8: noqa
"""Contains a mapping of file extension to MIME types.

Multiple extensions can be mapped to the same MIME type Through
http://svn.apache.org/repos/asf/httpd/httpd/trunk/docs/conf/mime.types
"""
EXT_MIME_MAP = {
    "123": "application/vnd.lotus-1-2-3",
    "3dml": "text/vnd.in3d.3dml",
    "3ds": "image/x-3ds",
    "3g2": "video/3gpp2",
    "3gp": "video/3gpp",
    "7z": "application/x-7z-compressed",
    "aab": "application/x-authorware-bin",
    "aac": "audio/x-aac",
    "aam": "application/x-authorware-map",
    "aas": "application/x-authorware-seg",
    "abw": "application/x-abiword",
    "ac": "application/pkix-attr-cert",
    "acc": "application/vnd.americandynamics.acc",
    "ace": "application/x-ace-compressed",
    "acu": "application/vnd.acucobol",
    "acutc": "application/vnd.acucorp",
    "adp": "audio/adpcm",
    "aep": "application/vnd.audiograph",
    "afm": "application/x-font-type1",
    "afp": "application/vnd.ibm.modcap",
    "ahead": "application/vnd.ahead.space",
    "ai": "application/postscript",
    "aif": "audio/x-aiff",
    "aifc": "audio/x-aiff",
    "aiff": "audio/x-aiff",
    "air": "application/vnd.adobe.air-application-installer-package+zip",
    "ait": "application/vnd.dvb.ait",
    "ami": "application/vnd.amiga.ami",
    "apk": "application/vnd.android.package-archive",
    "appcache": "text/cache-manifest",
    "application": "application/x-ms-application",
    "apr": "application/vnd.lotus-approach",
    "arc": "application/x-freearc",
    "asc": "application/pgp-signature",
    "asf": "video/x-ms-asf",
    "asm": "text/x-asm",
    "aso": "application/vnd.accpac.simply.aso",
    "asx": "video/x-ms-asf",
    "atc": "application/vnd.acucorp",
    "atom": "application/atom+xml",
    "atomcat": "application/atomcat+xml",
    "atomsvc": "application/atomsvc+xml",
    "atx": "application/vnd.antix.game-component",
    "au": "audio/basic",
    "avi": "video/x-msvideo",
    "aw": "application/applixware",
    "azf": "application/vnd.airzip.filesecure.azf",
    "azs": "application/vnd.airzip.filesecure.azs",
    "azw": "application/vnd.amazon.ebook",
    "bat": "application/x-msdownload",
    "bcpio": "application/x-bcpio",
    "bdf": "application/x-font-bdf",
    "bdm": "application/vnd.syncml.dm+wbxml",
    "bed": "application/vnd.realvnc.bed",
    "bh2": "application/vnd.fujitsu.oasysprs",
    "bin": "application/octet-stream",
    "blb": "application/x-blorb",
    "blorb": "application/x-blorb",
    "bmi": "application/vnd.bmi",
    "bmp": "image/bmp",
    "book": "application/vnd.framemaker",
    "box": "application/vnd.previewsystems.box",
    "boz": "application/x-bzip2",
    "bpk": "application/octet-stream",
    "btif": "image/prs.btif",
    "bz2": "application/x-bzip2",
    "bz": "application/x-bzip",
    "c11amc": "application/vnd.cluetrust.cartomobile-config",
    "c11amz": "application/vnd.cluetrust.cartomobile-config-pkg",
    "c4d": "application/vnd.clonk.c4group",
    "c4f": "application/vnd.clonk.c4group",
    "c4g": "application/vnd.clonk.c4group",
    "c4p": "application/vnd.clonk.c4group",
    "c4u": "application/vnd.clonk.c4group",
    "cab": "application/vnd.ms-cab-compressed",
    "caf": "audio/x-caf",
    "cap": "application/vnd.tcpdump.pcap",
    "car": "application/vnd.curl.car",
    "cat": "application/vnd.ms-pki.seccat",
    "cb7": "application/x-cbr",
    "cba": "application/x-cbr",
    "cbr": "application/x-cbr",
    "cbt": "application/x-cbr",
    "cbz": "application/x-cbr",
    "cct": "application/x-director",
    "cc": "text/x-c",
    "ccxml": "application/ccxml+xml",
    "cdbcmsg": "application/vnd.contact.cmsg",
    "cdf": "application/x-netcdf",
    "cdkey": "application/vnd.mediastation.cdkey",
    "cdmia": "application/cdmi-capability",
    "cdmic": "application/cdmi-container",
    "cdmid": "application/cdmi-domain",
    "cdmio": "application/cdmi-object",
    "cdmiq": "application/cdmi-queue",
    "cdx": "chemical/x-cdx",
    "cdxml": "application/vnd.chemdraw+xml",
    "cdy": "application/vnd.cinderella",
    "cer": "application/pkix-cert",
    "cfs": "application/x-cfs-compressed",
    "cgm": "image/cgm",
    "chat": "application/x-chat",
    "chm": "application/vnd.ms-htmlhelp",
    "chrt": "application/vnd.kde.kchart",
    "cif": "chemical/x-cif",
    "cii": "application/vnd.anser-web-certificate-issue-initiation",
    "cil": "application/vnd.ms-artgalry",
    "cla": "application/vnd.claymore",
    "class": "application/java-vm",
    "clkk": "application/vnd.crick.clicker.keyboard",
    "clkp": "application/vnd.crick.clicker.palette",
    "clkt": "application/vnd.crick.clicker.template",
    "clkw": "application/vnd.crick.clicker.wordbank",
    "clkx": "application/vnd.crick.clicker",
    "clp": "application/x-msclip",
    "cmc": "application/vnd.cosmocaller",
    "cmdf": "chemical/x-cmdf",
    "cml": "chemical/x-cml",
    "cmp": "application/vnd.yellowriver-custom-menu",
    "cmx": "image/x-cmx",
    "cod": "application/vnd.rim.cod",
    "com": "application/x-msdownload",
    "conf": "text/plain",
    "cpio": "application/x-cpio",
    "cpp": "text/x-c",
    "cpt": "application/mac-compactpro",
    "crd": "application/x-mscardfile",
    "crl": "application/pkix-crl",
    "crt": "application/x-x509-ca-cert",
    "cryptonote": "application/vnd.rig.cryptonote",
    "csh": "application/x-csh",
    "csml": "chemical/x-csml",
    "csp": "application/vnd.commonspace",
    "css": "text/css",
    "cst": "application/x-director",
    "csv": "text/csv",
    "c": "text/x-c",
    "cu": "application/cu-seeme",
    "curl": "text/vnd.curl",
    "cww": "application/prs.cww",
    "cxt": "application/x-director",
    "cxx": "text/x-c",
    "dae": "model/vnd.collada+xml",
    "daf": "application/vnd.mobius.daf",
    "dart": "application/vnd.dart",
    "dataless": "application/vnd.fdsn.seed",
    "davmount": "application/davmount+xml",
    "dbk": "application/docbook+xml",
    "dcr": "application/x-director",
    "dcurl": "text/vnd.curl.dcurl",
    "dd2": "application/vnd.oma.dd2+xml",
    "ddd": "application/vnd.fujixerox.ddd",
    "deb": "application/x-debian-package",
    "def": "text/plain",
    "deploy": "application/octet-stream",
    "der": "application/x-x509-ca-cert",
    "dfac": "application/vnd.dreamfactory",
    "dgc": "application/x-dgc-compressed",
    "dic": "text/x-c",
    "dir": "application/x-director",
    "dis": "application/vnd.mobius.dis",
    "dist": "application/octet-stream",
    "distz": "application/octet-stream",
    "djv": "image/vnd.djvu",
    "djvu": "image/vnd.djvu",
    "dll": "application/x-msdownload",
    "dmg": "application/x-apple-diskimage",
    "dmp": "application/vnd.tcpdump.pcap",
    "dms": "application/octet-stream",
    "dna": "application/vnd.dna",
    "doc": "application/msword",
    "docm": "application/vnd.ms-word.document.macroenabled.12",
    "docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    "dot": "application/msword",
    "dotm": "application/vnd.ms-word.template.macroenabled.12",
    "dotx": "application/vnd.openxmlformats-officedocument.wordprocessingml.template",
    "dp": "application/vnd.osgi.dp",
    "dpg": "application/vnd.dpgraph",
    "dra": "audio/vnd.dra",
    "dsc": "text/prs.lines.tag",
    "dssc": "application/dssc+der",
    "dtb": "application/x-dtbook+xml",
    "dtd": "application/xml-dtd",
    "dts": "audio/vnd.dts",
    "dtshd": "audio/vnd.dts.hd",
    "dump": "application/octet-stream",
    "dvb": "video/vnd.dvb.file",
    "dvi": "application/x-dvi",
    "dwf": "model/vnd.dwf",
    "dwg": "image/vnd.dwg",
    "dxf": "image/vnd.dxf",
    "dxp": "application/vnd.spotfire.dxp",
    "dxr": "application/x-director",
    "ecelp4800": "audio/vnd.nuera.ecelp4800",
    "ecelp7470": "audio/vnd.nuera.ecelp7470",
    "ecelp9600": "audio/vnd.nuera.ecelp9600",
    "ecma": "application/ecmascript",
    "edm": "application/vnd.novadigm.edm",
    "edx": "application/vnd.novadigm.edx",
    "efif": "application/vnd.picsel",
    "ei6": "application/vnd.pg.osasli",
    "elc": "application/octet-stream",
    "emf": "application/x-msmetafile",
    "eml": "message/rfc822",
    "emma": "application/emma+xml",
    "emz": "application/x-msmetafile",
    "eol": "audio/vnd.digital-winds",
    "eot": "application/vnd.ms-fontobject",
    "eps": "application/postscript",
    "epub": "application/epub+zip",
    "es3": "application/vnd.eszigno3+xml",
    "esa": "application/vnd.osgi.subsystem",
    "esf": "application/vnd.epson.esf",
    "et3": "application/vnd.eszigno3+xml",
    "etx": "text/x-setext",
    "eva": "application/x-eva",
    "evy": "application/x-envoy",
    "exe": "application/x-msdownload",
    "exi": "application/exi",
    "ext": "application/vnd.novadigm.ext",
    "ez2": "application/vnd.ezpix-album",
    "ez3": "application/vnd.ezpix-package",
    "ez": "application/andrew-inset",
    "f4v": "video/x-f4v",
    "f77": "text/x-fortran",
    "f90": "text/x-fortran",
    "fbs": "image/vnd.fastbidsheet",
    "fcdt": "application/vnd.adobe.formscentral.fcdt",
    "fcs": "application/vnd.isac.fcs",
    "fdf": "application/vnd.fdf",
    "fe_launch": "application/vnd.denovo.fcselayout-link",
    "fg5": "application/vnd.fujitsu.oasysgp",
    "fgd": "application/x-director",
    "fh4": "image/x-freehand",
    "fh5": "image/x-freehand",
    "fh7": "image/x-freehand",
    "fhc": "image/x-freehand",
    "fh": "image/x-freehand",
    "fig": "application/x-xfig",
    "flac": "audio/x-flac",
    "fli": "video/x-fli",
    "flo": "application/vnd.micrografx.flo",
    "flv": "video/x-flv",
    "flw": "application/vnd.kde.kivio",
    "flx": "text/vnd.fmi.flexstor",
    "fly": "text/vnd.fly",
    "fm": "application/vnd.framemaker",
    "fnc": "application/vnd.frogans.fnc",
    "for": "text/x-fortran",
    "fpx": "image/vnd.fpx",
    "frame": "application/vnd.framemaker",
    "fsc": "application/vnd.fsc.weblaunch",
    "fst": "image/vnd.fst",
    "ftc": "application/vnd.fluxtime.clip",
    "f": "text/x-fortran",
    "fti": "application/vnd.anser-web-funds-transfer-initiation",
    "fvt": "video/vnd.fvt",
    "fxp": "application/vnd.adobe.fxp",
    "fxpl": "application/vnd.adobe.fxp",
    "fzs": "application/vnd.fuzzysheet",
    "g2w": "application/vnd.geoplan",
    "g3": "image/g3fax",
    "g3w": "application/vnd.geospace",
    "gac": "application/vnd.groove-account",
    "gam": "application/x-tads",
    "gbr": "application/rpki-ghostbusters",
    "gca": "application/x-gca-compressed",
    "gdl": "model/vnd.gdl",
    "geo": "application/vnd.dynageo",
    "gex": "application/vnd.geometry-explorer",
    "ggb": "application/vnd.geogebra.file",
    "ggs": "application/vnd.geogebra.slides",
    "ggt": "application/vnd.geogebra.tool",
    "ghf": "application/vnd.groove-help",
    "gif": "image/gif",
    "gim": "application/vnd.groove-identity-message",
    "gml": "application/gml+xml",
    "gmx": "application/vnd.gmx",
    "gnumeric": "application/x-gnumeric",
    "gph": "application/vnd.flographit",
    "gpx": "application/gpx+xml",
    "gqf": "application/vnd.grafeq",
    "gqs": "application/vnd.grafeq",
    "gram": "application/srgs",
    "gramps": "application/x-gramps-xml",
    "gre": "application/vnd.geometry-explorer",
    "grv": "application/vnd.groove-injector",
    "grxml": "application/srgs+xml",
    "gsf": "application/x-font-ghostscript",
    "gtar": "application/x-gtar",
    "gtm": "application/vnd.groove-tool-message",
    "gtw": "model/vnd.gtw",
    "gv": "text/vnd.graphviz",
    "gxf": "application/gxf",
    "gxt": "application/vnd.geonext",
    "h261": "video/h261",
    "h263": "video/h263",
    "h264": "video/h264",
    "hal": "application/vnd.hal+xml",
    "hbci": "application/vnd.hbci",
    "hdf": "application/x-hdf",
    "hh": "text/x-c",
    "hlp": "application/winhlp",
    "hpgl": "application/vnd.hp-hpgl",
    "hpid": "application/vnd.hp-hpid",
    "hps": "application/vnd.hp-hps",
    "hqx": "application/mac-binhex40",
    "h": "text/x-c",
    "htke": "application/vnd.kenameaapp",
    "html": "text/html",
    "htm": "text/html",
    "hvd": "application/vnd.yamaha.hv-dic",
    "hvp": "application/vnd.yamaha.hv-voice",
    "hvs": "application/vnd.yamaha.hv-script",
    "i2g": "application/vnd.intergeo",
    "icc": "application/vnd.iccprofile",
    "ice": "x-conference/x-cooltalk",
    "icm": "application/vnd.iccprofile",
    "ico": "image/x-icon",
    "ics": "text/calendar",
    "ief": "image/ief",
    "ifb": "text/calendar",
    "ifm": "application/vnd.shana.informed.formdata",
    "iges": "model/iges",
    "igl": "application/vnd.igloader",
    "igm": "application/vnd.insors.igm",
    "igs": "model/iges",
    "igx": "application/vnd.micrografx.igx",
    "iif": "application/vnd.shana.informed.interchange",
    "imp": "application/vnd.accpac.simply.imp",
    "ims": "application/vnd.ms-ims",
    "ink": "application/inkml+xml",
    "inkml": "application/inkml+xml",
    "install": "application/x-install-instructions",
    "in": "text/plain",
    "iota": "application/vnd.astraea-software.iota",
    "ipfix": "application/ipfix",
    "ipk": "application/vnd.shana.informed.package",
    "irm": "application/vnd.ibm.rights-management",
    "irp": "application/vnd.irepository.package+xml",
    "iso": "application/x-iso9660-image",
    "itp": "application/vnd.shana.informed.formtemplate",
    "ivp": "application/vnd.immervision-ivp",
    "ivu": "application/vnd.immervision-ivu",
    "jad": "text/vnd.sun.j2me.app-descriptor",
    "jam": "application/vnd.jam",
    "jar": "application/java-archive",
    "java": "text/x-java-source",
    "jisp": "application/vnd.jisp",
    "jlt": "application/vnd.hp-jlyt",
    "jnlp": "application/x-java-jnlp-file",
    "joda": "application/vnd.joost.joda-archive",
    "jpeg": "image/jpeg",
    "jpe": "image/jpeg",
    "jpg": "image/jpeg",
    "jpgm": "video/jpm",
    "jpgv": "video/jpeg",
    "jpm": "video/jpm",
    "json": "application/json",
    "jsonml": "application/jsonml+json",
    "js": "text/javascript",
    "kar": "audio/midi",
    "karbon": "application/vnd.kde.karbon",
    "kfo": "application/vnd.kde.kformula",
    "kia": "application/vnd.kidspiration",
    "kml": "application/vnd.google-earth.kml+xml",
    "kmz": "application/vnd.google-earth.kmz",
    "kne": "application/vnd.kinar",
    "knp": "application/vnd.kinar",
    "kon": "application/vnd.kde.kontour",
    "kpr": "application/vnd.kde.kpresenter",
    "kpt": "application/vnd.kde.kpresenter",
    "kpxx": "application/vnd.ds-keypoint",
    "ksp": "application/vnd.kde.kspread",
    "ktr": "application/vnd.kahootz",
    "ktx": "image/ktx",
    "ktz": "application/vnd.kahootz",
    "kwd": "application/vnd.kde.kword",
    "kwt": "application/vnd.kde.kword",
    "lasxml": "application/vnd.las.las+xml",
    "latex": "application/x-latex",
    "lbd": "application/vnd.llamagraphics.life-balance.desktop",
    "lbe": "application/vnd.llamagraphics.life-balance.exchange+xml",
    "les": "application/vnd.hhe.lesson-player",
    "lha": "application/x-lzh-compressed",
    "link66": "application/vnd.route66.link66+xml",
    "list3820": "application/vnd.ibm.modcap",
    "listafp": "application/vnd.ibm.modcap",
    "list": "text/plain",
    "lnk": "application/x-ms-shortcut",
    "log": "text/plain",
    "lostxml": "application/lost+xml",
    "lrf": "application/octet-stream",
    "lrm": "application/vnd.ms-lrm",
    "ltf": "application/vnd.frogans.ltf",
    "lvp": "audio/vnd.lucent.voice",
    "lwp": "application/vnd.lotus-wordpro",
    "lzh": "application/x-lzh-compressed",
    "m13": "application/x-msmediaview",
    "m14": "application/x-msmediaview",
    "m1v": "video/mpeg",
    "m21": "application/mp21",
    "m2a": "audio/mpeg",
    "m2v": "video/mpeg",
    "m3a": "audio/mpeg",
    "m3u8": "application/vnd.apple.mpegurl",
    "m3u": "audio/x-mpegurl",
    "m4a": "audio/mp4",
    "m4u": "video/vnd.mpegurl",
    "m4v": "video/x-m4v",
    "ma": "application/mathematica",
    "mads": "application/mads+xml",
    "mag": "application/vnd.ecowin.chart",
    "maker": "application/vnd.framemaker",
    "man": "text/troff",
    "mar": "application/octet-stream",
    "mathml": "application/mathml+xml",
    "mb": "application/mathematica",
    "mbk": "application/vnd.mobius.mbk",
    "mbox": "application/mbox",
    "mc1": "application/vnd.medcalcdata",
    "mcd": "application/vnd.mcd",
    "mcurl": "text/vnd.curl.mcurl",
    "mdb": "application/x-msaccess",
    "mdi": "image/vnd.ms-modi",
    "mesh": "model/mesh",
    "meta4": "application/metalink4+xml",
    "metalink": "application/metalink+xml",
    "me": "text/troff",
    "mets": "application/mets+xml",
    "mfm": "application/vnd.mfmp",
    "mft": "application/rpki-manifest",
    "mgp": "application/vnd.osgeo.mapguide.package",
    "mgz": "application/vnd.proteus.magazine",
    "mid": "audio/midi",
    "midi": "audio/midi",
    "mie": "application/x-mie",
    "mif": "application/vnd.mif",
    "mime": "message/rfc822",
    "mj2": "video/mj2",
    "mjp2": "video/mj2",
    "mjs": "text/javascript",
    "mk3d": "video/x-matroska",
    "mka": "audio/x-matroska",
    "mks": "video/x-matroska",
    "mkv": "video/x-matroska",
    "mlp": "application/vnd.dolby.mlp",
    "mmd": "application/vnd.chipnuts.karaoke-mmd",
    "mmf": "application/vnd.smaf",
    "mmr": "image/vnd.fujixerox.edmics-mmr",
    "mng": "video/x-mng",
    "mny": "application/x-msmoney",
    "mobi": "application/x-mobipocket-ebook",
    "mods": "application/mods+xml",
    "movie": "video/x-sgi-movie",
    "mov": "video/quicktime",
    "mp21": "application/mp21",
    "mp2a": "audio/mpeg",
    "mp2": "audio/mpeg",
    "mp3": "audio/mpeg",
    "mp4a": "audio/mp4",
    "mp4s": "application/mp4",
    "mp4": "video/mp4",
    "mp4v": "video/mp4",
    "mpc": "application/vnd.mophun.certificate",
    "mpeg": "video/mpeg",
    "mpe": "video/mpeg",
    "mpg4": "video/mp4",
    "mpga": "audio/mpeg",
    "mpg": "video/mpeg",
    "mpkg": "application/vnd.apple.installer+xml",
    "mpm": "application/vnd.blueice.multipass",
    "mpn": "application/vnd.mophun.application",
    "mpp": "application/vnd.ms-project",
    "mpt": "application/vnd.ms-project",
    "mpy": "application/vnd.ibm.minipay",
    "mqy": "application/vnd.mobius.mqy",
    "mrc": "application/marc",
    "mrcx": "application/marcxml+xml",
    "mscml": "application/mediaservercontrol+xml",
    "mseed": "application/vnd.fdsn.mseed",
    "mseq": "application/vnd.mseq",
    "msf": "application/vnd.epson.msf",
    "msh": "model/mesh",
    "msi": "application/x-msdownload",
    "msl": "application/vnd.mobius.msl",
    "ms": "text/troff",
    "msty": "application/vnd.muvee.style",
    "mts": "model/vnd.mts",
    "mus": "application/vnd.musician",
    "musicxml": "application/vnd.recordare.musicxml+xml",
    "mvb": "application/x-msmediaview",
    "mwf": "application/vnd.mfer",
    "mxf": "application/mxf",
    "mxl": "application/vnd.recordare.musicxml",
    "mxml": "application/xv+xml",
    "mxs": "application/vnd.triscape.mxs",
    "mxu": "video/vnd.mpegurl",
    "n3": "text/n3",
    "nb": "application/mathematica",
    "nbp": "application/vnd.wolfram.player",
    "nc": "application/x-netcdf",
    "ncx": "application/x-dtbncx+xml",
    "nfo": "text/x-nfo",
    "n-gage": "application/vnd.nokia.n-gage.symbian.install",
    "ngdat": "application/vnd.nokia.n-gage.data",
    "nitf": "application/vnd.nitf",
    "nlu": "application/vnd.neurolanguage.nlu",
    "nml": "application/vnd.enliven",
    "nnd": "application/vnd.noblenet-directory",
    "nns": "application/vnd.noblenet-sealer",
    "nnw": "application/vnd.noblenet-web",
    "npx": "image/vnd.net-fpx",
    "nsc": "application/x-conference",
    "nsf": "application/vnd.lotus-notes",
    "ntf": "application/vnd.nitf",
    "nzb": "application/x-nzb",
    "oa2": "application/vnd.fujitsu.oasys2",
    "oa3": "application/vnd.fujitsu.oasys3",
    "oas": "application/vnd.fujitsu.oasys",
    "obd": "application/x-msbinder",
    "obj": "application/x-tgif",
    "oda": "application/oda",
    "odb": "application/vnd.oasis.opendocument.database",
    "odc": "application/vnd.oasis.opendocument.chart",
    "odf": "application/vnd.oasis.opendocument.formula",
    "odft": "application/vnd.oasis.opendocument.formula-template",
    "odg": "application/vnd.oasis.opendocument.graphics",
    "odi": "application/vnd.oasis.opendocument.image",
    "odm": "application/vnd.oasis.opendocument.text-master",
    "odp": "application/vnd.oasis.opendocument.presentation",
    "ods": "application/vnd.oasis.opendocument.spreadsheet",
    "odt": "application/vnd.oasis.opendocument.text",
    "oga": "audio/ogg",
    "ogg": "audio/ogg",
    "ogv": "video/ogg",
    "ogx": "application/ogg",
    "omdoc": "application/omdoc+xml",
    "onepkg": "application/onenote",
    "onetmp": "application/onenote",
    "onetoc2": "application/onenote",
    "onetoc": "application/onenote",
    "opf": "application/oebps-package+xml",
    "opml": "text/x-opml",
    "oprc": "application/vnd.palm",
    "opus": "audio/ogg",
    "org": "application/vnd.lotus-organizer",
    "osf": "application/vnd.yamaha.openscoreformat",
    "osfpvg": "application/vnd.yamaha.openscoreformat.osfpvg+xml",
    "otc": "application/vnd.oasis.opendocument.chart-template",
    "otf": "font/otf",
    "otg": "application/vnd.oasis.opendocument.graphics-template",
    "oth": "application/vnd.oasis.opendocument.text-web",
    "oti": "application/vnd.oasis.opendocument.image-template",
    "otp": "application/vnd.oasis.opendocument.presentation-template",
    "ots": "application/vnd.oasis.opendocument.spreadsheet-template",
    "ott": "application/vnd.oasis.opendocument.text-template",
    "oxps": "application/oxps",
    "oxt": "application/vnd.openofficeorg.extension",
    "p10": "application/pkcs10",
    "p12": "application/x-pkcs12",
    "p7b": "application/x-pkcs7-certificates",
    "p7c": "application/pkcs7-mime",
    "p7m": "application/pkcs7-mime",
    "p7r": "application/x-pkcs7-certreqresp",
    "p7s": "application/pkcs7-signature",
    "p8": "application/pkcs8",
    "pas": "text/x-pascal",
    "paw": "application/vnd.pawaafile",
    "pbd": "application/vnd.powerbuilder6",
    "pbm": "image/x-portable-bitmap",
    "pcap": "application/vnd.tcpdump.pcap",
    "pcf": "application/x-font-pcf",
    "pcl": "application/vnd.hp-pcl",
    "pclxl": "application/vnd.hp-pclxl",
    "pct": "image/x-pict",
    "pcurl": "application/vnd.curl.pcurl",
    "pcx": "image/x-pcx",
    "pdb": "application/vnd.palm",
    "pdf": "application/pdf",
    "pfa": "application/x-font-type1",
    "pfb": "application/x-font-type1",
    "pfm": "application/x-font-type1",
    "pfr": "application/font-tdpfr",
    "pfx": "application/x-pkcs12",
    "pgm": "image/x-portable-graymap",
    "pgn": "application/x-chess-pgn",
    "pgp": "application/pgp-encrypted",
    "pic": "image/x-pict",
    "pkg": "application/octet-stream",
    "pki": "application/pkixcmp",
    "pkipath": "application/pkix-pkipath",
    "plb": "application/vnd.3gpp.pic-bw-large",
    "plc": "application/vnd.mobius.plc",
    "plf": "application/vnd.pocketlearn",
    "pls": "application/pls+xml",
    "pml": "application/vnd.ctc-posml",
    "png": "image/png",
    "pnm": "image/x-portable-anymap",
    "portpkg": "application/vnd.macports.portpkg",
    "pot": "application/vnd.ms-powerpoint",
    "potm": "application/vnd.ms-powerpoint.template.macroenabled.12",
    "potx": "application/vnd.openxmlformats-officedocument.presentationml.template",
    "ppam": "application/vnd.ms-powerpoint.addin.macroenabled.12",
    "ppd": "application/vnd.cups-ppd",
    "ppm": "image/x-portable-pixmap",
    "pps": "application/vnd.ms-powerpoint",
    "ppsm": "application/vnd.ms-powerpoint.slideshow.macroenabled.12",
    "ppsx": "application/vnd.openxmlformats-officedocument.presentationml.slideshow",
    "ppt": "application/vnd.ms-powerpoint",
    "pptm": "application/vnd.ms-powerpoint.presentation.macroenabled.12",
    "pptx": "application/vnd.openxmlformats-officedocument.presentationml.presentation",
    "pqa": "application/vnd.palm",
    "prc": "application/x-mobipocket-ebook",
    "pre": "application/vnd.lotus-freelance",
    "prf": "application/pics-rules",
    "ps": "application/postscript",
    "psb": "application/vnd.3gpp.pic-bw-small",
    "psd": "image/vnd.adobe.photoshop",
    "psf": "application/x-font-linux-psf",
    "pskcxml": "application/pskc+xml",
    "p": "text/x-pascal",
    "ptid": "application/vnd.pvi.ptid1",
    "pub": "application/x-mspublisher",
    "pvb": "application/vnd.3gpp.pic-bw-var",
    "pwn": "application/vnd.3m.post-it-notes",
    "pya": "audio/vnd.ms-playready.media.pya",
    "pyv": "video/vnd.ms-playready.media.pyv",
    "qam": "application/vnd.epson.quickanime",
    "qbo": "application/vnd.intu.qbo",
    "qfx": "application/vnd.intu.qfx",
    "qps": "application/vnd.publishare-delta-tree",
    "qt": "video/quicktime",
    "qwd": "application/vnd.quark.quarkxpress",
    "qwt": "application/vnd.quark.quarkxpress",
    "qxb": "application/vnd.quark.quarkxpress",
    "qxd": "application/vnd.quark.quarkxpress",
    "qxl": "application/vnd.quark.quarkxpress",
    "qxt": "application/vnd.quark.quarkxpress",
    "ra": "audio/x-pn-realaudio",
    "ram": "audio/x-pn-realaudio",
    "rar": "application/x-rar-compressed",
    "ras": "image/x-cmu-raster",
    "rcprofile": "application/vnd.ipunplugged.rcprofile",
    "rdf": "application/rdf+xml",
    "rdz": "application/vnd.data-vision.rdz",
    "rep": "application/vnd.businessobjects",
    "res": "application/x-dtbresource+xml",
    "rgb": "image/x-rgb",
    "rif": "application/reginfo+xml",
    "rip": "audio/vnd.rip",
    "ris": "application/x-research-info-systems",
    "rl": "application/resource-lists+xml",
    "rlc": "image/vnd.fujixerox.edmics-rlc",
    "rld": "application/resource-lists-diff+xml",
    "rm": "application/vnd.rn-realmedia",
    "rmi": "audio/midi",
    "rmp": "audio/x-pn-realaudio-plugin",
    "rms": "application/vnd.jcp.javame.midlet-rms",
    "rmvb": "application/vnd.rn-realmedia-vbr",
    "rnc": "application/relax-ng-compact-syntax",
    "roa": "application/rpki-roa",
    "roff": "text/troff",
    "rp9": "application/vnd.cloanto.rp9",
    "rpss": "application/vnd.nokia.radio-presets",
    "rpst": "application/vnd.nokia.radio-preset",
    "rq": "application/sparql-query",
    "rs": "application/rls-services+xml",
    "rsd": "application/rsd+xml",
    "rss": "application/rss+xml",
    "rtf": "application/rtf",
    "rtx": "text/richtext",
    "s3m": "audio/s3m",
    "saf": "application/vnd.yamaha.smaf-audio",
    "sbml": "application/sbml+xml",
    "sc": "application/vnd.ibm.secure-container",
    "scd": "application/x-msschedule",
    "scm": "application/vnd.lotus-screencam",
    "scq": "application/scvp-cv-request",
    "scs": "application/scvp-cv-response",
    "scurl": "text/vnd.curl.scurl",
    "sda": "application/vnd.stardivision.draw",
    "sdc": "application/vnd.stardivision.calc",
    "sdd": "application/vnd.stardivision.impress",
    "sdkd": "application/vnd.solent.sdkm+xml",
    "sdkm": "application/vnd.solent.sdkm+xml",
    "sdp": "application/sdp",
    "sdw": "application/vnd.stardivision.writer",
    "see": "application/vnd.seemail",
    "seed": "application/vnd.fdsn.seed",
    "sema": "application/vnd.sema",
    "semd": "application/vnd.semd",
    "semf": "application/vnd.semf",
    "ser": "application/java-serialized-object",
    "setpay": "application/set-payment-initiation",
    "setreg": "application/set-registration-initiation",
    "sfd-hdstx": "application/vnd.hydrostatix.sof-data",
    "sfs": "application/vnd.spotfire.sfs",
    "sfv": "text/x-sfv",
    "sgi": "image/sgi",
    "sgl": "application/vnd.stardivision.writer-global",
    "sgml": "text/sgml",
    "sgm": "text/sgml",
    "sh": "application/x-sh",
    "shar": "application/x-shar",
    "shf": "application/shf+xml",
    "sid": "image/x-mrsid-image",
    "sig": "application/pgp-signature",
    "sil": "audio/silk",
    "silo": "model/mesh",
    "sis": "application/vnd.symbian.install",
    "sisx": "application/vnd.symbian.install",
    "sit": "application/x-stuffit",
    "sitx": "application/x-stuffitx",
    "skd": "application/vnd.koan",
    "skm": "application/vnd.koan",
    "skp": "application/vnd.koan",
    "skt": "application/vnd.koan",
    "sldm": "application/vnd.ms-powerpoint.slide.macroenabled.12",
    "sldx": "application/vnd.openxmlformats-officedocument.presentationml.slide",
    "slt": "application/vnd.epson.salt",
    "sm": "application/vnd.stepmania.stepchart",
    "smf": "application/vnd.stardivision.math",
    "smi": "application/smil+xml",
    "smil": "application/smil+xml",
    "smv": "video/x-smv",
    "smzip": "application/vnd.stepmania.package",
    "snd": "audio/basic",
    "snf": "application/x-font-snf",
    "so": "application/octet-stream",
    "spc": "application/x-pkcs7-certificates",
    "spf": "application/vnd.yamaha.smaf-phrase",
    "spl": "application/x-futuresplash",
    "spot": "text/vnd.in3d.spot",
    "spp": "application/scvp-vp-response",
    "spq": "application/scvp-vp-request",
    "spx": "audio/ogg",
    "sql": "application/x-sql",
    "src": "application/x-wais-source",
    "srt": "application/x-subrip",
    "sru": "application/sru+xml",
    "srx": "application/sparql-results+xml",
    "ssdl": "application/ssdl+xml",
    "sse": "application/vnd.kodak-descriptor",
    "ssf": "application/vnd.epson.ssf",
    "ssml": "application/ssml+xml",
    "st": "application/vnd.sailingtracker.track",
    "stc": "application/vnd.sun.xml.calc.template",
    "std": "application/vnd.sun.xml.draw.template",
    "s": "text/x-asm",
    "stf": "application/vnd.wt.stf",
    "sti": "application/vnd.sun.xml.impress.template",
    "stk": "application/hyperstudio",
    "stl": "application/vnd.ms-pki.stl",
    "str": "application/vnd.pg.format",
    "stw": "application/vnd.sun.xml.writer.template",
    "sub": "text/vnd.dvb.subtitle",
    "sus": "application/vnd.sus-calendar",
    "susp": "application/vnd.sus-calendar",
    "sv4cpio": "application/x-sv4cpio",
    "sv4crc": "application/x-sv4crc",
    "svc": "application/vnd.dvb.service",
    "svd": "application/vnd.svd",
    "svg": "image/svg+xml",
    "svgz": "image/svg+xml",
    "swa": "application/x-director",
    "swf": "application/x-shockwave-flash",
    "swi": "application/vnd.aristanetworks.swi",
    "sxc": "application/vnd.sun.xml.calc",
    "sxd": "application/vnd.sun.xml.draw",
    "sxg": "application/vnd.sun.xml.writer.global",
    "sxi": "application/vnd.sun.xml.impress",
    "sxm": "application/vnd.sun.xml.math",
    "sxw": "application/vnd.sun.xml.writer",
    "t3": "application/x-t3vm-image",
    "taglet": "application/vnd.mynfc",
    "tao": "application/vnd.tao.intent-module-archive",
    "tar": "application/x-tar",
    "tcap": "application/vnd.3gpp2.tcap",
    "tcl": "application/x-tcl",
    "teacher": "application/vnd.smart.teacher",
    "tei": "application/tei+xml",
    "teicorpus": "application/tei+xml",
    "tex": "application/x-tex",
    "texi": "application/x-texinfo",
    "texinfo": "application/x-texinfo",
    "text": "text/plain",
    "tfi": "application/thraud+xml",
    "tfm": "application/x-tex-tfm",
    "tga": "image/x-tga",
    "thmx": "application/vnd.ms-officetheme",
    "tiff": "image/tiff",
    "tif": "image/tiff",
    "tmo": "application/vnd.tmobile-livetv",
    "torrent": "application/x-bittorrent",
    "tpl": "application/vnd.groove-tool-template",
    "tpt": "application/vnd.trid.tpt",
    "tra": "application/vnd.trueapp",
    "trm": "application/x-msterminal",
    "tr": "text/troff",
    "tsd": "application/timestamped-data",
    "tsv": "text/tab-separated-values",
    "ttc": "font/collection",
    "t": "text/troff",
    "ttf": "font/ttf",
    "ttl": "text/turtle",
    "twd": "application/vnd.simtech-mindmapper",
    "twds": "application/vnd.simtech-mindmapper",
    "txd": "application/vnd.genomatix.tuxedo",
    "txf": "application/vnd.mobius.txf",
    "txt": "text/plain",
    "u32": "application/x-authorware-bin",
    "udeb": "application/x-debian-package",
    "ufd": "application/vnd.ufdl",
    "ufdl": "application/vnd.ufdl",
    "ulx": "application/x-glulx",
    "umj": "application/vnd.umajin",
    "unityweb": "application/vnd.unity",
    "uoml": "application/vnd.uoml+xml",
    "uris": "text/uri-list",
    "uri": "text/uri-list",
    "urls": "text/uri-list",
    "ustar": "application/x-ustar",
    "utz": "application/vnd.uiq.theme",
    "uu": "text/x-uuencode",
    "uva": "audio/vnd.dece.audio",
    "uvd": "application/vnd.dece.data",
    "uvf": "application/vnd.dece.data",
    "uvg": "image/vnd.dece.graphic",
    "uvh": "video/vnd.dece.hd",
    "uvi": "image/vnd.dece.graphic",
    "uvm": "video/vnd.dece.mobile",
    "uvp": "video/vnd.dece.pd",
    "uvs": "video/vnd.dece.sd",
    "uvt": "application/vnd.dece.ttml+xml",
    "uvu": "video/vnd.uvvu.mp4",
    "uvva": "audio/vnd.dece.audio",
    "uvvd": "application/vnd.dece.data",
    "uvvf": "application/vnd.dece.data",
    "uvvg": "image/vnd.dece.graphic",
    "uvvh": "video/vnd.dece.hd",
    "uvvi": "image/vnd.dece.graphic",
    "uvvm": "video/vnd.dece.mobile",
    "uvvp": "video/vnd.dece.pd",
    "uvvs": "video/vnd.dece.sd",
    "uvvt": "application/vnd.dece.ttml+xml",
    "uvvu": "video/vnd.uvvu.mp4",
    "uvv": "video/vnd.dece.video",
    "uvvv": "video/vnd.dece.video",
    "uvvx": "application/vnd.dece.unspecified",
    "uvvz": "application/vnd.dece.zip",
    "uvx": "application/vnd.dece.unspecified",
    "uvz": "application/vnd.dece.zip",
    "vcard": "text/vcard",
    "vcd": "application/x-cdlink",
    "vcf": "text/x-vcard",
    "vcg": "application/vnd.groove-vcard",
    "vcs": "text/x-vcalendar",
    "vcx": "application/vnd.vcx",
    "vis": "application/vnd.visionary",
    "viv": "video/vnd.vivo",
    "vob": "video/x-ms-vob",
    "vor": "application/vnd.stardivision.writer",
    "vox": "application/x-authorware-bin",
    "vrml": "model/vrml",
    "vsd": "application/vnd.visio",
    "vsf": "application/vnd.vsf",
    "vss": "application/vnd.visio",
    "vst": "application/vnd.visio",
    "vsw": "application/vnd.visio",
    "vtu": "model/vnd.vtu",
    "vxml": "application/voicexml+xml",
    "w3d": "application/x-director",
    "wad": "application/x-doom",
    "wasm": "application/wasm",
    "wav": "audio/x-wav",
    "wax": "audio/x-ms-wax",
    "wbmp": "image/vnd.wap.wbmp",
    "wbs": "application/vnd.criticaltools.wbs+xml",
    "wbxml": "application/vnd.wap.wbxml",
    "wcm": "application/vnd.ms-works",
    "wdb": "application/vnd.ms-works",
    "wdp": "image/vnd.ms-photo",
    "weba": "audio/webm",
    "webm": "video/webm",
    "webp": "image/webp",
    "wg": "application/vnd.pmi.widget",
    "wgt": "application/widget",
    "wks": "application/vnd.ms-works",
    "wma": "audio/x-ms-wma",
    "wmd": "application/x-ms-wmd",
    "wmf": "application/x-msmetafile",
    "wmlc": "application/vnd.wap.wmlc",
    "wmlsc": "application/vnd.wap.wmlscriptc",
    "wmls": "text/vnd.wap.wmlscript",
    "wml": "text/vnd.wap.wml",
    "wm": "video/x-ms-wm",
    "wmv": "video/x-ms-wmv",
    "wmx": "video/x-ms-wmx",
    "wmz": "application/x-ms-wmz",
    "woff2": "font/woff2",
    "woff": "font/woff",
    "wpd": "application/vnd.wordperfect",
    "wpl": "application/vnd.ms-wpl",
    "wps": "application/vnd.ms-works",
    "wqd": "application/vnd.wqd",
    "wri": "application/x-mswrite",
    "wrl": "model/vrml",
    "wsdl": "application/wsdl+xml",
    "wspolicy": "application/wspolicy+xml",
    "wtb": "application/vnd.webturbo",
    "wvx": "video/x-ms-wvx",
    "x32": "application/x-authorware-bin",
    "x3db": "model/x3d+binary",
    "x3dbz": "model/x3d+binary",
    "x3d": "model/x3d+xml",
    "x3dv": "model/x3d+vrml",
    "x3dvz": "model/x3d+vrml",
    "x3dz": "model/x3d+xml",
    "xaml": "application/xaml+xml",
    "xap": "application/x-silverlight-app",
    "xar": "application/vnd.xara",
    "xbap": "application/x-ms-xbap",
    "xbd": "application/vnd.fujixerox.docuworks.binder",
    "xbm": "image/x-xbitmap",
    "xdf": "application/xcap-diff+xml",
    "xdm": "application/vnd.syncml.dm+xml",
    "xdp": "application/vnd.adobe.xdp+xml",
    "xdssc": "application/dssc+xml",
    "xdw": "application/vnd.fujixerox.docuworks",
    "xenc": "application/xenc+xml",
    "xer": "application/patch-ops-error+xml",
    "xfdf": "application/vnd.adobe.xfdf",
    "xfdl": "application/vnd.xfdl",
    "xht": "application/xhtml+xml",
    "xhtml": "application/xhtml+xml",
    "xhvml": "application/xv+xml",
    "xif": "image/vnd.xiff",
    "xla": "application/vnd.ms-excel",
    "xlam": "application/vnd.ms-excel.addin.macroenabled.12",
    "xlc": "application/vnd.ms-excel",
    "xlf": "application/x-xliff+xml",
    "xlm": "application/vnd.ms-excel",
    "xls": "application/vnd.ms-excel",
    "xlsb": "application/vnd.ms-excel.sheet.binary.macroenabled.12",
    "xlsm": "application/vnd.ms-excel.sheet.macroenabled.12",
    "xlsx": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    "xlt": "application/vnd.ms-excel",
    "xltm": "application/vnd.ms-excel.template.macroenabled.12",
    "xltx": "application/vnd.openxmlformats-officedocument.spreadsheetml.template",
    "xlw": "application/vnd.ms-excel",
    "xm": "audio/xm",
    "xml": "application/xml",
    "xo": "application/vnd.olpc-sugar",
    "xop": "application/xop+xml",
    "xpi": "application/x-xpinstall",
    "xpl": "application/xproc+xml",
    "xpm": "image/x-xpixmap",
    "xpr": "application/vnd.is-xpr",
    "xps": "application/vnd.ms-xpsdocument",
    "xpw": "application/vnd.intercon.formnet",
    "xpx": "application/vnd.intercon.formnet",
    "xsl": "application/xml",
    "xslt": "application/xslt+xml",
    "xsm": "application/vnd.syncml+xml",
    "xspf": "application/xspf+xml",
    "xul": "application/vnd.mozilla.xul+xml",
    "xvm": "application/xv+xml",
    "xvml": "application/xv+xml",
    "xwd": "image/x-xwindowdump",
    "xyz": "chemical/x-xyz",
    "xz": "application/x-xz",
    "yang": "application/yang",
    "yin": "application/yin+xml",
    "z1": "application/x-zmachine",
    "z2": "application/x-zmachine",
    "z3": "application/x-zmachine",
    "z4": "application/x-zmachine",
    "z5": "application/x-zmachine",
    "z6": "application/x-zmachine",
    "z7": "application/x-zmachine",
    "z8": "application/x-zmachine",
    "zaz": "application/vnd.zzazz.deck+xml",
    "zip": "application/zip",
    "zir": "application/vnd.zul",
    "zirz": "application/vnd.zul",
    "zmm": "application/vnd.handheld-entertainment+xml",
}


================================================
File: src/unstract/sdk/tool/mixin.py
================================================
import logging
from typing import Any

from unstract.sdk.file_storage import FileStorage, FileStorageProvider
from unstract.sdk.utils import ToolUtils

logger = logging.getLogger(__name__)


class ToolConfigHelper:
    """Helper class to handle static commands for tools."""

    @staticmethod
    def spec(
        spec_file: str = "config/spec.json",
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> dict[str, Any]:
        """Returns the JSON schema for the tool settings.

        Args:
            spec_file (str): The path to the JSON schema file.
            The default is config/spec.json.
        Returns:
            str: The JSON schema of the tool.
        """
        return ToolUtils.load_json(spec_file, fs=fs)

    @staticmethod
    def properties(
        properties_file: str = "config/properties.json",
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> dict[str, Any]:
        """Returns the properties of the tool.

        Args:
            properties_file (str): The path to the properties file.
            The default is config/properties.json.
        Returns:
            str: The properties of the tool.
        """
        return ToolUtils.load_json(properties_file, fs)

    @staticmethod
    def variables(
        variables_file: str = "config/runtime_variables.json",
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> dict[str, Any]:
        """Returns the JSON schema of the runtime variables.

        Args:
            variables_file (str): The path to the JSON schema file.
            The default is config/runtime_variables.json.
        Returns:
            str: The JSON schema for the runtime variables.
        """

        try:
            return ToolUtils.load_json(variables_file, fs)
        # Allow runtime variables definition to be optional
        except FileNotFoundError:
            logger.info("No runtime variables defined for tool")
            return {}

    @staticmethod
    def icon(
        icon_file: str = "config/icon.svg",
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> str:
        """Returns the icon of the tool.

        Args:
            icon_file (str): The path to the icon file.
                The default is config/icon.svg.
        Returns:
            str: The icon of the tool.
        """
        icon = fs.read(path=icon_file, mode="rb", encoding="utf-8")
        return icon


================================================
File: src/unstract/sdk/tool/parser.py
================================================
import argparse
from typing import Optional

from dotenv import find_dotenv, load_dotenv

from unstract.sdk.constants import LogLevel


class ToolArgsParser:
    """Class to help with parsing arguments to a tool."""

    @staticmethod
    def parse_args(args_to_parse: list[str]) -> argparse.Namespace:
        """Helps parse arguments to a tool.

        Args:
            args_to_parse (List[str]): Command line arguments received by a tool

        Returns:
            argparse.Namespace: Parsed arguments
        """
        parser = argparse.ArgumentParser()
        parser.add_argument(
            "--command", type=str, help="Command to execute", required=True
        )
        parser.add_argument(
            "--settings", type=str, help="Settings to be used", required=False
        )
        parser.add_argument(
            "--log-level",
            type=LogLevel,
            help="Log level",
            required=False,
            default=LogLevel.ERROR,
        )
        parser.add_argument(
            "--env",
            type=str,
            help="Env file to load environment from",
            required=False,
            default=find_dotenv(usecwd=True),
        )
        parsed_args = parser.parse_args(args_to_parse)
        ToolArgsParser.load_environment(parsed_args.env)
        return parsed_args

    @staticmethod
    def load_environment(path: Optional[str] = None) -> None:
        """Loads env variables with python-dotenv.

        Args:
            path (Optional[str], optional): Path to the env file to load.
                Defaults to None.
        """
        if path is None:
            load_dotenv()
        else:
            load_dotenv(path)


================================================
File: src/unstract/sdk/tool/stream.py
================================================
import datetime
import json
import logging
import os
from typing import Any

from deprecated import deprecated

from unstract.sdk.constants import Command, LogLevel, LogStage, ToolEnv
from unstract.sdk.utils import Utils
from unstract.sdk.utils.common_utils import UNSTRACT_TO_PY_LOG_LEVEL


class StreamMixin:
    """Helper class for streaming Unstract tool commands.

    A utility class to make writing Unstract tools easier. It provides
    methods to stream the JSON schema, properties, icon, log messages,
    cost, single step messages, and results using the Unstract protocol
    to stdout.
    """

    def __init__(self, log_level: LogLevel = LogLevel.INFO, **kwargs) -> None:
        """
        Args:
            log_level (LogLevel): The log level for filtering of log messages.
            The default is INFO.
                Allowed values are DEBUG, INFO, WARN, ERROR, and FATAL.

        """
        self.log_level = log_level
        self._exec_by_tool = Utils.str_to_bool(
            os.environ.get(ToolEnv.EXECUTION_BY_TOOL, "False")
        )
        if self.is_exec_by_tool:
            self._configure_logger()
        super().__init__(**kwargs)

    @property
    def is_exec_by_tool(self):
        """Flag to determine if SDK library is used in a tool's context.

        Returns:
            bool: True if SDK is used by a tool else False
        """
        return self._exec_by_tool

    def _configure_logger(self) -> None:
        """Helps configure the logger for the tool run."""
        rootlogger = logging.getLogger("")
        # Avoids adding multiple handlers
        if rootlogger.hasHandlers():
            return
        handler = logging.StreamHandler()
        handler.setLevel(level=UNSTRACT_TO_PY_LOG_LEVEL[self.log_level])
        handler.setFormatter(
            logging.Formatter(
                "[%(asctime)s] %(levelname)s in %(module)s: %(message)s",
            )
        )
        rootlogger.addHandler(handler)
        rootlogger.setLevel(level=UNSTRACT_TO_PY_LOG_LEVEL[self.log_level])

    def stream_log(
        self,
        log: str,
        level: LogLevel = LogLevel.INFO,
        stage: str = LogStage.TOOL_RUN,
        **kwargs: Any,
    ) -> None:
        """Streams a log message using the Unstract protocol LOG to stdout.

        Args:
            log (str): The log message.
            level (LogLevel): The log level. The default is INFO.
                Allowed values are DEBUG, INFO, WARN, ERROR, and FATAL.
            stage (str): LogStage from constant default Tool_RUN
        Returns:
            None
        """
        levels = [
            LogLevel.DEBUG,
            LogLevel.INFO,
            LogLevel.WARN,
            LogLevel.ERROR,
            LogLevel.FATAL,
        ]
        if levels.index(level) < levels.index(self.log_level):
            return

        record = {
            "type": "LOG",
            "stage": stage,
            "level": level.value,
            "log": log,
            "emitted_at": datetime.datetime.now().isoformat(),
            **kwargs,
        }
        print(json.dumps(record))

    def stream_error_and_exit(self, message: str) -> None:
        """Stream error log and exit.

        Args:
            message (str): Error message
        """
        self.stream_log(message, level=LogLevel.ERROR)
        if self._exec_by_tool:
            exit(1)
        else:
            raise RuntimeError("RuntimeError from SDK, check the above log for details")

    def get_env_or_die(self, env_key: str) -> str:
        """Returns the value of an env variable.

        If its empty or None, raises an error and exits

        Args:
            env_key (str): Key to retrieve

        Returns:
            str: Value of the env
        """
        env_value = os.environ.get(env_key)
        if env_value is None or env_value == "":
            self.stream_error_and_exit(f"Env variable '{env_key}' is required")
        return env_value

    @staticmethod
    def stream_spec(spec: str) -> None:
        """Streams JSON schema of the tool using the Unstract protocol SPEC to
        stdout.

        Args:
            spec (str): The JSON schema of the tool.
            Typically returned by the spec() method.

        Returns:
            None
        """
        record = {
            "type": "SPEC",
            "spec": spec,
            "emitted_at": datetime.datetime.now().isoformat(),
        }
        print(json.dumps(record))

    @staticmethod
    def stream_properties(properties: str) -> None:
        """Streams the properties of the tool using the Unstract protocol
        PROPERTIES to stdout.

        Args:
            properties (str): The properties of the tool.
            Typically returned by the properties() method.
        Returns:
            None
        """
        record = {
            "type": "PROPERTIES",
            "properties": properties,
            "emitted_at": datetime.datetime.now().isoformat(),
        }
        print(json.dumps(record))

    @staticmethod
    def stream_variables(variables: str) -> None:
        """Streams JSON schema of the tool's variables using the Unstract
        protocol VARIABLES to stdout.

        Args:
            variables (str): The tool's runtime variables.
            Typically returned by the spec() method.

        Returns:
            None
        """
        record = {
            "type": Command.VARIABLES,
            "variables": variables,
            "emitted_at": datetime.datetime.now().isoformat(),
        }
        print(json.dumps(record))

    @staticmethod
    def stream_icon(icon: str) -> None:
        """Streams the icon of the tool using the Unstract protocol ICON to
        stdout.

        Args:
            icon (str): The icon of the tool.
                Typically returned by the icon() method.
        Returns:
            None
        """
        record = {
            "type": "ICON",
            "icon": icon,
            "emitted_at": datetime.datetime.now().isoformat(),
        }
        print(json.dumps(record))

    @staticmethod
    def stream_update(message: str, state: str, **kwargs: Any) -> None:
        """Streams a log message using the Unstract protocol UPDATE to stdout.

        Args:
            message (str): The log message.
            state (str): LogState from constant
        """
        record = {
            "type": "UPDATE",
            "state": state,
            "message": message,
            "emitted_at": datetime.datetime.now().isoformat(),
            **kwargs,
        }
        print(json.dumps(record))

    @staticmethod
    @deprecated(version="0.4.4", reason="Unused in workflow execution")
    def stream_cost(cost: float, cost_units: str, **kwargs: Any) -> None:
        """Streams the cost of the tool using the Unstract protocol COST to
        stdout.

        Args:
            cost (float): The cost of the tool.
            cost_units (str): The cost units of the tool.
            **kwargs: Additional keyword arguments to include in the record.
        Returns:
            None
        """
        record = {
            "type": "COST",
            "cost": cost,
            "cost_units": cost_units,
            "emitted_at": datetime.datetime.now().isoformat(),
            **kwargs,
        }
        print(json.dumps(record))

    @staticmethod
    @deprecated(version="0.4.4", reason="Unused in workflow execution")
    def stream_single_step_message(message: str, **kwargs: Any) -> None:
        """Streams a single step message using the Unstract protocol
        SINGLE_STEP_MESSAGE to stdout.

        Args:
            message (str): The single step message.
            **kwargs: Additional keyword arguments to include in the record.
        Returns:
            None
        """
        record = {
            "type": "SINGLE_STEP_MESSAGE",
            "message": message,
            "emitted_at": datetime.datetime.now().isoformat(),
            **kwargs,
        }
        print(json.dumps(record))

    @staticmethod
    @deprecated(version="0.4.4", reason="Use `BaseTool.write_to_result()` instead")
    def stream_result(result: dict[Any, Any], **kwargs: Any) -> None:
        """Streams the result of the tool using the Unstract protocol RESULT to
        stdout.

        Args:
            result (dict): The result of the tool. Refer to the
                Unstract protocol for the format of the result.
            **kwargs: Additional keyword arguments to include in the record.
        Returns:
            None
        """
        record = {
            "type": "RESULT",
            "result": result,
            "emitted_at": datetime.datetime.now().isoformat(),
            **kwargs,
        }
        print(json.dumps(record))


================================================
File: src/unstract/sdk/tool/validator.py
================================================
import re
from json import JSONDecodeError
from pathlib import Path
from typing import Any

from jsonschema import Draft202012Validator, ValidationError, validators

from unstract.sdk.constants import MetadataKey, PropKey
from unstract.sdk.tool.base import BaseTool
from unstract.sdk.tool.mime_types import EXT_MIME_MAP
from unstract.sdk.utils import Utils


def extend_with_default(validator_class: Any) -> Any:
    """Extend a JSON schema validator class with a default value functionality.

    Parameters:
    - validator_class (Any): The JSON schema validator class to be extended.

    Returns:
    - Any: The extended JSON schema validator class.

    Example:
    extend_with_default(Draft202012Validator)
    """
    validate_properties = validator_class.VALIDATORS["properties"]

    def set_defaults(
        validator: Any, properties: Any, instance: Any, schema: Any
    ) -> Any:
        for property_, subschema in properties.items():
            if "default" in subschema:
                instance.setdefault(property_, subschema["default"])

        yield from validate_properties(
            validator,
            properties,
            instance,
            schema,
        )

    return validators.extend(
        validator_class,
        {"properties": set_defaults},
    )


# Helps validate a JSON against a schema and applies missing key's defaults too.
DefaultsGeneratingValidator = extend_with_default(Draft202012Validator)


class ToolValidator:
    """Class to validate a tool and its configuration before its executed with
    an input."""

    def __init__(self, tool: BaseTool) -> None:
        self.tool = tool
        props = self.tool.properties
        self.restrictions = props.get(PropKey.RESTRICTIONS)

    def validate_pre_execution(self, settings: dict[str, Any]) -> dict[str, Any]:
        """Performs validation before the tool executes on the input file.

        Args:
            settings (dict[str, Any]): Settings configured for the tool

        Returns:
            dict[str, Any]: Settings JSON for a tool (filled with defaults)
        """
        input_file = Path(self.tool.get_input_file())
        file_exists = self.tool.workflow_filestorage.exists(path=input_file)

        if not file_exists:
            self.tool.stream_error_and_exit(f"Input file not found: {input_file}")
        self._validate_restrictions(input_file)
        self._validate_settings_and_fill_defaults(settings)
        # Call tool's validation hook to execute custom validation
        self.tool.validate(str(input_file), settings)
        return settings

    def _validate_restrictions(self, input_file: Path) -> None:
        """Validates the restrictions mentioned in the tool's PROPERTIES.

        Args:
            input_file (Path): Path object to the input file to be validated
        """
        self._validate_file_size(input_file)
        self._validate_file_type(input_file)

    def _validate_settings_and_fill_defaults(
        self, tool_settings: dict[str, Any]
    ) -> None:
        """Validates and obtains settings for a tool.

        Validation is done against the tool's settings based
        on its declared SPEC. Validator also fills in the missing defaults.

        Args:
            tool_settings (dict[str, Any]): Tool settings to validate
        """
        try:
            spec_schema = self.tool.spec
            DefaultsGeneratingValidator(spec_schema).validate(tool_settings)
        except JSONDecodeError as e:
            self.tool.stream_error_and_exit(f"Settings is not a valid JSON: {str(e)}")
        except ValidationError as e:
            self.tool.stream_error_and_exit(f"Invalid settings: {str(e)}")

    def _validate_file_size(self, input_file: Path) -> None:
        """Validates the input file size against the max allowed size set in
        the tool's PROPERTIES.

        Raises:
            RuntimeError: File size exceeds max allowed size
        """
        max_file_size = self.restrictions.get(PropKey.MAX_FILE_SIZE)
        max_size_in_bytes = self._parse_size_string(max_file_size)

        self.tool.stream_log(
            f"Checking input file size... (max file size: {max_file_size})"
        )
        file_size = self.tool.workflow_filestorage.size(path=input_file)
        self.tool.stream_log(f"Input file size: {Utils.pretty_file_size(file_size)}")

        if file_size > max_size_in_bytes:
            source_name = self.tool.get_exec_metadata.get(MetadataKey.SOURCE_NAME)
            self.tool.stream_error_and_exit(
                f"File {source_name} exceeds the maximum "
                f"allowed size of {max_file_size}"
            )

    def _parse_size_string(self, size_string: str) -> int:
        """Parses the size string for validation.

        Args:
            size_string (str): Size string to be parsed

        Raises:
            ValueError: Invalid size format

        Returns:
            int: Size in bytes
        """
        size_match = re.match(PropKey.FILE_SIZE_REGEX, size_string)
        if not size_match:
            self.tool.stream_error_and_exit(
                f"Invalid size string format: {size_string}"
            )

        size, unit = size_match.groups()
        size_in_bytes = int(size)
        if unit.upper() == "KB":
            size_in_bytes *= 1024
        elif unit.upper() == "MB":
            size_in_bytes *= 1024 * 1024
        elif unit.upper() == "GB":
            size_in_bytes *= 1024 * 1024 * 1024
        elif unit.upper() == "TB":
            size_in_bytes *= 1024 * 1024 * 1024 * 1024

        return size_in_bytes

    def _validate_file_type(self, input_file: Path) -> None:
        """Validate the input file type against the allowed types mentioned in
        tool's PROPERTIES.

        Args:
            input_file (Path): Path obj of input file to validate

        Raises:
            RuntimeError: If file type is not supported by the tool
        """
        self.tool.stream_log("Checking input file type...")

        allowed_exts: list[str] = self.restrictions.get(PropKey.ALLOWED_FILE_TYPES)
        allowed_exts = [allowed_type.lower() for allowed_type in allowed_exts]
        if "*" in allowed_exts:
            self.tool.stream_log("Skipping check, tool allows all file types")
            return

        allowed_mimes = []
        for ext in allowed_exts:
            if ext not in EXT_MIME_MAP:
                self.tool.stream_error_and_exit(
                    f"{ext} mentioned in tool PROPERTIES is not supported"
                )
            allowed_mimes.append(EXT_MIME_MAP[ext])
        tool_fs = self.tool.workflow_filestorage
        input_file_mime = tool_fs.mime_type(input_file)
        self.tool.stream_log(f"Input file MIME: {input_file_mime}")
        if input_file_mime not in allowed_mimes:
            self.tool.stream_error_and_exit(
                f"File type of {input_file_mime} is not supported by"
                " the tool, check its PROPERTIES for a list of supported types"
            )


================================================
File: src/unstract/sdk/utils/__init__.py
================================================
from .common_utils import CommonUtils, Utils  # noqa
from .file_storage_utils import FileStorageUtils  # noqa
from .tool_utils import ToolUtils  # noqa


================================================
File: src/unstract/sdk/utils/callback_manager.py
================================================
import logging
from typing import Callable, Optional, Union

import tiktoken
from deprecated import deprecated
from llama_index.core.callbacks import CallbackManager as LlamaIndexCallbackManager
from llama_index.core.callbacks import TokenCountingHandler
from llama_index.core.embeddings import BaseEmbedding
from llama_index.core.llms import LLM

from unstract.sdk.utils.usage_handler import UsageHandler

logger = logging.getLogger(__name__)


class CallbackManager:
    """Class representing the CallbackManager to manage callbacks.

    Use this over the default service context of llama index

    This class supports a tokenizer, token counter,
    usage handler, and  callback manager.

    Attributes:
        None

    Methods:
        set_callback_manager: Returns a standard callback manager

    Example:
        callback_manager = CallbackManager.
                            set_callback_manager(
                                llm="default",
                                embedding="default")
    """

    @staticmethod
    def set_callback(
        platform_api_key: str,
        model: Union[LLM, BaseEmbedding],
        kwargs,
    ) -> None:
        """Sets the standard callback manager for the llm. This is to be called
        explicitly whenever there is a need for the callback handling defined
        here as handlers is to be invoked.

        Parameters:
            llm (LLM): The LLM type

        Returns:
            CallbackManager type of llama index

        Example:
            UNCallbackManager.set_callback_manager(
                platform_api_key: "abc",
                llm=llm,
                embedding=embedding
            )
        """

        # Nothing to do if callback manager is already set for the instance
        if (
            model
            and model.callback_manager
            and len(model.callback_manager.handlers) > 0
        ):
            return

        model.callback_manager = CallbackManager.get_callback_manager(
            model, platform_api_key, kwargs
        )

    @staticmethod
    def get_callback_manager(
        model: Union[LLM, BaseEmbedding],
        platform_api_key: str,
        kwargs,
    ) -> LlamaIndexCallbackManager:
        llm = None
        embedding = None
        handler_list = []
        if isinstance(model, LLM):
            llm = model
            usage_handler = UsageHandler(
                platform_api_key=platform_api_key,
                llm_model=llm,
                embed_model=embedding,
                kwargs=kwargs,
            )
            handler_list.append(usage_handler)
        elif isinstance(model, BaseEmbedding):
            embedding = model
            # Get a tokenizer
            tokenizer = CallbackManager.get_tokenizer(model)
            token_counter = TokenCountingHandler(tokenizer=tokenizer, verbose=True)
            usage_handler = UsageHandler(
                token_counter=token_counter,
                platform_api_key=platform_api_key,
                llm_model=llm,
                embed_model=embedding,
                kwargs=kwargs,
            )
            handler_list.append(token_counter)
            handler_list.append(usage_handler)

        callback_manager: LlamaIndexCallbackManager = LlamaIndexCallbackManager(
            handlers=handler_list
        )
        return callback_manager

    @staticmethod
    def get_tokenizer(
        model: Optional[Union[LLM, BaseEmbedding, None]],
        fallback_tokenizer: Callable[[str], list] = tiktoken.encoding_for_model(
            "gpt-3.5-turbo"
        ).encode,
    ) -> Callable[[str], list]:
        """Returns a tokenizer function based on the provided model.

        Args:
            model (Optional[Union[LLM, BaseEmbedding]]): The model to use for
            tokenization.

        Returns:
            Callable[[str], List]: The tokenizer function.

        Raises:
            OSError: If an error occurs while loading the tokenizer.
        """

        try:
            if isinstance(model, LLM):
                model_name: str = model.metadata.model_name
            elif isinstance(model, BaseEmbedding):
                model_name = model.model_name

            tokenizer: Callable[[str], list] = tiktoken.encoding_for_model(
                model_name
            ).encode
            return tokenizer
        except (KeyError, ValueError) as e:
            logger.warning(str(e))
            return fallback_tokenizer

    @staticmethod
    @deprecated("Use set_callback() instead")
    def set_callback_manager(
        platform_api_key: str,
        llm: Optional[LLM] = None,
        embedding: Optional[BaseEmbedding] = None,
        **kwargs,
    ) -> LlamaIndexCallbackManager:
        callback_manager: LlamaIndexCallbackManager = LlamaIndexCallbackManager()
        if llm:
            CallbackManager.set_callback(platform_api_key, model=llm, **kwargs)
            callback_manager = llm.callback_manager
        if embedding:
            CallbackManager.set_callback(platform_api_key, model=embedding, **kwargs)
            callback_manager = embedding.callback_manager
        return callback_manager


================================================
File: src/unstract/sdk/utils/common_utils.py
================================================
import functools
import logging
import time
import uuid

from unstract.sdk.constants import LogLevel
from unstract.sdk.metrics_mixin import MetricsMixin

logger = logging.getLogger(__name__)


class Utils:
    @staticmethod
    def generate_uuid() -> str:
        """Class method to get uuid."""
        return str(uuid.uuid4())

    @staticmethod
    def str_to_bool(string: str) -> bool:
        """String value of boolean to boolean.

        Useful while parsing envs to bool.

        Args:
            string (str): value like "true", "True" etc..

        Returns:
            bool
        """
        return string.lower() == "true"

    @staticmethod
    def pretty_file_size(num: float, suffix: str = "B") -> str:
        """Gets the human readable size for a file,

        Args:
            num (int): Size in bytes to parse
            suffix (str, optional): _description_. Defaults to "B".

        Returns:
            str: Human readable size
        """
        for unit in ("", "K", "M", "G", "T"):
            if abs(num) < 1024.0:
                return f"{num:.2f} {unit}{suffix}"
            num /= 1024.0
        return f"{num:.2f} {suffix}"


# TODO: Kept for backward compatibility, make use of Utils instead
class CommonUtils(Utils):
    pass


# Mapping from python log level to Unstract counterpart
PY_TO_UNSTRACT_LOG_LEVEL = {
    logging.DEBUG: LogLevel.DEBUG,
    logging.INFO: LogLevel.INFO,
    logging.WARNING: LogLevel.WARN,
    logging.ERROR: LogLevel.ERROR,
}

# Mapping from Unstract log level to python counterpart
UNSTRACT_TO_PY_LOG_LEVEL = {
    LogLevel.DEBUG: logging.DEBUG,
    LogLevel.INFO: logging.INFO,
    LogLevel.WARN: logging.WARNING,
    LogLevel.ERROR: logging.ERROR,
}


def log_elapsed(operation):
    """Adds an elapsed time log.

    Args:
        operation (str): Operation being measured
    """

    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
            finally:
                end_time = time.time()
                elapsed_time = end_time - start_time
                logger.info(f"Time taken for '{operation}': {elapsed_time:.3f}s")
            return result

        return wrapper

    return decorator


def capture_metrics(func):
    """Decorator to capture metrics at the start and end of a function."""

    @functools.wraps(func)
    def wrapper(self, *args, **kwargs):
        # Ensure the required attributes exist; if not,
        # execute the function and return its result
        if not all(
            hasattr(self, attr) for attr in ["_run_id", "_capture_metrics", "_metrics"]
        ):
            return func(self, *args, **kwargs)

        # Check if run_id exists and if metrics should be captured
        metrics_mixin = None
        time_taken_key = MetricsMixin.TIME_TAKEN_KEY
        if self._run_id and self._capture_metrics:
            metrics_mixin = MetricsMixin(run_id=self._run_id)

        try:
            result = func(self, *args, **kwargs)
        finally:
            # If metrics are being captured, collect and assign them at the end
            if metrics_mixin:
                new_metrics = metrics_mixin.collect_metrics()

                # If time_taken(s) exists in both self._metrics and new_metrics, sum it
                if (
                    self._metrics
                    and time_taken_key in self._metrics
                    and time_taken_key in new_metrics
                ):
                    previously_measured_time = self._metrics.get(time_taken_key)
                    newly_measured_time = new_metrics.get(time_taken_key)

                    # Only sum if both are valid
                    if previously_measured_time and newly_measured_time:
                        self._metrics[time_taken_key] = (
                            previously_measured_time + newly_measured_time
                        )
                    else:
                        self._metrics[time_taken_key] = None
                else:
                    # If the key isn't in self._metrics, set it to new_metrics
                    self._metrics = new_metrics

        return result

    return wrapper


================================================
File: src/unstract/sdk/utils/file_storage_utils.py
================================================
import logging

from unstract.sdk.file_storage.impl import FileStorage

logger = logging.getLogger(__name__)


class FileStorageUtils:
    @staticmethod
    def copy_file_to_destination(
        source_storage: FileStorage,
        destination_storage: FileStorage,
        source_path: str,
        destination_paths: list[str],
        chunk_size: int = 4096,
    ) -> None:
        """Copy a file from a source storage to one or more paths in a
        destination storage.

        This function reads the source file in chunks and writes each chunk to
        the specified destination paths. The function will continue until the
        entire source file is copied.

        Args:
            source_storage (FileStorage): The storage object from which
                the file is read.
            destination_storage (FileStorage): The storage object to which
                the file is written.
            source_path (str): The path of the file in the source storage.
            destination_paths (list[str]): A list of paths where the file will be
                copied in the destination storage.
            chunk_size (int, optional): The number of bytes to read per chunk.
                Default is 4096 bytes.
        """
        seek_position = 0  # Start from the beginning
        end_of_file = False

        # Loop to read and write in chunks until the end of the file
        while not end_of_file:
            # Read a chunk from the source file
            chunk = source_storage.read(
                path=source_path,
                mode="rb",
                seek_position=seek_position,
                length=chunk_size,
            )
            # Check if the end of the file has been reached
            if not chunk:
                end_of_file = True
            else:
                # Write the chunk to each destination path
                for destination_file in destination_paths:
                    destination_storage.write(
                        path=destination_file,
                        mode="ab",
                        seek_position=seek_position,
                        data=chunk,
                    )

                # Update the seek position
                seek_position += len(chunk)


================================================
File: src/unstract/sdk/utils/token_counter.py
================================================
from typing import Any, Union

from llama_index.core.callbacks.schema import EventPayload
from llama_index.core.llms import ChatResponse, CompletionResponse


class Constants:
    DEFAULT_TOKEN_COUNT = 0


class TokenCounter:
    prompt_llm_token_count: int
    completion_llm_token_count: int
    total_llm_token_count: int = 0
    total_embedding_token_count: int = 0

    def __init__(self, input_tokens, output_tokens):
        self.prompt_llm_token_count = input_tokens
        self.completion_llm_token_count = output_tokens
        self.total_llm_token_count = (
            self.prompt_llm_token_count + self.completion_llm_token_count
        )

    # TODO: Add unit test cases for the following function
    #  for ease of manintenance
    @staticmethod
    def get_llm_token_counts(payload: dict[str, Any]):
        prompt_tokens = Constants.DEFAULT_TOKEN_COUNT
        completion_tokens = Constants.DEFAULT_TOKEN_COUNT
        if EventPayload.PROMPT in payload:
            response = payload.get(EventPayload.COMPLETION)
            (
                prompt_tokens,
                completion_tokens,
            ) = TokenCounter._get_tokens_from_response(response)
        elif EventPayload.MESSAGES in payload:
            response = payload.get(EventPayload.RESPONSE)
            if response:
                (
                    prompt_tokens,
                    completion_tokens,
                ) = TokenCounter._get_tokens_from_response(response)

        token_counter = TokenCounter(
            input_tokens=prompt_tokens,
            output_tokens=completion_tokens,
        )
        return token_counter

    @staticmethod
    def _get_tokens_from_response(
        response: Union[CompletionResponse, ChatResponse, dict]
    ) -> tuple[int, int]:
        """Get the token counts from a raw response."""
        prompt_tokens, completion_tokens = 0, 0
        if isinstance(response, CompletionResponse) or isinstance(
            response, ChatResponse
        ):
            raw_response = response.raw
            if not isinstance(raw_response, dict):
                raw_response = dict(raw_response)

            usage = raw_response.get("usage", None)
        if usage is None:
            if (
                hasattr(response, "additional_kwargs")
                and "prompt_tokens" in response.additional_kwargs
            ):
                usage = response.additional_kwargs
            elif hasattr(response, "raw"):
                completion_raw = response.raw
                if ("_raw_response" in completion_raw) and hasattr(
                    completion_raw["_raw_response"], "usage_metadata"
                ):
                    usage = completion_raw["_raw_response"].usage_metadata
                    prompt_tokens = usage.prompt_token_count
                    completion_tokens = usage.candidates_token_count
                    return prompt_tokens, completion_tokens
                elif "inputTextTokenCount" in completion_raw:
                    prompt_tokens = completion_raw["inputTextTokenCount"]
                    if "results" in completion_raw:
                        result_list: list = completion_raw["results"]
                        if len(result_list) > 0:
                            result: dict = result_list[0]
                            if "tokenCount" in result:
                                completion_tokens = result.get("tokenCount", 0)
                    return prompt_tokens, completion_tokens
                else:
                    usage = response.raw
            else:
                usage = response

        if not isinstance(usage, dict):
            usage = usage.model_dump()

        possible_input_keys = (
            "prompt_tokens",
            "input_tokens",
            "prompt_eval_count",
        )
        possible_output_keys = (
            "completion_tokens",
            "output_tokens",
            "eval_count",
        )

        prompt_tokens = 0
        for input_key in possible_input_keys:
            if input_key in usage:
                prompt_tokens = int(usage[input_key])
                break

        completion_tokens = 0
        for output_key in possible_output_keys:
            if output_key in usage:
                completion_tokens = int(usage[output_key])
                break

        return prompt_tokens, completion_tokens


================================================
File: src/unstract/sdk/utils/tool_utils.py
================================================
import json
import logging
import os
import warnings
from hashlib import md5, sha256
from pathlib import Path
from typing import Any

import magic

from unstract.sdk.exceptions import FileStorageError
from unstract.sdk.file_storage import (
    FileStorage,
    FileStorageProvider,
    SharedTemporaryFileStorage,
)

logger = logging.getLogger(__name__)


class ToolUtils:
    """Class containing utility methods."""

    @staticmethod
    def hash_str(string_to_hash: Any, hash_method: str = "sha256") -> str:
        """Computes the hash for a given input string.

        Useful to hash strings needed for caching and other purposes.
        Hash method defaults to "md5"

        Args:
            string_to_hash (str): String to be hashed
            hash_method (str): Hash hash_method to use, supported ones
                - "md5"

        Returns:
            str: Hashed string
        """
        if hash_method == "md5":
            if isinstance(string_to_hash, bytes):
                return str(md5(string_to_hash).hexdigest())
            return str(md5(string_to_hash.encode()).hexdigest())
        elif hash_method == "sha256":
            if isinstance(string_to_hash, (bytes, bytearray)):
                return str(sha256(string_to_hash).hexdigest())
            return str(sha256(string_to_hash.encode()).hexdigest())
        else:
            raise ValueError(f"Unsupported hash_method: {hash_method}")

    @staticmethod
    def get_hash_from_file(
        file_path: str,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> str:
        """Computes the hash for a file.

        Uses sha256 to compute the file hash through a buffered read.

        Args:
            file_path (str): Path to file that needs to be hashed

        Returns:
            str: SHA256 hash of the file
        """

        # Adding the following DeprecationWarning manually as the package "deprecated"
        # does not support deprecation on static methods.
        warnings.warn(
            "`get_hash_from_file` is deprecated. "
            "Use `FileStorage get_hash_from_file()` instead.",
            DeprecationWarning,
        )
        return fs.get_hash_from_file(path=file_path)

    @staticmethod
    def load_json(
        file_to_load: str,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> dict[str, Any]:
        """Loads and returns a JSON from a file.

        Args:
            file_to_load (str): Path to the file containing JSON

        Returns:
            dict[str, Any]: The JSON loaded from file
        """
        file_contents: str = fs.read(path=file_to_load, mode="r", encoding="utf-8")
        loaded_json: dict[str, Any] = json.loads(file_contents)
        return loaded_json

    @staticmethod
    def dump_json(
        json_to_dump: dict[str, Any],
        file_to_dump: str,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> None:
        """Helps dump the JSON to a file.

        Args:
            json_to_dump (dict[str, Any]): Input JSON to dump
            file_to_dump (str): Path to the file to dump the JSON to
        """
        compact_json = json.dumps(json_to_dump, separators=(", ", ":"))
        fs.write(path=file_to_dump, mode="w", data=compact_json)

    @staticmethod
    def json_to_str(json_to_dump: dict[str, Any]) -> str:
        """Helps convert the JSON to a string. Useful for dumping the JSON to a
        file.

        Args:
            json_to_dump (dict[str, Any]): Input JSON to dump

        Returns:
            str: String representation of the JSON
        """
        compact_json = json.dumps(json_to_dump, separators=(",", ":"))
        return compact_json

    # ToDo: get_file_mime_type() to be removed once migrated to FileStorage
    # FileStorage has mime_type() which could be used instead.
    @staticmethod
    def get_file_mime_type(
        input_file: Path,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> str:
        """Gets the file MIME type for an input file. Uses libmagic to perform
        the same.

        Args:
            input_file (Path): Path object of the input file

        Returns:
            str: MIME type of the file
        """
        # Adding the following DeprecationWarning manually as the package "deprecated"
        # does not support deprecation on static methods.
        warnings.warn(
            "`get_file_mime_type` is deprecated. "
            "Use `FileStorage mime_type()` instead.",
            DeprecationWarning,
        )
        input_file_mime = ""
        sample_contents = fs.read(path=input_file, mode="rb", length=100)
        input_file_mime = magic.from_buffer(sample_contents, mime=True)
        return input_file_mime

    @staticmethod
    def get_file_size(
        input_file: Path,
        fs: FileStorage = FileStorage(provider=FileStorageProvider.LOCAL),
    ) -> int:
        """Gets the file size in bytes for an input file.
        Args:
            input_file (Path): Path object of the input file

        Returns:
            str: MIME type of the file
        """
        file_length = fs.size(path=input_file)
        return file_length

    # Used the same function from LLMWhisperer
    @staticmethod
    def calculate_page_count(
        pages_string: str, max_page: int = 0, min_page: int = 1
    ) -> int:
        """Calculates the total number of pages based on the input string of
        page numbers or ranges.

        Parses the input 'pages_string' to extract individual page numbers or
        ranges separated by commas.
        Supports ranges like '1-5' or open-ended ranges like '4-'.
        The 'max_page' parameter defines the upper limit for page numbers.
        The 'min_page' parameter defines the lower limit for page numbers.

        Args:
            pages_string (str): String containing page numbers or ranges
            separated by commas
            max_page (int): Upper limit for page numbers (default is 0)
            min_page (int): Lower limit for page numbers (default is 1)

        Returns:
            int: Total count of individual pages extracted from the input string
        """
        if not pages_string:
            return max_page
        pages_list: list[int] = []
        parts = pages_string.split(",")
        for part in parts:
            part = part.strip()
            if "-" in part:
                if part.startswith("-"):  # e.g., "-5"
                    end = int(part[1:])
                    end = min(end, max_page)
                    pages_list.extend(range(min_page, end + 1))
                elif part.endswith("-"):  # e.g., "4-"
                    start = int(part[:-1])
                    if start < 0:
                        start = 0
                    if max_page is None:
                        raise ValueError(
                            "max_page must be defined for open-ended ranges like '4-'"
                        )
                    pages_list.extend(range(start, max_page + 1))
                else:  # e.g., "1-5"
                    start, end = map(int, part.split("-"))
                    if start < 0:
                        start = 0
                    if end > max_page:
                        end = max_page
                    pages_list.extend(range(start, end + 1))
            else:
                pages_list.append(int(part))
        return len(pages_list)

    @staticmethod
    def get_filestorage_provider(
        var_name: str, default: str = "minio"
    ) -> FileStorageProvider:
        """Retrieve the file storage provider based on an environment
        variable."""
        provider_name = os.environ.get(var_name, default).upper()
        try:
            # Attempt to map the provider name to an enum value, case-insensitively
            return FileStorageProvider[provider_name]
        except KeyError:
            allowed_providers = ", ".join(
                [provider.name for provider in FileStorageProvider]
            )
            logger.error(
                f"Invalid provider '{provider_name}'. Allowed providers: "
                f"{allowed_providers}"
            )
            raise FileStorageError(f"Invalid provider '{provider_name}'")

    @staticmethod
    def get_filestorage_credentials(var_name: str) -> dict[str, Any]:
        """Retrieve the file storage credentials based on an environment
        variable."""
        credentials = os.environ.get(var_name, "{}")
        try:
            return json.loads(credentials)
        except json.JSONDecodeError:
            raise ValueError(
                "File storage credentials are not set properly. "
                "Please check your settings."
            )

    @staticmethod
    def get_workflow_filestorage(
        provider: FileStorageProvider,
        credentials: dict[str, Any] = {},
    ) -> SharedTemporaryFileStorage:
        """Get the file storage for the workflow."""
        return SharedTemporaryFileStorage(provider=provider, **credentials)


================================================
File: src/unstract/sdk/utils/usage_handler.py
================================================
from typing import Any, Optional

from llama_index.core.callbacks import CBEventType, TokenCountingHandler
from llama_index.core.callbacks.base_handler import BaseCallbackHandler
from llama_index.core.callbacks.schema import EventPayload
from llama_index.core.embeddings import BaseEmbedding
from llama_index.core.llms import LLM

from unstract.sdk.audit import Audit
from unstract.sdk.constants import LogLevel
from unstract.sdk.tool.stream import StreamMixin
from unstract.sdk.utils.token_counter import TokenCounter


class UsageHandler(StreamMixin, BaseCallbackHandler):
    """UsageHandler class is a subclass of BaseCallbackHandler and is
    responsible for handling usage events in the LLM or Embedding models. It
    provides methods for starting and ending traces, as well as handling event
    starts and ends.

    Attributes:
        - token_counter (TokenCountingHandler): The token counter object used
          to count tokens in the LLM or Embedding models.
        - llm_model (LLM): The LLM model object.
        - embed_model (BaseEmbedding): The embedding model object.
        - workflow_id (str): The ID of the workflow.
        - execution_id (str): The ID of the execution.
        - event_starts_to_ignore (Optional[list[CBEventType]]): A list of event
          types to ignore at the start.
        - event_ends_to_ignore (Optional[list[CBEventType]]): A list of event
          types to ignore at the end.
        - verbose (bool): A flag indicating whether to print verbose output.
    """

    def __init__(
        self,
        platform_api_key: str,
        token_counter: Optional[TokenCountingHandler] = None,
        llm_model: LLM = None,
        embed_model: BaseEmbedding = None,
        event_starts_to_ignore: Optional[list[CBEventType]] = None,
        event_ends_to_ignore: Optional[list[CBEventType]] = None,
        verbose: bool = False,
        log_level: LogLevel = LogLevel.INFO,
        kwargs: dict[Any, Any] = None,
    ) -> None:
        self.kwargs = kwargs.copy()
        self._verbose = verbose
        self.token_counter = token_counter
        self.llm_model = llm_model
        self.embed_model = embed_model
        self.platform_api_key = platform_api_key
        super().__init__(
            log_level=log_level,  # StreamMixin's args
            event_starts_to_ignore=event_starts_to_ignore or [],
            event_ends_to_ignore=event_ends_to_ignore or [],
        )

    def start_trace(self, trace_id: Optional[str] = None) -> None:
        return

    def end_trace(
        self,
        trace_id: Optional[str] = None,
        trace_map: Optional[dict[str, list[str]]] = None,
    ) -> None:
        return

    def on_event_start(
        self,
        event_type: CBEventType,
        payload: Optional[dict[str, Any]] = None,
        event_id: str = "",
        parent_id: str = "",
        kwargs: dict[Any, Any] = None,
    ) -> str:
        return event_id

    def on_event_end(
        self,
        event_type: CBEventType,
        payload: Optional[dict[str, Any]] = None,
        event_id: str = "",
        kwargs: dict[Any, Any] = None,
    ) -> None:
        """Push the usage of  LLM or Embedding to platform service."""
        if (
            event_type == CBEventType.LLM
            and event_type not in self.event_ends_to_ignore
            and payload is not None
            and EventPayload.COMPLETION in payload
        ):
            model_name = self.llm_model.metadata.model_name
            # Need to push the data to via platform service
            self.stream_log(
                log=f"Pushing llm usage for model {model_name}", level=LogLevel.DEBUG
            )
            llm_token_counter: TokenCounter = TokenCounter.get_llm_token_counts(payload)
            Audit(log_level=self.log_level).push_usage_data(
                platform_api_key=self.platform_api_key,
                token_counter=llm_token_counter,
                event_type=event_type,
                model_name=self.llm_model.metadata.model_name,
                kwargs=self.kwargs,
            )

        elif (
            event_type == CBEventType.EMBEDDING
            and event_type not in self.event_ends_to_ignore
            and payload is not None
        ):
            model_name = self.embed_model.model_name
            # Need to push the data to via platform service
            self.stream_log(
                log=f"Pushing embedding usage for model {model_name}",
                level=LogLevel.DEBUG,
            )
            Audit(log_level=self.log_level).push_usage_data(
                platform_api_key=self.platform_api_key,
                token_counter=self.token_counter,
                event_type=event_type,
                model_name=self.embed_model.model_name,
                kwargs=self.kwargs,
            )
            self.token_counter.reset_counts()


================================================
File: tests/sample.env
================================================
PLATFORM_SERVICE_HOST=http://127.0.0.1
PLATFORM_SERVICE_PORT=3001
PLATFORM_SERVICE_API_KEY=

X2TEXT_HOST=http://localhost
X2TEXT_PORT=3004

LLM_TEST_VALUES=["", "", ""]
EMBEDDING_TEST_VALUES=["", "", ""]
VECTOR_DB_TEST_VALUES=["", "", ""]
OCR_TEST_VALUES=["", ""]
X2TEXT_TEST_VALUES=["", ""]

READ_FOLDER_PATH=fsspec-test/input
WRITE_FOLDER_PATH=fsspec-test/output
RECURSION_FOLDER_PATH=fsspec-test/output/recursion-test
READ_PDF_FILE=fsspec-test/input/2.pdf
READ_TEXT_FILE=fsspec-test/input/1.txt
WRITE_PDF_FILE=fsspec-test/output/2.pdf
WRITE_TEXT_FILE=fsspec-test/output/1.txt
TEST_FOLDER=fsspec-test/test-folder
GCS_BUCKET=fsspec-test/input
TEXT_CONTENT=Writing directly from string as read_file is not passed
FILE_STORAGE_GCS='{"token": "/path/to/google/creds.json"}'
FILE_STORAGE_MINIO='{"endpoint_url": "http://localhost:9000","key": "xxxx", "secret": "xxxx"}'
FILE_STORAGE_LOCAL='{"auto_mkdir": True}'
FILE_STORAGE_AZURE = '{"azure_account_name":"xxxx","azure_access_key":"XXX","connection_string":"xxxx","azure_bucket_name":"fsspec-test"}'
FILE_STORAGE_S3 = '{"s3_key":'xxxx',"s3_secret":'XXXX',"s3_bucket":'fsspec-test',"s3_endpoint":'https://s3.ap-south-1.amazonaws.com/',"s3_region":'ap-south-1'}'
TEST_PERMANENT_STORAGE_GCS='{"provider": "gcs", "credentials": {"token": "/path/to/google/creds.json"}}'
TEST_TEMPORARY_STORAGE='{"provider": "minio", "credentials": {"endpoint_url": "http://unstract-minio:9000", "key": "xxxx", "secret": "xxxx"}}'
TEST_LOCAL_STORAGE='{"provider":"local"}'
TEST_PERMANENT_STORAGE_AZURE = '{"provider": "abfs", "credentials": {"azure_account_name":"unstractpocstorage","azure_access_key":"xxxx","azure_bucket_name":"fsspec-test","connection_string":"xxxx"}}'


================================================
File: tests/test_cache.py
================================================
import unittest
from typing import Any

from unstract.sdk.cache import ToolCache
from unstract.sdk.tool.base import BaseTool


# Requires the platform service to be run and
# PLATFORM_SERVICE_API_KEY env to be set
class UnstractToolCacheTest(unittest.TestCase):
    class MockTool(BaseTool):
        def run(
            self,
            params: dict[str, Any] = {},
            settings: dict[str, Any] = {},
            workflow_id: str = "",
        ) -> None:
            # self.stream_log("Mock tool running")
            pass

    @classmethod
    def setUpClass(cls):
        cls.tool = cls.MockTool()

    def test_set(self):
        cache = ToolCache(
            tool=self.tool, platform_host="http://localhost", platform_port=3001
        )
        result = cache.set(key="test_key", value="test_value")
        self.assertTrue(result)

    def test_get(self):
        cache = ToolCache(
            tool=self.tool, platform_host="http://localhost", platform_port=3001
        )
        cache.set(key="test_key", value="test_value")
        result = cache.get(key="test_key")
        self.assertEqual(result, "test_value")

    def test_delete(self):
        cache = ToolCache(
            tool=self.tool, platform_host="http://localhost", platform_port=3001
        )
        cache.set(key="test_key", value="test_value")
        result = cache.delete(key="test_key")
        self.assertTrue(result)
        result = cache.get(key="test_key")
        self.assertIsNone(result)


if __name__ == "__main__":
    unittest.main()


================================================
File: tests/test_embedding.py
================================================
import json
import os
import unittest

from dotenv import load_dotenv
from llama_index.core.embeddings import BaseEmbedding
from parameterized import parameterized

from unstract.sdk.embedding import ToolEmbedding
from unstract.sdk.tool.base import BaseTool

load_dotenv()


def get_test_values(env_key: str) -> list[str]:
    test_values = json.loads(os.environ.get(env_key))
    return test_values


class ToolEmbeddingTest(unittest.TestCase):
    TEST_SNIPPET = "Hello, I am Unstract"

    class MockTool(BaseTool):
        def run(
            self,
        ) -> None:
            self.stream_log("Mock tool running")

    def setUp(self) -> None:
        self.tool = self.MockTool()

    def run_embedding_test(self, adapter_instance_id):
        embedding = ToolEmbedding(tool=self.tool)
        embed_model = embedding.get_embedding(adapter_instance_id)
        self.assertIsNotNone(embed_model)
        self.assertIsInstance(embed_model, BaseEmbedding)
        response = embed_model._get_text_embedding(
            ToolEmbeddingTest.TEST_SNIPPET
        )
        self.assertIsNotNone(response)

    @parameterized.expand(get_test_values("EMBEDDING_TEST_VALUES"))
    def test_get_embedding(self, adapter_instance_id: str) -> None:
        self.run_embedding_test(adapter_instance_id)


if __name__ == "__main__":
    unittest.main()


================================================
File: tests/test_file_storage.py
================================================
import io
import json
import os.path
from json import JSONDecodeError

import pdfplumber
import pytest
from dotenv import load_dotenv

from unstract.sdk.constants import MimeType
from unstract.sdk.exceptions import FileOperationError
from unstract.sdk.file_storage.constants import FileOperationParams, StorageType
from unstract.sdk.file_storage.env_helper import EnvHelper
from unstract.sdk.file_storage.impl import FileStorage
from unstract.sdk.file_storage.provider import FileStorageProvider

load_dotenv()


class TEST_CONSTANTS:
    READ_FOLDER_PATH = os.environ.get("READ_FOLDER_PATH")
    WRITE_FOLDER_PATH = os.environ.get("WRITE_FOLDER_PATH")
    RECURSION_FOLDER_PATH = os.environ.get("RECURSION_FOLDER_PATH")
    READ_PDF_FILE = os.environ.get("READ_PDF_FILE")
    READ_TEXT_FILE = os.environ.get("READ_TEXT_FILE")
    WRITE_PDF_FILE = os.environ.get("WRITE_PDF_FILE")
    WRITE_TEXT_FILE = os.environ.get("WRITE_TEXT_FILE")
    TEST_FOLDER = os.environ.get("TEST_FOLDER")
    GCS_BUCKET = os.environ.get("GCS_BUCKET")
    TEXT_CONTENT = os.environ.get("TEXT_CONTENT")
    FILE_STORAGE_GCS = "FILE_STORAGE_GCS"
    FILE_STORAGE_MINIO = "FILE_STORAGE_MINIO"
    FILE_STORAGE_LOCAL = "FILE_STORAGE_LOCAL"
    FILE_STORAGE_S3 = "FILE_STORAGE_S3"
    FILE_STORAGE_AZURE = "FILE_STORAGE_AZURE"


def file_storage(provider: FileStorageProvider):
    try:
        if provider == FileStorageProvider.GCS:
            creds = json.loads(os.environ.get(TEST_CONSTANTS.FILE_STORAGE_GCS, "{}"))
        elif provider == FileStorageProvider.MINIO:
            creds = json.loads(os.environ.get(TEST_CONSTANTS.FILE_STORAGE_MINIO, "{}"))
        elif provider == FileStorageProvider.LOCAL:
            creds = json.loads(os.environ.get(TEST_CONSTANTS.FILE_STORAGE_LOCAL, "{}"))
        elif provider == FileStorageProvider.S3:
            creds = json.loads(os.environ.get(TEST_CONSTANTS.FILE_STORAGE_S3, "{}"))
        elif provider == FileStorageProvider.AZURE:
            creds = json.loads(os.environ.get(TEST_CONSTANTS.FILE_STORAGE_AZURE, "{}"))
    except JSONDecodeError:
        creds = {}
    file_storage = FileStorage(provider, **creds)
    assert file_storage is not None
    return file_storage


@pytest.mark.parametrize(
    "file_storage, path, mode, read_length, expected_read_length",
    [
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_PDF_FILE,
            "rb",
            -1,
            os.path.getsize(TEST_CONSTANTS.READ_PDF_FILE),
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "rb",
            0,
            0,
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "r",
            -1,
            os.path.getsize(TEST_CONSTANTS.READ_TEXT_FILE),
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "rb",
            0,
            0,
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "r",
            -1,
            os.path.getsize(TEST_CONSTANTS.READ_TEXT_FILE),
        ),
        (
            file_storage(provider=FileStorageProvider.S3),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "rb",
            0,
            os.path.getsize(TEST_CONSTANTS.READ_TEXT_FILE),
        ),
        (
            file_storage(provider=FileStorageProvider.AZURE),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "rb",
            0,
            os.path.getsize(TEST_CONSTANTS.READ_TEXT_FILE),
        ),
    ],
)
def test_file_read(file_storage, path, mode, read_length, expected_read_length):
    file_contents = file_storage.read(path=path, mode=mode, length=read_length)
    assert len(file_contents) == expected_read_length


@pytest.mark.parametrize(
    "file_storage, path, mode, read_length",
    [
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            "1.txt",
            "rb",
            -1,
        ),
    ],
)
def test_file_read_exception(file_storage, path, mode, read_length):
    with pytest.raises(FileNotFoundError):
        file_storage.read(path=path, mode=mode, length=read_length)


@pytest.mark.parametrize(
    "file_storage, read_file_path, read_mode, file_contents, "
    "write_file_path, write_mode, read_length, expected_write_length",
    [
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_PDF_FILE,
            "rb",
            None,
            TEST_CONSTANTS.WRITE_PDF_FILE,
            "wb",
            -1,
            os.path.getsize(TEST_CONSTANTS.READ_PDF_FILE),
        ),
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "rb",
            None,
            TEST_CONSTANTS.WRITE_TEXT_FILE,
            "wb",
            0,
            0,
        ),
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "rb",
            None,
            TEST_CONSTANTS.WRITE_TEXT_FILE,
            "wb",
            0,
            0,
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_PDF_FILE,
            "rb",
            None,
            TEST_CONSTANTS.WRITE_PDF_FILE,
            "wb",
            -1,
            os.path.getsize(TEST_CONSTANTS.READ_PDF_FILE),
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "rb",
            None,
            TEST_CONSTANTS.WRITE_TEXT_FILE,
            "wb",
            -1,
            os.path.getsize(TEST_CONSTANTS.READ_TEXT_FILE),
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "rb",
            None,
            TEST_CONSTANTS.WRITE_TEXT_FILE,
            "wb",
            0,
            0,
        ),
        (
            file_storage(provider=FileStorageProvider.GCS),
            None,
            "rb",
            TEST_CONSTANTS.TEXT_CONTENT,
            TEST_CONSTANTS.WRITE_TEXT_FILE,
            "w",
            0,
            len(TEST_CONSTANTS.TEXT_CONTENT),
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            None,
            "rb",
            TEST_CONSTANTS.TEXT_CONTENT.encode(),
            TEST_CONSTANTS.WRITE_TEXT_FILE,
            "wb",
            0,
            len(TEST_CONSTANTS.TEXT_CONTENT.encode()),
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_PDF_FILE,
            "rb",
            None,
            TEST_CONSTANTS.WRITE_PDF_FILE,
            "wb",
            -1,
            os.path.getsize(TEST_CONSTANTS.READ_PDF_FILE),
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "rb",
            None,
            TEST_CONSTANTS.WRITE_TEXT_FILE,
            "wb",
            0,
            0,
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "rb",
            None,
            TEST_CONSTANTS.WRITE_TEXT_FILE,
            "wb",
            0,
            0,
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            None,
            "rb",
            TEST_CONSTANTS.TEXT_CONTENT,
            TEST_CONSTANTS.WRITE_TEXT_FILE,
            "w",
            0,
            len(TEST_CONSTANTS.TEXT_CONTENT),
        ),
        (
            file_storage(provider=FileStorageProvider.AZURE),
            TEST_CONSTANTS.READ_PDF_FILE,
            "rb",
            None,
            TEST_CONSTANTS.WRITE_PDF_FILE,
            "wb",
            -1,
            os.path.getsize(TEST_CONSTANTS.READ_PDF_FILE),
        ),
        (
            file_storage(provider=FileStorageProvider.AZURE),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "rb",
            None,
            TEST_CONSTANTS.WRITE_TEXT_FILE,
            "wb",
            0,
            0,
        ),
        (
            file_storage(provider=FileStorageProvider.AZURE),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "rb",
            None,
            TEST_CONSTANTS.WRITE_TEXT_FILE,
            "wb",
            0,
            0,
        ),
        (
            file_storage(provider=FileStorageProvider.AZURE),
            None,
            "rb",
            TEST_CONSTANTS.TEXT_CONTENT,
            TEST_CONSTANTS.WRITE_TEXT_FILE,
            "w",
            0,
            len(TEST_CONSTANTS.TEXT_CONTENT),
        ),
    ],
)
def test_file_write(
    file_storage,
    read_file_path,
    read_mode,
    file_contents,
    write_file_path,
    write_mode,
    read_length,
    expected_write_length,
):
    if file_storage.exists(path=TEST_CONSTANTS.WRITE_FOLDER_PATH):
        file_storage.rm(path=TEST_CONSTANTS.WRITE_FOLDER_PATH, recursive=True)
    assert file_storage.exists(path=TEST_CONSTANTS.WRITE_FOLDER_PATH) is False
    file_storage.mkdir(path=TEST_CONSTANTS.WRITE_FOLDER_PATH)
    # assert file_storage.path_exists(path=TEST_CONSTANTS.WRITE_FOLDER_PATH) == True
    if read_file_path:
        file_contents = file_storage.read(
            path=read_file_path, mode=read_mode, length=read_length
        )
    else:
        file_contents = file_contents
    bytes_written = file_storage.write(
        path=write_file_path, mode=write_mode, data=file_contents
    )
    assert bytes_written == expected_write_length


@pytest.mark.parametrize(
    "file_storage, folder_path, expected_result",
    [
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.TEST_FOLDER,
            False,  # mkdir does not work for blob storages
            # as they only support creating buckets. For
            # further details pls check implementation of mkdir in GCSFS
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.TEST_FOLDER,
            True,
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.TEST_FOLDER,
            False,  # mkdir does not work for blob storages
            # as they only support creating buckets. For
            # further details pls check implementation of mkdir in S3
        ),
        (
            file_storage(provider=FileStorageProvider.AZURE),
            TEST_CONSTANTS.TEST_FOLDER,
            False,
        ),
    ],
)
def test_make_dir(file_storage, folder_path, expected_result):
    if file_storage.exists(path=folder_path):
        file_storage.rm(path=folder_path, recursive=True)
    file_storage.mkdir(folder_path)
    assert file_storage.exists(path=folder_path) == expected_result


@pytest.mark.parametrize(
    "file_storage, folder_path, expected_result",
    [
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.GCS_BUCKET,
            True,
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.TEST_FOLDER,
            True,
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            "dummy",
            False,
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.GCS_BUCKET,
            True,
        ),
        (
            file_storage(provider=FileStorageProvider.AZURE),
            TEST_CONSTANTS.GCS_BUCKET,
            True,
        ),
    ],
)
def test_path_exists(file_storage, folder_path, expected_result):
    assert file_storage.exists(path=folder_path) == expected_result


@pytest.mark.parametrize(
    "file_storage, folder_path, expected_file_count",
    [
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_FOLDER_PATH,
            3,
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_FOLDER_PATH,
            3,
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_TEXT_FILE,
            1,
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_FOLDER_PATH,
            2,
        ),
        (
            file_storage(provider=FileStorageProvider.AZURE),
            TEST_CONSTANTS.READ_FOLDER_PATH,
            2,
        ),
    ],
)
def test_ls(file_storage, folder_path, expected_file_count):
    assert len(file_storage.ls(folder_path)) == expected_file_count


@pytest.mark.parametrize(
    "file_storage, folder_path",
    [
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.WRITE_FOLDER_PATH,
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.WRITE_FOLDER_PATH,
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.WRITE_FOLDER_PATH,
        ),
        (
            file_storage(provider=FileStorageProvider.AZURE),
            TEST_CONSTANTS.WRITE_FOLDER_PATH,
        ),
    ],
)
def test_rm(file_storage, folder_path):
    if not file_storage.exists(path=folder_path):
        file_storage.mkdir(path=folder_path)
        if not file_storage.exists(path=folder_path):
            file_storage.fs.touch(path=folder_path)
    file_storage.rm(path=folder_path)
    assert file_storage.exists(path=folder_path) is False


@pytest.mark.parametrize(
    "file_storage, folder_path, recursive, test_folder_path",
    [
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.RECURSION_FOLDER_PATH,
            True,
            TEST_CONSTANTS.WRITE_FOLDER_PATH,
        ),
    ],
)
def test_rm_recursive(file_storage, folder_path, recursive, test_folder_path):
    if not file_storage.exists(path=folder_path):
        file_storage.mkdir(path=folder_path)
    assert file_storage.exists(path=folder_path) is True
    file_storage.rm(path=test_folder_path, recursive=recursive)
    assert file_storage.exists(path=test_folder_path) is False


@pytest.mark.parametrize(
    "file_storage, folder_path, recursive, test_folder_path",
    [
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.RECURSION_FOLDER_PATH,
            False,
            TEST_CONSTANTS.WRITE_FOLDER_PATH,
        ),
    ],
)
def test_rm_recursive_exception(file_storage, folder_path, recursive, test_folder_path):
    if not file_storage.exists(path=folder_path):
        file_storage.mkdir(path=folder_path)
    assert file_storage.exists(path=folder_path) is True
    with pytest.raises(FileOperationError):
        file_storage.rm(path=test_folder_path, recursive=recursive)


@pytest.mark.parametrize(
    "file_storage, file_path, mode, location, whence, expected_size",
    [
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "rb",
            1,
            0,
            os.path.getsize(TEST_CONSTANTS.READ_TEXT_FILE) - 1,
        ),
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_PDF_FILE,
            "r",
            0,
            2,
            0,
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "rb",
            1,
            0,
            os.path.getsize(TEST_CONSTANTS.READ_TEXT_FILE) - 1,
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_PDF_FILE,
            "r",
            0,
            2,
            0,
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "rb",
            1,
            0,
            os.path.getsize(TEST_CONSTANTS.READ_TEXT_FILE) - 1,
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_PDF_FILE,
            "r",
            0,
            2,
            0,
        ),
    ],
)
def test_seek_file(file_storage, file_path, mode, location, whence, expected_size):
    seek_position = file_storage.seek(file_path, location, whence)
    file_contents = file_storage.read(
        path=file_path, mode=mode, seek_position=seek_position
    )
    print(file_contents)
    assert len(file_contents) == expected_size


@pytest.mark.parametrize("provider", [(FileStorageProvider.GCS)])
def test_file(provider):
    os.environ.clear()
    file_storage = FileStorage(provider=provider)
    assert file_storage is not None
    with pytest.raises(FileOperationError):
        file_storage.exists(TEST_CONSTANTS.READ_PDF_FILE)
    load_dotenv()


@pytest.mark.parametrize(
    "file_storage, lpath, rpath, recursive, expected_result",
    [
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_TEXT_FILE,
            TEST_CONSTANTS.TEST_FOLDER,
            True,
            True,
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_TEXT_FILE,
            TEST_CONSTANTS.TEST_FOLDER,
            True,
            True,
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_FOLDER_PATH,
            TEST_CONSTANTS.TEST_FOLDER,
            True,
            True,
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_FOLDER_PATH,
            TEST_CONSTANTS.TEST_FOLDER,
            False,
            False,
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_TEXT_FILE,
            TEST_CONSTANTS.TEST_FOLDER,
            True,
            True,
        ),
    ],
)
def test_cp(file_storage, lpath, rpath, recursive, expected_result):
    file_storage.cp(lpath, rpath, recursive=recursive, overwrite=True)
    actual_result = file_storage.exists(rpath)
    assert actual_result == expected_result
    if actual_result:
        file_storage.rm(rpath, recursive=True)
    assert file_storage.exists(rpath) is False


def test_pdf_read():
    fs = file_storage(FileStorageProvider.LOCAL)
    pdf_contents = io.BytesIO(fs.read(path=TEST_CONSTANTS.READ_PDF_FILE, mode="rb"))
    page_count = 0
    with pdfplumber.open(pdf_contents) as pdf:
        # calculate the number of pages
        page_count = len(pdf.pages)
        print(f"Page count: {page_count}")
    assert page_count == 7


@pytest.mark.parametrize(
    "file_storage, path, expected_size",
    [
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_PDF_FILE,
            os.path.getsize(TEST_CONSTANTS.READ_PDF_FILE),
        ),
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_TEXT_FILE,
            os.path.getsize(TEST_CONSTANTS.READ_TEXT_FILE),
        ),
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_PDF_FILE,
            os.path.getsize(TEST_CONSTANTS.READ_PDF_FILE),
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_TEXT_FILE,
            os.path.getsize(TEST_CONSTANTS.READ_TEXT_FILE),
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_TEXT_FILE,
            os.path.getsize(TEST_CONSTANTS.READ_TEXT_FILE),
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_PDF_FILE,
            os.path.getsize(TEST_CONSTANTS.READ_PDF_FILE),
        ),
    ],
)
def test_file_size(file_storage, path, expected_size):
    file_size = file_storage.size(path=path)
    assert file_size == expected_size


@pytest.mark.parametrize(
    "file_storage, path, read_length, expected_mime_type",
    [
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_PDF_FILE,
            FileOperationParams.READ_ENTIRE_LENGTH,
            MimeType.PDF,
        ),
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_TEXT_FILE,
            FileOperationParams.READ_ENTIRE_LENGTH,
            MimeType.TEXT,
        ),
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_PDF_FILE,
            FileOperationParams.READ_ENTIRE_LENGTH,
            MimeType.PDF,
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_TEXT_FILE,
            50,
            MimeType.TEXT,
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_PDF_FILE,
            100,
            MimeType.PDF,
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_TEXT_FILE,
            100,
            MimeType.TEXT,
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_PDF_FILE,
            100,
            MimeType.PDF,
        ),
    ],
)
def test_file_mime_type(file_storage, path, read_length, expected_mime_type):
    mime_type = file_storage.mime_type(path=path)
    file_storage.mkdir(path=TEST_CONSTANTS.READ_FOLDER_PATH)
    assert mime_type == expected_mime_type
    mime_type_read_length = file_storage.mime_type(path=path, read_length=read_length)
    file_storage.mkdir(path=TEST_CONSTANTS.READ_FOLDER_PATH)
    assert mime_type_read_length == expected_mime_type


@pytest.mark.parametrize(
    "file_storage, from_path, to_path",
    [
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_TEXT_FILE,
            TEST_CONSTANTS.TEST_FOLDER + "/1.txt",
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_TEXT_FILE,
            TEST_CONSTANTS.TEST_FOLDER + "/2.txt",
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_TEXT_FILE,
            TEST_CONSTANTS.TEST_FOLDER + "/3.txt",
        ),
        (
            file_storage(provider=FileStorageProvider.AZURE),
            TEST_CONSTANTS.READ_TEXT_FILE,
            TEST_CONSTANTS.TEST_FOLDER + "/3.txt",
        ),
    ],
)
def test_download(file_storage, from_path, to_path):
    local_file_storage = FileStorage(FileStorageProvider.LOCAL)
    if local_file_storage.exists(to_path):
        local_file_storage.rm(to_path, recursive=True)
    assert local_file_storage.exists(to_path) is False
    file_storage.download(from_path, to_path)
    assert local_file_storage.exists(to_path) is True


@pytest.mark.parametrize(
    "file_storage, from_path, to_path",
    [
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_TEXT_FILE,
            TEST_CONSTANTS.TEST_FOLDER + "/1.txt",
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_TEXT_FILE,
            TEST_CONSTANTS.TEST_FOLDER + "/2.txt",
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_TEXT_FILE,
            TEST_CONSTANTS.TEST_FOLDER + "/3.txt",
        ),
        (
            file_storage(provider=FileStorageProvider.AZURE),
            TEST_CONSTANTS.READ_TEXT_FILE,
            TEST_CONSTANTS.TEST_FOLDER + "/3.txt",
        ),
    ],
)
def test_upload(file_storage, from_path, to_path):
    local_file_storage = FileStorage(FileStorageProvider.LOCAL)
    assert local_file_storage.exists(from_path) is True
    if file_storage.exists(to_path):
        file_storage.rm(to_path, recursive=True)
    file_storage.upload(from_path, to_path)
    assert file_storage.exists(to_path) is True


@pytest.mark.parametrize(
    "file_storage, file_path, expected_result",
    [
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "4a08b5721f75657eb883202cae16c74ca62df2c605e4126e50f4bf341d4fd693",
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "4a08b5721f75657eb883202cae16c74ca62df2c605e4126e50f4bf341d4fd693",
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "4a08b5721f75657eb883202cae16c74ca62df2c605e4126e50f4bf341d4fd693",
        ),
        (
            file_storage(provider=FileStorageProvider.AZURE),
            TEST_CONSTANTS.READ_TEXT_FILE,
            "4a08b5721f75657eb883202cae16c74ca62df2c605e4126e50f4bf341d4fd693",
        ),
    ],
)
def test_get_hash_from_file(file_storage, file_path, expected_result):
    actual_file_hash = file_storage.get_hash_from_file(file_path)
    assert actual_file_hash == expected_result


@pytest.mark.parametrize(
    "file_storage, folder_path, expected_result",
    [
        (
            file_storage(provider=FileStorageProvider.GCS),
            TEST_CONSTANTS.READ_FOLDER_PATH + "/*.pdf",
            1,
        ),
        (
            file_storage(provider=FileStorageProvider.LOCAL),
            TEST_CONSTANTS.READ_FOLDER_PATH + "/*.txt",
            2,
        ),
        (
            file_storage(provider=FileStorageProvider.MINIO),
            TEST_CONSTANTS.READ_FOLDER_PATH + "/*.pdf",
            1,
        ),
        (
            file_storage(provider=FileStorageProvider.AZURE),
            TEST_CONSTANTS.READ_FOLDER_PATH + "/*.pdf",
            1,
        ),
    ],
)
def test_glob(file_storage, folder_path, expected_result):
    file_list = file_storage.glob(path=folder_path)
    print(f"Files: {file_list}")
    assert len(file_list) == expected_result


@pytest.mark.parametrize(
    "storage_type, env_name, expected",
    [
        (
            StorageType.PERMANENT,
            "TEST_PERMANENT_STORAGE_GCS",
            FileStorageProvider.GCS,
        ),
        (
            StorageType.SHARED_TEMPORARY,
            "TEST_TEMPORARY_STORAGE",
            FileStorageProvider.MINIO,
        ),
        (
            StorageType.PERMANENT,
            "TEST_LOCAL_STORAGE",
            FileStorageProvider.LOCAL,
        ),
        (
            StorageType.PERMANENT,
            "TEST_PERMANENT_STORAGE_AZURE",
            FileStorageProvider.AZURE,
        ),
    ],
)
def test_get_storage(storage_type, env_name, expected):
    file_storage = EnvHelper.get_storage(storage_type, env_name)
    assert file_storage.provider == expected
    print(file_storage)


@pytest.mark.parametrize(
    "storage_type, env_name, path",
    [
        (
            StorageType.PERMANENT,
            "TEST_PERMANENT_STORAGE_GCS",
            "fsspec-test",
        ),
        (
            StorageType.SHARED_TEMPORARY,
            "TEST_TEMPORARY_STORAGE",
            "unstract/execution/mock_org/"
            "13484b52-2127-48c2-b1a3-b517365346c3/"
            "39fcdcba-90bb-44ce-9446-67253adcb4d7/COPY_TO_FOLDER",
        ),
    ],
)
def test_dir_walk(storage_type, env_name, path):
    file_storage = EnvHelper.get_storage(storage_type, env_name)
    try:
        root, dirs, files = next(file_storage.walk(path))
    except StopIteration:
        return []
    for dir_name in dirs:
        print(dir_name)
    for file_name in files:
        print(file_name)
    if storage_type == StorageType.PERMANENT:
        assert len(files) > 0
    elif storage_type == StorageType.SHARED_TEMPORARY:
        assert len(files) == 0


def list_print_dir(file_storage, path, iter_num):
    print(f"PATH: {path}")
    print(f"\nItertion: {iter_num}")
    try:
        root, dirs, files = next(file_storage.walk(path))
    except StopIteration:
        return []
    for dir_name in dirs:
        print(dir_name)
    for file_name in files:
        print(file_name)
    print(f"Files: {files}")


@pytest.mark.parametrize(
    "storage_type, env_name, path",
    [
        (
            StorageType.SHARED_TEMPORARY,
            "TEST_TEMPORARY_STORAGE",
            "unstract/execution/mock_org/"
            "13484b52-2127-48c2-b1a3-b517365346c3/b"
            "f7b3d81-d0aa-4e9e-883d-25dd0f3a6466/COPY_TO_FOLDER",
        ),
    ],
)
def test_dir_ls(storage_type, env_name, path):
    new_file = os.path.join(path, "tmp.txt")
    file_storage = EnvHelper.get_storage(storage_type, env_name)
    if file_storage.exists(new_file):
        file_storage.rm(new_file)
    list_print_dir(file_storage, path, "1")
    file_storage.write(new_file, "w", data="Hello")
    list_print_dir(file_storage, path, "2")


================================================
File: tests/test_fs_permanent.py
================================================
import json
import os.path
from json import JSONDecodeError

import pytest
from dotenv import load_dotenv

from unstract.sdk.file_storage import FileStorageProvider, PermanentFileStorage

load_dotenv()


class TEST_CONSTANTS:
    READ_FOLDER_PATH = os.environ.get("READ_FOLDER_PATH")
    WRITE_FOLDER_PATH = os.environ.get("WRITE_FOLDER_PATH")
    RECURSION_FOLDER_PATH = os.environ.get("RECURSION_FOLDER_PATH")
    READ_PDF_FILE = os.environ.get("READ_PDF_FILE")
    READ_TEXT_FILE = os.environ.get("READ_TEXT_FILE")
    WRITE_PDF_FILE = os.environ.get("WRITE_PDF_FILE")
    WRITE_TEXT_FILE = os.environ.get("WRITE_TEXT_FILE")
    TEST_FOLDER = os.environ.get("TEST_FOLDER")
    GCS_BUCKET = os.environ.get("GCS_BUCKET")
    TEXT_CONTENT = os.environ.get("TEXT_CONTENT")
    FILE_STORAGE_GCS = "FILE_STORAGE_GCS"
    FILE_STORAGE_MINIO = "FILE_STORAGE_MINIO"
    FILE_STORAGE_LOCAL = "FILE_STORAGE_LOCAL"


def permanent_file_storage(provider: FileStorageProvider):
    try:
        if provider == FileStorageProvider.GCS:
            creds = json.loads(os.environ.get(TEST_CONSTANTS.FILE_STORAGE_GCS, "{}"))
        elif provider == FileStorageProvider.LOCAL:
            creds = json.loads(os.environ.get(TEST_CONSTANTS.FILE_STORAGE_LOCAL, "{}"))
        elif provider == FileStorageProvider.MINIO:
            creds = json.loads(os.environ.get(TEST_CONSTANTS.FILE_STORAGE_MINIO, "{}"))
    except JSONDecodeError:
        creds = {}
    file_storage = PermanentFileStorage(provider=provider, **creds)
    assert file_storage is not None
    return file_storage


@pytest.mark.parametrize(
    "file_storage, file_read_path, read_mode",
    [
        (
            permanent_file_storage(provider=FileStorageProvider.GCS),
            "fsspec-test/input/3.txt",
            "r",
        ),
        (
            permanent_file_storage(provider=FileStorageProvider.MINIO),
            "fsspec-test/input/3.txt",
            "r",
        ),
        (
            permanent_file_storage(provider=FileStorageProvider.AZURE),
            "fsspec-test/input/3.txt",
            "r",
        ),
    ],
)
def test_permanent_fs_copy_on_read(file_storage, file_read_path, read_mode):
    if file_storage.exists(file_read_path):
        file_storage.rm(file_read_path)
    with pytest.raises(FileNotFoundError):
        file_storage.read(
            file_read_path,
            read_mode,
        )


@pytest.mark.parametrize(
    "file_storage, file_read_path, read_mode, legacy_storage_path, "
    "file_write_path, write_mode",
    [
        (
            permanent_file_storage(provider=FileStorageProvider.GCS),
            "fsspec-test/input/3.txt",
            "r",
            "fsspec-test/legacy_storage/3.txt",
            "fsspec-test/output/copy_on_read_legacy_storage.txt",
            "w",
        ),
        (
            permanent_file_storage(provider=FileStorageProvider.MINIO),
            "fsspec-test/input/3.txt",
            "r",
            "fsspec-test/legacy_storage/3.txt",
            "fsspec-test/output/copy_on_read_legacy_storage.txt",
            "w",
        ),
        (
            permanent_file_storage(provider=FileStorageProvider.AZURE),
            "fsspec-test/input/3.txt",
            "r",
            "fsspec-test/legacy_storage/3.txt",
            "fsspec-test/output/copy_on_read_legacy_storage.txt",
            "w",
        ),
    ],
)
def test_permanent_fs_copy_on_read_with_legacy_storage(
    file_storage,
    file_read_path,
    read_mode,
    legacy_storage_path,
    file_write_path,
    write_mode,
):
    if file_storage.exists(file_read_path):
        file_storage.rm(file_read_path)
    file_read_contents = file_storage.read(
        file_read_path, read_mode, legacy_storage_path=legacy_storage_path
    )
    print(file_read_contents)
    if file_storage.exists(file_write_path):
        file_storage.rm(file_write_path)
    file_storage.write(file_write_path, write_mode, data=file_read_contents)

    file_write_contents = file_storage.read(file_write_path, read_mode)
    assert len(file_read_contents) == len(file_write_contents)


@pytest.mark.parametrize(
    "file_storage, file_read_path, read_mode, file_write_path, write_mode",
    [
        (
            permanent_file_storage(provider=FileStorageProvider.LOCAL),
            "fsspec-test/input/3.txt",
            "r",
            "fsspec-test/output/copy_on_write.txt",
            "w",
        ),
    ],
)
def test_permanent_fs_copy(
    file_storage, file_read_path, read_mode, file_write_path, write_mode
):
    file_read_contents = file_storage.read(file_read_path, read_mode)
    print(file_read_contents)
    if file_storage.exists(file_write_path):
        file_storage.rm(file_write_path)
    file_storage.write(file_write_path, write_mode, data=file_read_contents)
    file_write_contents = file_storage.read(file_write_path, read_mode)
    assert len(file_read_contents) == len(file_write_contents)


@pytest.mark.parametrize(
    "file_storage, from_path, read_mode, to_path, write_mode",
    [
        (
            permanent_file_storage(provider=FileStorageProvider.GCS),
            "fsspec-test/input/3.txt",
            "r",
            "fsspec-test/output/test_write.txt",
            "w",
        ),
        (
            permanent_file_storage(provider=FileStorageProvider.MINIO),
            "fsspec-test/input/3.txt",
            "r",
            "fsspec-test/output/test_write.txt",
            "w",
        ),
        (
            permanent_file_storage(provider=FileStorageProvider.AZURE),
            "fsspec-test/input/3.txt",
            "r",
            "fsspec-test/output/test_write.txt",
            "w",
        ),
    ],
)
def test_permanent_fs_download(file_storage, from_path, read_mode, to_path, write_mode):
    file_read_contents = file_storage.read(from_path, read_mode)
    print(file_read_contents)
    file_storage.download(from_path, to_path)
    local_file_storage = permanent_file_storage(provider=FileStorageProvider.LOCAL)
    file_write_contents = local_file_storage.read(to_path, read_mode)
    assert len(file_read_contents) == len(file_write_contents)


@pytest.mark.parametrize(
    "provider",
    [(FileStorageProvider.GCS), (FileStorageProvider.LOCAL)],
)
def test_permanent_supported_file_storage_mode(provider):
    file_storage = permanent_file_storage(provider=provider)
    assert file_storage is not None and isinstance(file_storage, PermanentFileStorage)


================================================
File: tests/test_index.py
================================================
import json
import logging
import os
import unittest
from typing import Any, Optional
from unittest.mock import Mock, patch

from dotenv import load_dotenv
from parameterized import parameterized

from unstract.sdk.index import Index
from unstract.sdk.tool.base import BaseTool

load_dotenv()

logger = logging.getLogger(__name__)


def get_test_values(env_key: str) -> list[str]:
    test_values = json.loads(os.environ.get(env_key))
    return test_values


class ToolLLMTest(unittest.TestCase):
    class MockTool(BaseTool):
        def run(
            self,
            params: dict[str, Any] = {},
            settings: dict[str, Any] = {},
            workflow_id: str = "",
        ) -> None:
            # self.stream_log("Mock tool running")
            pass

    @classmethod
    def setUpClass(cls):
        cls.tool = cls.MockTool()

    @patch(
        "unstract.sdk.index.Index.generate_index_key",
        Mock(
            return_value="77843eb8d9e30ad56bfcb018c2633fa32feef2f0c09762b6b820c75664b64c1b"
        ),
    )
    def test_generate_file_id(self):
        expected = "77843eb8d9e30ad56bfcb018c2633fa32feef2f0c09762b6b820c75664b64c1b"
        index = Index(tool=self.tool)
        actual = index.generate_file_id(
            tool_id="8ac26867-7811-4dc7-a17b-b16d3b561583",
            vector_db="81f1f6a8-cae8-4b8e-b2a4-57f80de512f6",
            embedding="ecf998d6-ded0-4aca-acd1-372a21daf0f9",
            x2text="59bc55fc-e2a7-48dd-ae93-794b4d81d46e",
            chunk_size="1024",
            chunk_overlap="128",
            file_path="/a/b/c",
            file_hash="045b1a67824592b67426f8e60c1f8328e8d2a35139f9983e0aa0a7b6f10915c3",
        )
        assert expected == actual

    test_data = [
        {
            "vector_db": "81f1f6a8-cae8-4b8e-b2a4-57f80de512f6",
            "embedding": "ecf998d6-ded0-4aca-acd1-372a21daf0f9",
            "x2text": "59bc55fc-e2a7-48dd-ae93-794b4d81d46e",
            "chunk_size": "1024",
            "chunk_overlap": "128",
            "file_path": "/a/b/c",
        },
        {
            "vector_db": "81f1f6a8-cae8-4b8e-b2a4-57f80de512f6",
            "embedding": "ecf998d6-ded0-4aca-acd1-372a21daf0f9",
            "x2text": "59bc55fc-e2a7-48dd-ae93-794b4d81d46e",
            "chunk_size": "1024",
            "chunk_overlap": "128",
            "file_path": "/a/b/c",
            "file_hash": "045b1a67824592b67426f8e60c1f8328e8d2a35139f9983e0aa0a7b6f10915c3",
        },
    ]

    @parameterized.expand(test_data)
    @patch(
        "unstract.sdk.adapter.ToolAdapter.get_adapter_config",
        Mock(return_value={}),
    )
    @patch(
        "unstract.sdk.utils.ToolUtils.hash_str",
        Mock(
            return_value="77843eb8d9e30ad56bfcb018c2633fa32feef2f0c09762b6b820c75664b64c1b"
        ),
    )
    @patch(
        "unstract.sdk.utils.ToolUtils.get_hash_from_file",
        Mock(
            return_value="ab940bb34a60d2a7876dd8e1bd1b22f5dc85936a9e2af3c49bfc888a1d944ff0"
        ),
    )
    def test_generate_index_key(
        self,
        vector_db: str,
        embedding: str,
        x2text: str,
        chunk_size: str,
        chunk_overlap: str,
        file_path: Optional[str] = None,
        file_hash: Optional[str] = None,
    ):
        expected = "77843eb8d9e30ad56bfcb018c2633fa32feef2f0c09762b6b820c75664b64c1b"
        index = Index(tool=self.tool)
        actual = index.generate_index_key(
            vector_db,
            embedding,
            x2text,
            chunk_size,
            chunk_overlap,
            file_path,
            file_hash,
        )
        assert expected == actual


if __name__ == "__main__":
    unittest.main()


================================================
File: tests/test_llm.py
================================================
import json
import logging
import os
import unittest
from typing import Any

from dotenv import load_dotenv
from parameterized import parameterized
from unstract.adapters.llm.helper import LLMHelper

from unstract.sdk.llm import ToolLLM
from unstract.sdk.tool.base import BaseTool

load_dotenv()

logger = logging.getLogger(__name__)


def get_test_values(env_key: str) -> list[str]:
    test_values = json.loads(os.environ.get(env_key))
    return test_values


class ToolLLMTest(unittest.TestCase):
    class MockTool(BaseTool):
        def run(
            self,
            params: dict[str, Any] = {},
            settings: dict[str, Any] = {},
            workflow_id: str = "",
        ) -> None:
            # self.stream_log("Mock tool running")
            pass

    @classmethod
    def setUpClass(cls):
        cls.tool = cls.MockTool()

    @parameterized.expand(
        get_test_values("LLM_TEST_VALUES")
        # AzureOpenAI (Works)
        # OpenAI (Works)
        # AnyScale (llm FAILS)
        # Anthropic (llm.complete FAILS)
        # 1. unsupported params: max_token, stop.
        # TypeError: create() got an unexpected keyword argument
        # 'max_tokens'
        # 2. anthropic.APIConnectionError: Connection error.
        # PaLM (Works)
        # Errors
        # 1. unexpected keyword argument 'max_tokens', 'stop'
        # Replicate (llm.complete FAILS)
        # Errors
        # 1. replicate.exceptions.ReplicateError:
        # You did not pass an authentication token
        # Mistral (llm.complete FAILS)
        # Errors
        # 1.TypeError: chat() got an unexpected keyword argument 'stop'
    )
    def test_get_llm(self, adapter_instance_id):
        tool_llm = ToolLLM(tool=self.tool)
        llm = tool_llm.get_llm(adapter_instance_id)
        self.assertIsNotNone(llm)

        result = LLMHelper.test_llm_instance(llm)
        logger.error(result)
        self.assertEqual(result, True)


if __name__ == "__main__":
    unittest.main()


================================================
File: tests/test_ocr.py
================================================
import json
import logging
import os
import unittest
from typing import Any

from dotenv import load_dotenv
from parameterized import parameterized

from unstract.sdk.ocr import OCR
from unstract.sdk.tool.base import BaseTool

load_dotenv()

logger = logging.getLogger(__name__)


def get_test_values(env_key: str) -> list[str]:
    values = json.loads(os.environ.get(env_key))
    return values


def get_env_value(env_key: str) -> str:
    value = os.environ.get(env_key)
    return value


class ToolOCRTest(unittest.TestCase):
    class MockTool(BaseTool):
        def run(
            self,
            params: dict[str, Any] = {},
            settings: dict[str, Any] = {},
            workflow_id: str = "",
        ) -> None:
            pass

    @classmethod
    def setUpClass(cls):
        cls.tool = cls.MockTool()

    @parameterized.expand(get_test_values("OCR_TEST_VALUES"))
    def test_get_ocr(self, adapter_instance_id):
        tool_ocr = OCR(tool=self.tool)
        ocr = tool_ocr.get_ocr(adapter_instance_id)
        result = ocr.test_connection()
        self.assertTrue(result)
        input_file = get_env_value("INPUT_FILE_PATH")
        output_file = get_env_value("OUTPUT_FILE_PATH")
        if os.path.isfile(output_file):
            os.remove(output_file)
        output = ocr.process(input_file, output_file)
        file_size = os.path.getsize(output_file)
        self.assertGreater(file_size, 0)
        if os.path.isfile(output_file):
            os.remove(output_file)
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(output)
            f.close()
        file_size = os.path.getsize(output_file)
        self.assertGreater(file_size, 0)


if __name__ == "__main__":
    unittest.main()


================================================
File: tests/test_vector_db.py
================================================
import json
import logging
import os
import unittest

from dotenv import load_dotenv
from llama_index.core import MockEmbedding
from llama_index.core.vector_stores.types import (
    BasePydanticVectorStore,
    VectorStore,
)
from parameterized import parameterized
from unstract.adapters.vectordb.helper import VectorDBHelper

from unstract.sdk.tool.base import BaseTool
from unstract.sdk.vector_db import ToolVectorDB

load_dotenv()

logger = logging.getLogger(__name__)


def get_test_values(env_key: str) -> list[str]:
    test_values = json.loads(os.environ.get(env_key))
    return test_values


class ToolVectorDBTest(unittest.TestCase):
    class MockTool(BaseTool):
        def run(
            self,
        ) -> None:
            self.stream_log("Mock tool running")

    def setUp(self) -> None:
        self.tool = self.MockTool()

    @parameterized.expand(
        get_test_values("VECTOR_DB_TEST_VALUES")
        # Works for Qdrant and Postgres
    )
    def test_get_vector_db(self, adapter_instance_id: str) -> None:
        mock_embedding = MockEmbedding(embed_dim=1)
        unstract_tool_vector_db = ToolVectorDB(tool=self.tool)
        vector_store = unstract_tool_vector_db.get_vector_db(
            adapter_instance_id, mock_embedding.embed_dim
        )
        self.assertIsNotNone(vector_store)
        self.assertIsInstance(
            vector_store, (BasePydanticVectorStore, VectorStore)
        )

        result = VectorDBHelper.test_vector_db_instance(vector_store)
        self.assertEqual(result, True)


if __name__ == "__main__":
    unittest.main()


================================================
File: tests/test_x2text.py
================================================
import json
import logging
import os
import unittest
from typing import Any

from dotenv import load_dotenv
from parameterized import parameterized

from unstract.sdk.tool.base import BaseTool
from unstract.sdk.x2txt import X2Text

load_dotenv()

logger = logging.getLogger(__name__)


def get_test_values(env_key: str) -> list[str]:
    values = json.loads(os.environ.get(env_key))
    return values


def get_env_value(env_key: str) -> str:
    value = os.environ.get(env_key)
    return value


class ToolX2TextTest(unittest.TestCase):
    class MockTool(BaseTool):
        def run(
            self,
            params: dict[str, Any] = {},
            settings: dict[str, Any] = {},
            workflow_id: str = "",
        ) -> None:
            # Dummify method for dummy tool
            pass

    @classmethod
    def setUpClass(cls):
        cls.tool = cls.MockTool()

    @parameterized.expand(get_test_values("X2TEXT_TEST_VALUES"))
    def test_get_x2text(self, adapter_instance_id):
        tool_x2text = X2Text(tool=self.tool)
        x2text = tool_x2text.get_x2text(adapter_instance_id)
        self.assertIsNotNone(x2text)
        self.assertTrue(x2text.test_connection())

        input_file = get_env_value("INPUT_FILE_PATH")
        output_file = get_env_value("OUTPUT_FILE_PATH")

        if os.path.isfile(output_file):
            os.remove(output_file)
        file_content = x2text.process(input_file, output_file)
        file_size = os.path.getsize(output_file)
        self.assertGreater(file_size, 0)

        if os.path.isfile(output_file):
            os.remove(output_file)
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(file_content)
            f.close()
        file_size = os.path.getsize(output_file)
        self.assertGreater(file_size, 0)


if __name__ == "__main__":
    unittest.main()


================================================
File: tests/config/properties.json
================================================
{
  "schemaVersion": "0.0.1",
  "displayName": "<TODO: Enter the display name of the tool>",
  "functionName": "<TODO: Enter the unique function name of the tool>",
  "toolVersion": "0.0.1",
  "description": "<TODO: Enter the tool's description, which will be fed into an LLM for workflow generation>",
  "input": {
    "description": "<TODO: Description of the input file to a tool>"
  },
  "output": {
    "description": "<TODO: Description of what the tool outputs and requires to be copied to a destination>"
  },
  "result": {
    "type": "<TODO: The result type returned from a tool, can be one of JSON/TEXT>",
    "description": "<TODO: Description of what the tool returns>",
    "schema": {}
  },
  "adapter": {
    "languageModel": {
      "isEnabled": false,
      "title": "<TODO: The title to be expected in tool settings>",
      "isRequired": false,
      "description": "<TODO: The description for adapter>"
    },
    "embeddingService": {
      "isEnabled": false
    },
    "vectorStore": {
      "isEnabled": false
    }
  },
  "ioCompatibility": {
    "api": {
      "sourceSupport": true,
      "destinationSupport": true,
      "additionalArgs": {
        "sync": true
      }
    },
    "file": {
      "sourceSupport": true,
      "destinationSupport": true,
      "additionalArgs": {}
    },
    "db": {
      "destinationSupport": true,
      "additionalArgs": {}
    }
  },
  "restrictions": {
    "maxFileSize": "<TODO: Allowed input file size it can process in B, KB, MB or GB. For eg: 10MB>",
    "allowedFileTypes": [
      "<TODO: List of allowed input file types it can process, * represents all types. For eg: pdf, txt.etc.>"
    ]
  }
}


================================================
File: tests/config/runtime_variables.json
================================================
{
    "title": "Runtime Variables",
    "description": "Runtime Variables for mock tool",
    "type": "object",
    "required": [
      "MOCK_KEY"
    ],
    "properties": {
      "MOCK_KEY": {
        "type": "string",
        "title": "Mock API Key",
        "description": "Your mock API key"
      }
    }
  }


================================================
File: tests/config/spec.json
================================================
{
  "title": "<TODO: Add title>",
  "description": "<TODO: Add description>",
  "type": "object",
  "required": [
  ],
  "properties": {
  }
}


================================================
File: tests/fsspec-test/input/1.txt
================================================
Hello, This is a test message.


================================================
File: tests/fsspec-test/input/3.txt
================================================
This is a copy test.


================================================
File: tests/tool/test_entrypoint.py
================================================
import unittest
from io import StringIO
from typing import Any
from unittest.mock import patch

from unstract.sdk.constants import LogLevel
from unstract.sdk.tool.base import BaseTool
from unstract.sdk.tool.entrypoint import ToolEntrypoint


class UnstractSDKToolsEntrypointTest(unittest.TestCase):
    INFO_MESSAGE = "Running a mock tool"
    DEBUG_MESSAGE = "Example DEBUG message"

    class MockTool(BaseTool):
        def run(
            self,
            params: dict[str, Any] = {},
            settings: dict[str, Any] = {},
            workflow_id: str = "",
        ) -> None:
            self.stream_log(UnstractSDKToolsEntrypointTest.INFO_MESSAGE)
            self.stream_log(
                UnstractSDKToolsEntrypointTest.DEBUG_MESSAGE, LogLevel.DEBUG
            )

    @classmethod
    def setUpClass(cls):
        cls.tool = cls.MockTool(log_level=LogLevel.DEBUG)

    def _launch_tool(self, args: list[str]) -> str:
        captured_output = StringIO()
        with patch("sys.stdout", new=captured_output):
            ToolEntrypoint.launch(tool=self.tool, args=args)
        return captured_output.getvalue()

    def test_spec(self):
        args = [
            "--command",
            "SPEC",
            "--workflow-id",
            "00000000-0000-0000-0000-000000000000",
            "--log-level",
            "INFO",
        ]
        captured_output_str = self._launch_tool(args=args)
        self.assertIn("SPEC", captured_output_str)

    def test_properties(self):
        args = [
            "--command",
            "PROPERTIES",
            "--workflow-id",
            "00000000-0000-0000-0000-000000000000",
            "--log-level",
            "INFO",
        ]
        captured_output_str = self._launch_tool(args=args)
        self.assertIn("PROPERTIES", captured_output_str)

    def test_variables(self):
        args = [
            "--command",
            "VARIABLES",
            "--workflow-id",
            "00000000-0000-0000-0000-000000000000",
            "--log-level",
            "INFO",
        ]
        captured_output_str = self._launch_tool(args=args)
        self.assertIn("VARIABLES", captured_output_str)

    def test_run(self):
        args = [
            "--command",
            "RUN",
            "--params",
            "{}",
            "--settings",
            "{}",
            "--workflow-id",
            "00000000-0000-0000-0000-000000000000",
            "--log-level",
            "INFO",
        ]
        captured_output_str = self._launch_tool(args=args)
        self.assertIn(self.INFO_MESSAGE, captured_output_str)

    def test_run_debug(self):
        args = [
            "--command",
            "RUN",
            "--params",
            "{}",
            "--settings",
            "{}",
            "--workflow-id",
            "00000000-0000-0000-0000-000000000000",
            "--log-level",
            "DEBUG",
        ]
        captured_output_str = self._launch_tool(args=args)
        self.assertIn(self.DEBUG_MESSAGE, captured_output_str)


if __name__ == "__main__":
    unittest.main()


================================================
File: tests/tool/test_static.py
================================================
import unittest
from io import StringIO
from unittest.mock import patch

from unstract.sdk.tool.mixin import ToolConfigHelper
from unstract.sdk.tool.stream import StreamMixin
from unstract.sdk.utils import ToolUtils


class UnstractSDKToolsStaticTest(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.tool = StreamMixin()

    def test_spec(self):
        spec = ToolUtils.json_to_str(
            ToolConfigHelper.spec(spec_file="config/tool_spec.json")
        )
        self.assertIsNotNone(spec)

    def test_stream_spec(self):
        spec = ToolUtils.json_to_str(
            ToolConfigHelper.spec(spec_file="config/tool_spec.json")
        )
        captured_output = StringIO()
        with patch("sys.stdout", new=captured_output):
            self.tool.stream_spec(spec)
        captured_output_str = captured_output.getvalue()
        # print(captured_output_str)
        self.assertIn("SPEC", captured_output_str)

    def test_properties(self):
        properties = ToolUtils.json_to_str(
            ToolConfigHelper.properties(
                properties_file="config/tool_properties.json"
            )
        )
        self.assertIsNotNone(properties)

    def test_stream_properties(self):
        properties = ToolUtils.json_to_str(
            ToolConfigHelper.properties(
                properties_file="config/tool_properties.json"
            )
        )
        captured_output = StringIO()
        with patch("sys.stdout", new=captured_output):
            self.tool.stream_properties(properties)
        captured_output_str = captured_output.getvalue()
        # print(captured_output_str)
        self.assertIn("PROPERTIES", captured_output_str)

    def test_icon(self):
        icon = ToolConfigHelper.icon(icon_file="config/icon.svg")
        self.assertIsNotNone(icon)

    def test_stream_icon(self):
        icon = ToolConfigHelper.icon(icon_file="config/icon.svg")
        captured_output = StringIO()
        with patch("sys.stdout", new=captured_output):
            self.tool.stream_icon(icon)
        captured_output_str = captured_output.getvalue()
        # print(captured_output_str)
        self.assertIn("ICON", captured_output_str)


if __name__ == "__main__":
    unittest.main()


================================================
File: tests/tool/config/properties.json
================================================
{
  "display_name": "File Operations",
  "function_name": "fileops",
  "description": "This is a tool which can open a folder and list all the files in a given folder. Operation can be \"list files\", \"get file\",\"put file\".",
  "parameters": [
    {
      "name": "operation",
      "type": "string",
      "description": "The file operation which needs to be performed"
    },
    {
      "name": "file",
      "type": "string",
      "description": "The file or folder on which the operation is to be performed"
    }
  ],
  "versions": [
    "1.0.0"
  ],
  "is_cacheable": false,
  "input_type": "string",
  "output_type": "string",
  "requires": {
    "files": {
      "input": true,
      "output": true
    },
    "databases": {
      "input": false,
      "output": false
    }
  }
}


================================================
File: tests/tool/config/runtime_variables.json
================================================
{
    "title": "Runtime Variables",
    "description": "Runtime Variables for mock tool",
    "type": "object",
    "required": [
      "MOCK_KEY"
    ],
    "properties": {
      "MOCK_KEY": {
        "type": "string",
        "title": "Mock API Key",
        "description": "Your mock API key"
      }
    }
  }


================================================
File: tests/tool/config/spec.json
================================================
{
  "title": "File operations tool",
  "description": "Setup the file operations tool",
  "type": "object",
  "required": [
  ],
  "properties": {
    "rootFolder": {
      "type": "string",
      "title": "Folder to process",
      "default": "",
      "description": "The root folder to start processing files from. Leave it empty to use the root folder"
    },
    "processSubDirectories": {
      "type": "boolean",
      "title": "Process sub directories",
      "default": true,
      "description": "Process sub directories recursively"
    },
    "fileExtensions": {
      "type": "array",
      "title": "File types to process",
      "description": "Limit the file types to process. Leave it empty to process all files",
      "items": {
        "type": "string",
        "enum": [
          "PDF documents",
          "Text documents",
          "Word documents",
          "Openoffice documents",
          "Images"
        ]
      }
    },
    "maxFiles": {
      "type": "number",
      "title": "Max files to process",
      "default": 100,
      "description": "The maximum number of files to process"
    }
  }
}


================================================
File: tests/tool/config/tool_properties.json
================================================
{
  "display_name": "File Operations",
  "function_name": "fileops",
  "description": "This is a tool which can open a folder and list all the files in a given folder. Operation can be \"list files\", \"get file\",\"put file\".",
  "parameters": [
    {
      "name": "operation",
      "type": "string",
      "description": "The file operation which needs to be performed"
    },
    {
      "name": "file",
      "type": "string",
      "description": "The file or folder on which the operation is to be performed"
    }
  ],
  "versions": [
    "1.0.0"
  ],
  "is_cacheable": false,
  "input_type": "string",
  "output_type": "string",
  "requires": {
    "files": {
      "input": true,
      "output": true
    },
    "databases": {
      "input": false,
      "output": false
    }
  }
}


================================================
File: tests/tool/config/tool_spec.json
================================================
{
  "title": "File operations tool",
  "description": "Setup the file operations tool",
  "type": "object",
  "required": [
  ],
  "properties": {
    "rootFolder": {
      "type": "string",
      "title": "Folder to process",
      "default": "",
      "description": "The root folder to start processing files from. Leave it empty to use the root folder"
    },
    "processSubDirectories": {
      "type": "boolean",
      "title": "Process sub directories",
      "default": true,
      "description": "Process sub directories recursively"
    },
    "fileExtensions": {
      "type": "array",
      "title": "File types to process",
      "description": "Limit the file types to process. Leave it empty to process all files",
      "items": {
        "type": "string",
        "enum": [
          "PDF documents",
          "Text documents",
          "Word documents",
          "Openoffice documents",
          "Images"
        ]
      }
    },
    "maxFiles": {
      "type": "number",
      "title": "Max files to process",
      "default": 100,
      "description": "The maximum number of files to process"
    }
  }
}


================================================
File: .github/pull_request_template.md
================================================
## What

...

## Why

...

## How

...

## Relevant Docs

-

## Related Issues or PRs

-

## Dependencies Versions / Env Variables

-

## Notes on Testing

...

## Screenshots

...

## Checklist

I have read and understood the [Contribution Guidelines]().


================================================
File: .github/ISSUE_TEMPLATE/bug_report.md
================================================
---
name: Bug report
about: Report any issues found and help us improve
title: 'fix: [ISSUE]'
labels: bug
assignees: ''

---

## Describe the bug
A clear and concise description of what the bug is.

## To reproduce
Steps to reproduce the behavior.

## Expected behavior
A clear and concise description of what you expected to happen.

## Environment details
 - Version: [e.g. 0.41.2]

## Additional context
Add any other context about the problem here.

## Screenshots
If applicable, add screenshots to help explain your problem.


================================================
File: .github/ISSUE_TEMPLATE/feature_request.md
================================================
---
name: Feature request
about: Suggest an idea for this project
title: 'feat: [ISSUE]'
labels: enhancement
assignees: ''

---

**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.


================================================
File: .github/workflows/pypi-publish.yml
================================================
name: Publish Python Package

on:
  release:
    types:
        - published

jobs:
  pypi-publish:
    name: upload release to PyPI
    runs-on: ubuntu-20.04
    permissions:
      contents: read
      id-token: write
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.release.tag_name }}

      - uses: actions/setup-python@v5
        with:
            python-version: '3.9.6'

      - name: Setup PDM
        uses: pdm-project/setup-pdm@v4
        with:
          python-version: 3.9.6
          version: 2.12.3

      - name: Publish package distributions to PyPI
        run: pdm publish


